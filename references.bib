@misc{338puckettehaganPdf,
  title = {338-Puckette-Hagan.Pdf},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/3G7B7ETM/view.html},
  howpublished = {https://drive.google.com/file/d/179jLns-W-KAuf7\_pv\_dVvBomrEk5ZFX9/view?usp=embed\_facebook},
  journal = {Google Docs},
  keywords = {gesture}
}

@article{adeliAudiovisualCorrespondenceMusical2014,
  title={Audiovisual correspondence between musical timbre and visual shapes},
  author={Adeli, Mohammad and Rouat, Jean and Molotchnikoff, St{\'e}phane},
  journal={Frontiers in human neuroscience},
  volume={8},
  pages={352},
  year={2014},
  publisher={Frontiers}
}

@article{adeliAudiovisualCorrespondenceMusical2014a,
  title = {Audiovisual Correspondence between Musical Timbre and Visual Shapes},
  author = {Adeli, Mohammad and Rouat, Jean and Molotchnikoff, St{\'e}phane},
  year = {2014},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2014.00352},
  abstract = {This article investigates the cross-modal correspondences between musical timbre and shapes. Previously, such features as pitch, loudness, light intensity, visual size, and color characteristics have mostly been used in studies of audio-visual correspondences. Moreover, in most studies, simple stimuli e.g. simple tones have been utilized. In this experiment, 23 musical sounds varying in fundamental frequency and timbre but fixed in loudness were used. Each sound was presented once against colored shapes and once against grayscale shapes. Subjects had to select the visual equivalent of a given sound i.e. its shape, color (or grayscale) and vertical position. This scenario permitted studying the associations between normalized timbre and visual shapes as well as some of the previous findings for more complex stimuli. 119 subjects (31 females and 88 males) participated in the online experiment. Subjects included 36 claimed professional musicians, 47 claimed amateur musicians and 36 claimed non-musicians. 31 subjects have also claimed to have synesthesia-like experiences. A strong association between timbre of envelope normalized sounds and visual shapes was observed. Subjects have strongly associated soft timbres with blue, green or light gray rounded shapes, harsh timbres with red, yellow or dark gray sharp angular shapes and timbres having elements of softness and harshness together with a mixture of the two previous shapes. Color or grayscale had no effect on timbre-shape associations. Fundamental frequency was not associated with height, grayscale or color. The significant correspondence between timbre and shape revealed by the present work allows designing substitution systems which might help the blind to perceive shapes through timbre.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/VJH3VRYG/Adeli et al. - 2014 - Audiovisual correspondence between musical timbre .pdf},
  journal = {Frontiers in Human Neuroscience},
  keywords = {Audiovisual correspondences,Color,pitch,sensory substitution,Shapes,sound shape,synesthesia,timbre},
  language = {English}
}

@inproceedings{agarwalUsabilityEvaluatingEmotional2009,
  title = {Beyond Usability: Evaluating Emotional Response as an Integral Part of the User Experience},
  shorttitle = {Beyond Usability},
  booktitle = {Proceedings of the 27th International Conference Extended Abstracts on {{Human}} Factors in Computing Systems - {{CHI EA}} '09},
  author = {Agarwal, Anshu and Meyer, Andrew},
  year = {2009},
  pages = {2919},
  publisher = {{ACM Press}},
  address = {{Boston, MA, USA}},
  doi = {10.1145/1520340.1520420},
  abstract = {The role of emotion as an integral component of user experience has mostly been overlooked in the HCI literature. Instead, usability has been relied upon as the key indicator of user experience. We developed a methodology that combined verbal and nonverbal emotion scales. A usability study was then conducted, in which we collected both traditional usability metrics and emotional response data. Results indicated insignificant differences in usability metrics but numerous significant differences between emotional responses of users. Exploration of these emotional responses successfully provided additional insight into the user experience.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/HTAQHG62/Agarwal and Meyer - 2009 - Beyond usability evaluating emotional response as.pdf},
  isbn = {978-1-60558-247-4},
  language = {en}
}

@article{ahonenFaceDescriptionLocal2006,
  title = {Face {{Description}} with {{Local Binary Patterns}}: {{Application}} to {{Face Recognition}}},
  shorttitle = {Face {{Description}} with {{Local Binary Patterns}}},
  author = {Ahonen, T. and Hadid, A. and Pietikainen, M.},
  year = {2006},
  month = dec,
  volume = {28},
  pages = {2037--2041},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2006.244},
  abstract = {This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/P8GZDVDA/Ahonen et al. - 2006 - Face Description with Local Binary Patterns Appli.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {LBP},
  language = {en},
  number = {12}
}

@article{alcaide-marzalExploratoryStudyUse2013,
  title = {An Exploratory Study on the Use of Digital Sculpting in Conceptual Product Design},
  author = {{Alcaide-Marzal}, Jorge and {Diego-M{\'a}s}, Jos{\'e} Antonio and {Asensio-Cuesta}, Sabina and {Piqueras-Fiszman}, Betina},
  year = {2013},
  month = mar,
  volume = {34},
  pages = {264--284},
  issn = {0142-694X},
  doi = {10.1016/j.destud.2012.09.001},
  abstract = {The product design process involves intensive manipulation of graphical data, from pencil sketches to CAD files. The use of graphic software is common among professionals in this field. Despite this, the conceptual design stage remains intensive in paper and pencil work, as CAD systems are still too rigid to allow a creative production of concepts. In this paper the use of digital sculpting software is proposed as a way of producing 3D sketches in the early stages of the process. An experiment is conducted to determine to which extent 3D sculpt sketches can be considered as a suitable tool for conceptual design. The results show a better performance of 2D drawings, but support the complementary use of digital sculpting.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/DKR9MKVH/Alcaide-Marzal et al. - 2013 - An exploratory study on the use of digital sculpti.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/5ABZQJ5T/S0142694X12000610.html},
  journal = {Design Studies},
  keywords = {3D sketching,computer aided design,conceptual design,product design},
  language = {en},
  number = {2}
}

@inproceedings{alemiGrooveNetRealTimeMusicDriven2017,
  title = {{{GrooveNet}}: {{Real}}-{{Time Music}}-{{Driven Dance Movement Generation}} Using {{Artificial Neural Networks}}},
  shorttitle = {{{GrooveNet}}},
  author = {Alemi, Omid and Fran{\c c}oise, Jules and Pasquier, Philippe},
  year = {2017},
  month = aug,
  abstract = {We present the preliminary results of GrooveNet, a generative system that learns to synthesize dance movements for a given audio track in real-time. Our intended application for GrooveNet is a public interactive installation in which the audience can provide their own music to interact with an avatar. We investigate training artificial neural networks, in particular, Factored Conditional Restricted Boltzmann Machines (FCRBM) and Recurrent Neural Networks (RNN), on a small dataset of four synchronized music and motion capture recordings of dance movements that we have captured for this project. Our initial results show that we can train the FCRBM on this small dataset to generate dance movements. However, the model cannot generalize well to music tracks beyond the training data. We outline our plans to further develop GrooveNet.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/BQU2VJXJ/Alemi et al. - 2017 - GrooveNet Real-Time Music-Driven Dance Movement G.pdf},
  keywords = {Dance}
}

@article{allikONTOLOGYAUDIOFEATURES2016,
  title = {{{AN ONTOLOGY FOR AUDIO FEATURES}}},
  author = {Allik, Alo and Fazekas, Gyorgy and Sandler, Mark},
  year = {2016},
  pages = {7},
  abstract = {A plurality of audio feature extraction toolsets and feature datasets are used by the MIR community. Their different conceptual organisation of features and output formats however present difficulties in exchanging or comparing data, while very limited means are provided to link features with content and provenance. These issues are hindering research reproducibility and the use of multiple tools in combination. We propose novel Semantic Web ontologies (1) to provide a common structure for feature data formats and (2) to represent computational workflows of audio features facilitating their comparison. The Audio Feature Ontology provides a descriptive framework for expressing different conceptualisations of and designing linked data formats for content-based audio features. To accommodate different views in organising features, the ontology does not impose a strict hierarchical structure, leaving this open to task and tool specific ontologies that derive from a common vocabulary. The ontologies are based on the analysis of existing feature extraction tools and the MIR literature, which was instrumental in guiding the design process. They are harmonised into a library of modular interlinked ontologies that describe the different entities and activities involved in music creation, production and consumption.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FDH5WEBF/Allik et al. - 2016 - AN ONTOLOGY FOR AUDIO FEATURES.pdf},
  journal = {New York City},
  language = {en}
}

@article{althoffInfluencePokemonGo,
  title = {Influence of {{Pok\'emon Go}} on {{Physical Activity}}: {{Study}} and {{Implications}}},
  author = {Althoff, Tim and White, Ryen W and Horvitz, Eric},
  pages = {14},
  abstract = {Background: Physical activity helps people maintain a healthy weight and reduces the risk for several chronic diseases. Although this knowledge is widely recognized, adults and children in many countries around the world do not get recommended amounts of physical activity. Although many interventions are found to be ineffective at increasing physical activity or reaching inactive populations, there have been anecdotal reports of increased physical activity due to novel mobile games that embed game play in the physical world. The most recent and salient example of such a game is Pok\'emon Go, which has reportedly reached tens of millions of users in the United States and worldwide. Objective: The objective of this study was to quantify the impact of Pok\'emon Go on physical activity. Methods: We study the effect of Pok\'emon Go on physical activity through a combination of signals from large-scale corpora of wearable sensor data and search engine logs for 32,000 Microsoft Band users over a period of 3 months. Pok\'emon Go players are identified through search engine queries and physical activity is measured through accelerometers. Results: We find that Pok\'emon Go leads to significant increases in physical activity over a period of 30 days, with particularly engaged users (ie, those making multiple search queries for details about game usage) increasing their activity by 1473 steps a day on average, a more than 25\% increase compared with their prior activity level (P{$<$}.001). In the short time span of the study, we estimate that Pok\'emon Go has added a total of 144 billion steps to US physical activity. Furthermore, Pok\'emon Go has been able to increase physical activity across men and women of all ages, weight status, and prior activity levels showing this form of game leads to increases in physical activity with significant implications for public health. In particular, we find that Pok\'emon Go is able to reach low activity populations, whereas all 4 leading mobile health apps studied in this work largely draw from an already very active population. Conclusions: Mobile apps combining game play with physical activity lead to substantial short-term activity increases and, in contrast to many existing interventions and mobile health apps, have the potential to reach activity-poor populations. Future studies are needed to investigate potential long-term effects of these applications.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/B7E6M57L/Althoff et al. - Influence of Pokémon Go on Physical Activity Stud.pdf},
  journal = {JOURNAL OF MEDICAL INTERNET RESEARCH},
  keywords = {Pokemon Go},
  language = {en}
}

@inproceedings{aoyamaFastSimilaritySearch2010,
  title = {Fast Similarity Search on a Large Speech Data Set with Neighborhood Graph Indexing},
  booktitle = {2010 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Aoyama, Kazuo and Watanabe, Shinji and Sawada, Hiroshi and Minami, Yasuhiro and Ueda, Naonori and Saito, Kazumi},
  year = {2010},
  month = mar,
  pages = {5358--5361},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2010.5494950},
  abstract = {This paper presents a novel graph-based approach for solving a problem of fast finding a speech model acoustically similar to a query model from a large set of speech models. Each speech model in the set is represented by a Gaussian mixture model and dissimilarity from a GMM to another is measured with a Kullback-Leibler divergence (KLD). Conventional pruning techniques based on the triangle inequality for fast similarity search are not available because the model space with a KLD is not a metric space. We propose a search method that is characterized by an index of a degree-reduced nearest neighbor (DRNN) graph. The search method can efficiently find the most similar (closest) GMM to a query, exploring the DRNN graph with a best-first manner. Experimental evaluations on utterance GMM search tasks reveal a significantly low computational cost of the proposed method.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/UUSC93AE/Aoyama et al. - 2010 - Fast similarity search on a large speech data set .pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/R8TH9KWK/5494950.html},
  keywords = {Acoustic measurements,Computational efficiency,degree-reduced nearest neighbor graph,Extraterrestrial measurements,fast similarity search,Gaussian mixture model,Gaussian processes,Graph index,graph theory,graph-based approach,Indexing,Kullback-Leibler divergence,Laboratories,large speech data set,Nearest neighbor searches,neighborhood graph indexing,pruning techniques,Search methods,Search problems,Signal processing,Similarity search,Speech,speech model,speech processing,triangle inequality,Utterance retrieval}
}

@article{aramakiControllingPerceivedMaterial2011,
  title = {Controlling the {{Perceived Material}} in an {{Impact Sound Synthesizer}}},
  author = {Aramaki, M. and Besson, M. and {Kronland-Martinet}, R. and Ystad, S.},
  year = {2011},
  month = feb,
  volume = {19},
  pages = {301--314},
  issn = {1558-7916},
  doi = {10.1109/TASL.2010.2047755},
  abstract = {In this paper, we focused on the identification of the perceptual properties of impacted materials to provide an intuitive control of an impact sound synthesizer. To investigate such properties, impact sounds from everyday life objects, made of different materials (wood, metal and glass), were recorded and analyzed. These sounds were synthesized using an analysis-synthesis technique and tuned to the same chroma. Sound continua were created to simulate progressive transitions between materials. Sounds from these continua were then used in a categorization experiment to determine sound categories representative of each material (called typical sounds). We also examined changes in electrical brain activity (using event related potentials (ERPs) method) associated with the categorization of these typical sounds. Moreover, acoustic analysis was conducted to investigate the relevance of acoustic descriptors known to be relevant for both timbre perception and material identification. Both acoustic and electrophysiological data confirmed the importance of damping and highlighted the relevance of spectral content for material perception. Based on these findings, controls for damping and spectral shaping were tested in synthesis applications. A global control strategy, with a three-layer architecture, was proposed for the synthesizer allowing the user to intuitively navigate in a ``material space'' and defining impact sounds directly from the material label. A formal perceptual evaluation was finally conducted to validate the proposed control strategy.},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Analysis–synthesis,control,event related potentials,impact sounds,mapping,material,sound categorization,timbre},
  number = {2}
}

@article{aramakiControllingPerceivedMaterial2011a,
  title={Controlling the perceived material in an impact sound synthesizer},
  author={Aramaki, Mitsuko and Besson, Mireille and Kronland-Martinet, Richard and Ystad, S{\o}lvi},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={19},
  number={2},
  pages={301--314},
  year={2010},
  publisher={IEEE}
}

@article{aramakiPercussiveSoundSynthesizer2006,
  title = {A {{Percussive Sound Synthesizer Based}} on {{Physical}} and {{Perceptual Attributes}}},
  author = {Aramaki, Mitsuko and {Kronland-Martinet}, Richard and Voinier, Thierry and Ystad, S{\o}lvi},
  year = {2006},
  month = jun,
  volume = {30},
  pages = {32--41},
  doi = {10.1162/comj.2006.30.2.32},
  abstract = {An efficient, hybrid synthesis technique for percussive sounds is proposed. The model reproduces two main contributions characterizing the perceived material and the perceptual dimensions of the structure. A real-time application of the model shows its accuracy, allowing the generation of a wide variety of impact sounds. When used with mapping strategies such as a morphing control, a material space, and an original approach to tune the complex sounds based on the standard-practice Western theory of harmony, the proposed model becomes an interesting research tool to investigate pitch perception of complex sounds.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2A2ITYPF/Aramaki et al. - 2006 - A Percussive Sound Synthesizer Based on Physical a.pdf},
  journal = {Computer Music Journal},
  keywords = {feature based synthesis}
}

@article{arfibStrategiesMappingGesture2002,
  title = {Strategies of Mapping between Gesture Data and Synthesis Model Parameters Using Perceptual Spaces},
  author = {Arfib, D. and Couturier, J. M. and Kessous, L. and Verfaille, V.},
  year = {2002},
  month = aug,
  volume = {7},
  pages = {127--144},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771802002054},
  abstract = {This paper is about mapping strategies between gesture data and synthesis model parameters by means of perceptual spaces. We define three layers in the mapping chain: from gesture data to gesture perceptual space, from sound perceptual space to synthesis model parameters, and between the two perceptual spaces. This approach makes the implementation highly modular. Both perceptual spaces are developed and depicted with their features. To get a simple mapping between the gesture perceptual subspace and the sound perceptual subspace, we need to focus our attention on the two other mappings. We explain the mapping types: explicit/implicit, static/dynamic. We also present the technical and aesthetical limits introduced by mapping. Some practical examples are given of the use of perceptual spaces in experiments done at LMA in a musical context. Finally, we discuss several implications of the mapping strategies: the influence of chosen mapping limits onto performers' virtuosity, and the incidence of mapping on the learning process with virtual instruments and on improvisation possibilities.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JM2T2NFK/Arfib et al. - 2002 - Strategies of mapping between gesture data and syn.pdf},
  journal = {Organised Sound},
  language = {en},
  number = {2}
}

@article{assmannTrackdrawGraphicalInterface1994,
  title = {Track-Draw: {{A}} Graphical Interface for Controlling the Parameters of a Speech Synthesizer},
  shorttitle = {Track-Draw},
  author = {Assmann, Peter and Ballard, Will and Bornstein, Laurie and Paschall, Dwayne},
  year = {1994},
  month = dec,
  volume = {26},
  pages = {431--436},
  issn = {1532-5970},
  doi = {10.3758/BF03204661},
  abstract = {In this report we describe a graphical interface for generating voiced speech using a frequency-domain implementation of the Klatt (1980) cascade formant synthesizer. The input to the synthesizer is a set of parameter vectors, calledtracks, which specify the overall amplitude, fundamental frequency, formant frequencies, and formant bandwidths at specified time intervals. Tracks are drawn with the aid of a computer mouse that can be used either inpoint-draw mode, which selects a parameter value for a single time frame, or inline-draw mode, which uses piecewise linear interpolation to connect two user-selected endpoints. Three versions of the program are described: (1) SYNTH draws tracks on an empty time-frequency grid, (2) SPECSYNTH creates a spectrogram of a recorded signal upon which tracks can be superimposed, and (3) SWSYNTH is similar to SPECSYNTH, except that it generatessine-wave speech (Remez, Rubin, Pisoni, \& Carrell, 1981) using a set of time-varying sinusoids rather than cascaded formants. The program is written for MATLAB, an interactive computing environment for matrix computation. Track-Draw provides a useful tool for investigating the perceptually salient properties of voiced speech and other sounds.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6DVJYMSZ/Assmann et al. - 1994 - Track-draw A graphical interface for controlling .pdf},
  journal = {Behavior Research Methods, Instruments, \& Computers},
  language = {en},
  number = {4}
}

@article{assmannTrackdrawGraphicalInterface1994a,
  title = {Track-Draw: {{A}} Graphical Interface for Controlling the Parameters of a Speech Synthesizer},
  shorttitle = {Track-Draw},
  author = {Assmann, Peter and Ballard, Will and Bornstein, Laurie and Paschall, Dwayne},
  year = {1994},
  month = dec,
  volume = {26},
  pages = {431--436},
  issn = {0743-3808, 1532-5970},
  doi = {10.3758/BF03204661},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FURQYE4Q/Assmann et al. - 1994 - Track-draw A graphical interface for controlling .pdf},
  journal = {Behavior Research Methods, Instruments, \& Computers},
  language = {en},
  number = {4}
}

@incollection{astridbinInthemomentCombiningPosthoc2017,
  title = {In-the-Moment and {{Beyond}}: {{Combining Post}}-Hoc and {{Real}}-{{Time Data}} for the {{Study}} of {{Audience Perception}} of {{Electronic Music Performance}}},
  shorttitle = {In-the-Moment and {{Beyond}}},
  booktitle = {Human-{{Computer Interaction}} - {{INTERACT}} 2017},
  author = {Astrid Bin, S. M. and Morreale, Fabio and {Bryan-Kinns}, Nick and McPherson, Andrew P.},
  editor = {Bernhaupt, Regina and Dalvi, Girish and Joshi, Anirudha and K. Balkrishan, Devanuj and O'Neill, Jacki and Winckler, Marco},
  year = {2017},
  volume = {10513},
  pages = {263--281},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-67744-6_18},
  abstract = {This paper presents a methodology for the study of audience perception of live performances, using a combined approach of post-hoc and real-time data. We conducted a study that queried audience enjoyment and their perception of error in digital musical instrument (DMI) performance. We collected quantitative and qualitative data from the participants via paper survey after each performance and at the end of the concert, and during the performances spectators were invited to indicate moments of enjoyment and incidences of error using a two-button mobile app interface. This produced 58 paired post-hoc and real-time data sets for analysis. We demonstrate that real-time indication of error does not translate to reported non-enjoyment and post-hoc and real-time data sets are not necessarily consistent for each participant. In conclusion we make the case for a combined approach to audience studies in live performance contexts.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PIEYDCA9/Astrid Bin et al. - 2017 - In-the-moment and Beyond Combining Post-hoc and R.pdf},
  isbn = {978-3-319-67743-9 978-3-319-67744-6},
  language = {en}
}

@inproceedings{baiSketchBasedImage2019,
  title = {Sketch {{Based Image Retrieval}} with {{Adversarial Network}}},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Video}} and {{Image Processing}}},
  author = {Bai, Zhe and Hou, Hong and Kong, Ni},
  year = {2019},
  month = dec,
  pages = {148--152},
  publisher = {{ACM}},
  address = {{Shanghai China}},
  doi = {10.1145/3376067.3376070},
  abstract = {Sketch retrieval is a specific cross-domain retrieval task. The core of sketch retrieval is to learn a common feature subspace, where the features of sketches and natural images can be both discriminative and domain-invariant. However, similarity constraints can impair the performance of the feature extractor, resulting in unsatisfactory retrieval accuracy. For this problem, we propose a novel sketch based image retrieval method based on adversarial network. Our method is demonstrated as follows: Firstly, we train the sketch image network and natural image network to improve the ability of classification; secondly, we train the adversarial network to promote the feature fusion of sketches ,of which the network is constituted by feature extractor and domain classifier; thirdly, we use the deep convolutional neural network to extract the deep feature to achieve retrieval. Experiments on retrieval show positive results.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/F5EGBW8Z/Bai et al. - 2019 - Sketch Based Image Retrieval with Adversarial Netw.pdf},
  isbn = {978-1-4503-7682-2},
  language = {en}
}

@inproceedings{baiSketchBasedImage2019a,
  title = {Sketch {{Based Image Retrieval}} with {{Adversarial Network}}},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Video}} and {{Image Processing}}},
  author = {Bai, Zhe and Hou, Hong and Kong, Ni},
  year = {2019},
  month = dec,
  pages = {148--152},
  publisher = {{Association for Computing Machinery}},
  address = {{Shanghai, China}},
  doi = {10.1145/3376067.3376070},
  abstract = {Sketch retrieval is a specific cross-domain retrieval task. The core of sketch retrieval is to learn a common feature subspace, where the features of sketches and natural images can be both discriminative and domain-invariant. However, similarity constraints can impair the performance of the feature extractor, resulting in unsatisfactory retrieval accuracy. For this problem, we propose a novel sketch based image retrieval method based on adversarial network. Our method is demonstrated as follows: Firstly, we train the sketch image network and natural image network to improve the ability of classification; secondly, we train the adversarial network to promote the feature fusion of sketches, of which the network is constituted by feature extractor and domain classifier; thirdly, we use the deep convolutional neural network to extract the deep feature to achieve retrieval. Experiments on retrieval show positive results.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4ZWNF4RR/Bai et al. - 2019 - Sketch Based Image Retrieval with Adversarial Netw.pdf},
  isbn = {978-1-4503-7682-2},
  keywords = {Adversarial Network,Convolutional Neural Network,Feature Extraction,image retrieval,Sketch Based Image Retrieval},
  series = {{{ICVIP}} 2019}
}

@article{barbosaConsideringAudienceView,
  title = {Considering {{Audience}}'s {{View Towards}} an {{Evaluation Methodology}} for {{Digital Musical Instruments}}},
  author = {Barbosa, Jer{\^o}nimo and Calegario, Filipe and Teichrieb, Veronica and Ramalho, Geber and McGlynn, Patrick},
  pages = {6},
  abstract = {The authors propose the development of a more complete Digital Music Instrument (DMI) evaluation methodology, which provides structured tools for the incremental development of prototypes based on user feedback. This paper emphasizes an important but often ignored stakeholder present in the context of musical performance: the audience. We demonstrate the practical application of an audience focused methodology through a case study (`Illusio'), discuss the obtained results and possible improvements for future works.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4H9Q4U4J/Barbosa et al. - Considering Audience's View Towards an Evaluation .pdf},
  language = {en}
}

@article{barthetAcousticalCorrelatesTimbre2010,
  title = {Acoustical {{Correlates}} of {{Timbre}} and {{Expressiveness}} in {{Clarinet Performance}}},
  author = {Barthet, Mathieu and Depalle, Philippe and {Kronland-Martinet}, Richard and Ystad, S{\o}lvi},
  year = {2010},
  month = dec,
  volume = {28},
  pages = {135--154},
  doi = {10.1525/mp.2010.28.2.135},
  abstract = {This study deals with the acoustical factors liable to account for expressiveness in clarinet performances. Mechanical and expressive performances of excerpts from Bach's Suite No. II and Mozart's Quintet for Clarinet and Strings were recorded. Timbre, timing, dynamics, and pitch descriptors were extracted from the recorded performances. The data were processed using a two-way analysis of variance, where the musician's expressive intentions and the note factors were defined as the independent variables. In both musical excerpts, a strong effect of the expressive intention was observed on the timbre (attack time, spectral centroid, odd/even ratio), timing (intertone onset intervals) and dynamics (root mean square envelope) descriptors. The changes in the timbre descriptors were found to depend on the position of the notes in the musical phrases. These results suggest that timbre, as well as timing and dynamics variations, may mediate expressiveness in the musical messages transmitted from performers to listeners.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RKTSS7RR/Barthet et al. - 2010 - Acoustical Correlates of Timbre and Expressiveness.pdf},
  journal = {Music Perception: An Interdisciplinary Journal}
}

@article{barthetCrossroadsInteractiveMusic,
  title = {Crossroads: {{Interactive Music Systems Transforming Performance}}, {{Production}} and {{Listening}}},
  author = {Barthet, Mathieu and Sandler, Mark and Thalmann, Florian and Wiggins, Geraint and Fazekas, Gy{\"o}rgy},
  pages = {4},
  abstract = {We discuss several state-of-the-art systems that propose new paradigms and user workflows for music composition, production, performance, and listening. We focus on a selection of systems that exploit recent advances in semantic and affective computing, music information retrieval (MIR) and semantic web, as well as insights from fields such as mobile computing and information visualisation. These systems offer the potential to provide transformative experiences for users, which is manifested in creativity, engagement, efficiency, discovery and affect.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8333IGMM/Barthet et al. - Crossroads Interactive Music Systems Transforming.pdf},
  language = {en}
}

@article{berndtSonicAffordancesSystem2018,
  title = {The {{Sonic Affordances}} of a {{System}} for {{Multitouch Manipulation}} of {{Stereophonic Noise Spectra}}},
  author = {Berndt, Axel and {Al-Kassab}, Nadia and Dachselt, Raimund},
  year = {2018},
  month = jan,
  volume = {41},
  pages = {32--44},
  issn = {0148-9267, 1531-5169},
  doi = {10.1162/comj_a_00438},
  abstract = {TouchNoise is a multitouch interface for creative work with noise. It allows direct and indirect manipulation of sound particles, which are added together in a panning and frequency space. Based on the mechanics of a multiagent system and flocking algorithms, novel possibilities for the creation and modulation of noise and harmonic spectra are supported. TouchNoise underwent extensive revisions and extensions throughout a three-year, iterative development process. This article provides a comprehensive overview of the final TouchNoise concept and its approach to mapping and interaction, from which a variety of unique sonic capabilities derives. This article is based on our experiences with a fully functional prototype implementation, and focuses on the systematic exploration and discussion of these new sonic capabilities and corresponding playing techniques, which differ strongly from traditional synthesis interfaces.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/IUB9GCCS/Berndt et al. - 2018 - The Sonic Affordances of a System for Multitouch M.pdf},
  journal = {Computer Music Journal},
  language = {en},
  number = {4}
}

@article{bertaminiObserversCurvatureThey2016,
  title = {Do Observers like Curvature or Do They Dislike Angularity?},
  author = {Bertamini, Marco and Palumbo, Letizia and Gheorghes, Tamara Nicoleta and Galatsidas, Mai},
  year = {2016},
  volume = {107},
  pages = {154--178},
  issn = {2044-8295},
  doi = {10.1111/bjop.12132},
  abstract = {Humans have a preference for curved over angular shapes, an effect noted by artists as well as scientists. It may be that people like smooth curves or that people dislike angles, or both. We investigated this phenomenon in four experiments. Using abstract shapes differing in type of contour (angular vs. curved) and complexity, Experiment 1 confirmed a preference for curvature not linked to perceived complexity. Experiment 2 tested whether the effect was modulated by distance. If angular shapes are associated with a threat, the effect may be stronger when they are presented within peripersonal space. This hypothesis was not supported. Experiment 3 tested whether preference for curves occurs when curved lines are compared to straight lines without angles. Sets of coloured lines (angular vs. curved vs. straight) were seen through a circular or square aperture. Curved lines were liked more than either angular or straight lines. Therefore, angles are not necessary to generate a preference for curved shapes. Finally, Experiment 4 used an implicit measure of preference, the manikin task, to measure approach/avoidance behaviour. Results did not confirm a pattern of avoidance for angularity but only a pattern of approach for curvature. Our experiments suggest that the threat association hypothesis cannot fully explain the curvature effect and that curved shapes are, per se, visually pleasant.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bjop.12132},
  copyright = {\textcopyright{} 2015 The Authors. British Journal of Psychology published by John Wiley \& Sons Ltd on behalf of the British Psychological Society},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/3KLYHB2N/Bertamini et al. - 2016 - Do observers like curvature or do they dislike ang.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9N8J67C9/bjop.html},
  journal = {British Journal of Psychology},
  keywords = {aesthetics,complexity,curvature,shape preference,visual preference},
  language = {en},
  number = {1}
}

@article{binRiskyBusinessDisfluency,
  title = {Risky Business: {{Disfluency}} as a Design Strategy},
  author = {Bin, S M Astrid and {Bryan-Kinns}, Nick and McPherson, Andrew P},
  pages = {6},
  abstract = {This paper presents a study examining the effects of disfluent design on audience perception of digital musical instrument (DMI) performance. Disfluency, defined as a barrier to effortless cognitive processing, has been shown to generate better results in some contexts as it engages higher levels of cognition. We were motivated to determine if disfluent design in a DMI would result in a risk state that audiences would be able to perceive, and if this would have any effect on their evaluation of the performance. A DMI was produced that incorporated a disfluent characteristic: It would turn itself off if not constantly moved. Six physically identical instruments were produced, each in one of three versions: Control (no disfluent characteristics), mild disfluency (turned itself off slowly), and heightened disfluency (turned itself off more quickly). 6 percussionists each performed on one instrument for a live audience (N=31), and data was collected in the form of real-time feedback (via a mobile phone app), and post-hoc surveys. Though there was little difference in ratings of enjoyment between the versions of the instrument, the real-time and qualitative data suggest that disfluent behaviour in a DMI may be a way for audiences to perceive and appreciate performer skill.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GKHWIZ7U/Bin et al. - Risky business Disﬂuency as a design strategy.pdf},
  language = {en}
}

@article{bittonNeuralGranularSound,
  title = {Neural {{Granular Sound Synthesis}}},
  author = {Bitton, Adrien and Esling, Philippe and Harada, Tatsuya},
  pages = {6},
  abstract = {Granular sound synthesis is a popular audio generation technique based on rearranging sequences of small waveform windows. In order to control the synthesis, all grains in a given corpus are analyzed through a set of acoustic descriptors. This provides a representation reflecting some form of local similarities across the grains. However, the quality of this grain space is bound by that of the descriptors. Its traversal is not continuously invertible to signal and does not render any structured temporality.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/I6WHK63F/Bitton et al. - Neural Granular Sound Synthesis.pdf},
  language = {en}
}

@article{blackburnTIMBRALNOTATIONSPECTROGRAMS,
  title = {{{TIMBRAL NOTATION FROM SPECTROGRAMS}} : {{NOTATING THE UN}}-{{NOTATABLE}}?},
  author = {Blackburn, Andrew and Penny, Jean},
  pages = {8},
  abstract = {This paper outlines a research project currently underway in Malaysia that, through spectography, seeks to find models that might assist in the future development of a timbral notation. Located within the music creation and performance practices of the researchers, the project has elements of interculturality, which both enrich and inform the research. The authors consider the nature of a music score, the explicit and implicit information it carries, and how this impacts on the models being developed. The understandings elicited to date are not only located in music practice, but are underpinned and supported by the theoretical works of a number of theorists. The overall research project is broken down into smaller discrete subprojects which are discussed, andcontextualized in the wider project. The paper includes a discussion of the score as artifact or `thing' the relationships that are implicit within it, and the infinite potential it contains. Other outcomes are suggestive of a possible model of gestural notation which will be a further avenue of research. The paper concludes with suggestions of future research areas following the models of timbral notation being explored in this project.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2GBSKXCE/Blackburn and Penny - TIMBRAL NOTATION FROM SPECTROGRAMS  NOTATING THE .pdf},
  keywords = {music notation},
  language = {en}
}

@article{blakeTimbreDifferentiationIndie2012,
  title={Timbre as differentiation in indie music},
  author={Blake, David K},
  journal={Music Theory Online},
  volume={18},
  number={2},
  year={2012}
}

@article{bottiniSoundSymbolismSighted2019,
  title={Sound symbolism in sighted and blind. The role of vision and orthography in sound-shape correspondences},
  author={Bottini, Roberto and Barilari, Marco and Collignon, Olivier},
  journal={Cognition},
  volume={185},
  pages={62--70},
  year={2019},
  publisher={Elsevier}
}

@article{boulezTimbreCompositionTimbre1987,
  title = {Timbre and Composition - Timbre and Language},
  author = {Boulez, Pierre},
  year = {1987},
  month = jan,
  volume = {2},
  pages = {161--171},
  issn = {0749-4467},
  doi = {10.1080/07494468708567057},
  abstract = {The function of timbre in 20th Century instrumental music is discussed in terms of the relation between timbre and musical language. Up to the 19th Century, the function of timbre was primarily related to its identity in addition to being charged with certain effective and symbolic characteristics. The identities of Western instruments are standardized, investing them with a certain neutrality that allows the construction of pitch hierarchies that are unperturbed by differences in timbre of the instruments. For small ensembles, timbre has a stability and a separating power that provides an identification and clarification of form and timbre. The function of timbral identity is one of articulation. With the modern orchestra, the use of instruments is more flexible and their identification becomes more mobile and temporary. Composing the blending of timbres into complex sound-objects follows from the confronting of established sound hierarchies and an enriching of the sound vocabulary. The function of timbre in composed sound-objects is one of illusion and is based upon the technique of fusion. One contrasts, then, notions of raw timbre and organized timbre. The importance of discontinuity in musical dimensions for composition is considered with respect to timbre. Finally, the relation between timbre, composition and sound transmission in an acoustic space is discussed.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/DPQW3KXA/07494468708567057.html},
  journal = {Contemporary Music Review},
  keywords = {articulation,composition,fusion,identity,illusion,musical language,timbre},
  number = {1}
}

@article{peeters2004large,
  title={A large set of audio features for sound description (similarity and classification) in the CUIDADO project},
  author={Peeters, Geoffroy},
  journal={CUIDADO Ist Project Report},
  volume={54},
  number={0},
  pages={1--25},
  year={2004}
}

@inproceedings{bourachotPerceptionObjectAttributes2019,
  title={Perception of the object attributes for sound synthesis purposes},
  author={Bourachot, Antoine and Kanzari, Khoubeib and Aramaki, Mitsuko and Ystad, S{\o}lvi and Kronland-Martinet, Richard},
  booktitle={Computer Music Multidisciplinary Research 2019},
  year={2019}
}

@inproceedings{bourachotPerceptionObjectAttributes2019a,
  title = {Perception of the Object Attributes for Sound Synthesis Purposes},
  booktitle = {Computer {{Music Multidisciplinary Research}} 2019},
  author = {Bourachot, Antoine and Kanzari, Khoubeib and ARAMAKI, Mitsuko and Ystad, S{\o}lvi and {Kronland-Martinet}, Richard},
  year = {2019},
  month = oct,
  address = {{Marseille, France}},
  abstract = {This paper presents a work in progress on the perception of the attributes of the shape of a resonant object. As part of the ecological approach to perception-assuming that a sound contains specific morphologies that convey perceptually relevant information responsible for its recognition, called invariants-the PRISM laboratory has developed an environmental sound synthesizer aiming to provide perceptual and intuitive controls for a non-expert user. Following a brief presentation of the di{$\carriagereturn$}erent strategies for controlling the perceptual attributes of the object, we present an experiment conducted with calibrated sounds generated by a physically-informed synthesis model. This test focuses on the perception of the shape of the object, more particularly its width and thickness since these attributes, especially the thickness, have not been much studied in the literature from a perceptual point of view. The first results show that the perception of width is di cult for listeners, while the perception of thickness is much easier. This study allows us to validate the proposed control strategy. Further works are planned to better characterize the perceptual invariants relevant for shape perception.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CE5D4RW5/Bourachot et al. - 2019 - Perception of the object attributes for sound synt.pdf},
  keywords = {impact sounds,intuitive control,Perception of shape,perceptual invariant,sound synthesis}
}

@article{bradleyMEASURINGEMOTIONSELFASSESSMENT,
  title = {{{MEASURING EMOTION}}: {{THE SELF}}-{{ASSESSMENT MANIKIN AND THE SEMANTIC DIFFERENTIAL}}},
  author = {Bradley, M and Lang, Peter J},
  pages = {11},
  abstract = {The Self-Assessment Manikin (SAM) is a non-verbal pictorial assessment technique that directly measures the pleasure, arousal, and dominance associated with a person's affective reaction to a wide variety of stimuli. In this experiment, we compare reports of affective experience obtained using SAM, which requires only three simple judgments, to the Semantic Differential scale devised by Mehrabian and Russell (An approach to environmental psychology, 1974) which requires 18 different ratings. Subjective reports were measured to a series of pictures that varied in both affective valence and intensity. Correlations across the two rating methods were high both for reports of experienced pleasure and felt arousal. Differences obtained in the dominance dimension of the two instruments suggest that SAM may better track the personal response to an affective stimulus. SAM is an inexpensive, easy method for quickly assessing reports of affective response in many contexts.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/S9683TMS/Bradley and Lang - MEASURING EMOTION THE SELF-ASSESSMENT MANIKIN AND.pdf},
  language = {en}
}

@techreport{brandonOnlineInstrumentDelivery2014,
  title = {Online {{Instrument Delivery}} and {{Participant Recruitment Services}}: {{Emerging Opportunities}} for {{Behavioral Accounting Research}}},
  shorttitle = {Online {{Instrument Delivery}} and {{Participant Recruitment Services}}},
  author = {Brandon, Duane M. and Long, James H. and Loraas, Tina M. and Mueller, Jennifer M. and Vansant, Brian},
  year = {2014},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {Behavioral accounting researchers have historically been constrained in their ability to reach externally valid research participants. The purpose of this paper is to familiarize researchers with two relatively new and innovative ways to overcome this issue. First, this paper discusses two online instrument delivery services provided by SurveyMonkey and Qualtrics that can be used to distribute experimental materials to geographically distributed participants quickly and inexpensively. Second, it reviews a number of participant recruitment services that behavioral accounting researchers can use to identify and recruit externally valid research participants. Specifically, this paper discusses commercial participant recruitment services provided by SurveyMonkey Audience, Qualtrics, Amazon's Mechanical Turk, and other commercial firms, as well as several non-commercial participant recruitment services associated with industry and professional organizations. Each service is evaluated against three criteria that are important to behavioral accounting researchers: (1) cost, (2) flexibility, and (3) access to populations of interest.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/HUMRNTTR/papers.html},
  keywords = {Amazon Mechanical Turk,instrument delivery,online experiment,online survey,participant panels,participant recruitment,Qualtrics,Survey Monkey},
  language = {en},
  number = {ID 2735510},
  type = {{{SSRN Scholarly Paper}}}
}

@article{braunUsingThematicAnalysis2006,
  title = {Using Thematic Analysis in Psychology},
  author = {Braun, Virginia and Clarke, Victoria},
  year = {2006},
  volume = {3},
  pages = {77--101},
  issn = {1478-0887, 1478-0895},
  doi = {10.1191/1478088706qp063oa},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7WBTBUAR/Braun and Clarke - 2006 - Using thematic analysis in psychology.pdf},
  journal = {Qualitative Research in Psychology},
  language = {en},
  number = {2}
}

@article{bremnerBoubaKikiNamibia2013,
  title={“Bouba” and “Kiki” in Namibia? A remote culture make similar shape--sound matches, but different shape--taste matches to Westerners},
  author={Bremner, Andrew J and Caparos, Serge and Davidoff, Jules and de Fockert, Jan and Linnell, Karina J and Spence, Charles},
  journal={Cognition},
  volume={126},
  number={2},
  pages={165--172},
  year={2013},
  publisher={Elsevier}
}

@article{breslauerLeapMotionSensor2019,
  title = {Leap {{Motion Sensor}} for {{Natural User Interface}}},
  author = {Breslauer, Nenad and Gali{\'c}, Irena and Kukec, Mihael and Samard{\v z}i{\'c}, Ivan},
  year = {2019},
  month = apr,
  volume = {26},
  pages = {560--565},
  issn = {1330-3651},
  doi = {10.17559/TV-20181012093055},
  abstract = {In Human Computer Interaction (HCI) research area, there is an increasing tendency to make devices as simple and as natural as possible for use. These devices are aiming to make input and output techniques, interaction, etc., easier. In...},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/L5GYXMS3/Breslauer et al. - 2019 - Leap Motion Sensor for Natural User Interface.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2XP9IP5T/219551.html},
  journal = {Tehni\v{c}ki vjesnik},
  language = {en},
  number = {2}
}

@article{bressoletteSoundSelectionGesture2017,
  title = {Sound {{Selection}} for {{Gesture Sonification}} and {{Manipulation}} of {{Virtual Objects}}},
  author = {Bressolette, Benjamin},
  year = {2017},
  volume = {11},
  pages = {7},
  abstract = {New sensors and technologies \textendash{} such as microphones, touchscreens or infrared sensors \textendash{} are currently making their appearance in the automotive sector, introducing new kinds of Human-Machine Interfaces (HMIs). The interactions with such tools might be cognitively expensive, thus unsuitable for driving tasks. It could for instance be dangerous to use touchscreens with a visual feedback while driving, as it distracts the driver's visual attention away from the road. Furthermore, new technologies in car cockpits modify the interactions of the users with the central system. In particular, touchscreens are preferred to arrays of buttons for space improvement and design purposes. However, the buttons' tactile feedback is no more available to the driver, which makes such interfaces more difficult to manipulate while driving. Gestures combined with an auditory feedback might therefore constitute an interesting alternative to interact with the HMI. Indeed, gestures can be performed without vision, which means that the driver's visual attention can be totally dedicated to the driving task. In fact, the auditory feedback can both inform the driver with respect to the task performed on the interface and on the performed gesture, which might constitute a possible solution to the lack of tactile information. As audition is a relatively unused sense in automotive contexts, gesture sonification can contribute to reducing the cognitive load thanks to the proposed multisensory exploitation. Our approach consists in using a virtual object (VO) to sonify the consequences of the gesture rather than the gesture itself. This approach is motivated by an ecological point of view: Gestures do not make sound, but their consequences do. In this experiment, the aim was to identify efficient sound strategies, to transmit dynamic information of VOs to users through sound. The swipe gesture was chosen for this purpose, as it is commonly used in current and new interfaces. We chose two VO parameters to sonify, the hand-VO distance and the VO velocity. Two kinds of sound parameters can be chosen to sonify the VO behavior: Spectral or temporal parameters. Pitch and brightness were tested as spectral parameters, and amplitude modulation as a temporal parameter. Performances showed a positive effect of sound compared to a no-sound situation, revealing the usefulness of sounds to accomplish the task.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/92I99QUT/Bressolette - 2017 - Sound Selection for Gesture Sonification and Manip.pdf},
  language = {en},
  number = {1}
}

@article{brufordGrooveExplorerIntelligent2019,
  title = {Groove {{Explorer}}: {{An Intelligent Visual Interface}} for {{Drum Loop Library Navigation}}},
  author = {Bruford, Fred and Barthet, Mathieu and McDonald, SKoT and Sandler, Mark},
  year = {2019},
  pages = {4},
  abstract = {Music producers nowadays rely on increasingly large libraries of loops, samples and virtual instrument sounds as part of the composition process. Intelligent interfaces are therefore useful in enabling navigation of these databases in a way that supports the production workflow. Within virtual drumming software, producers typically rely on large libraries of symbolic drum loops. Due to their large size, navigating and exploring these libraries can be a difficult process. To address this, preliminary work is presented into the Groove Explorer. Using Self-Organizing Maps, a large library of symbolic drum loops is automatically mapped on a 2D space according to rhythmic similarity. This space can then be explored via a Max/MSP prototype interface. Early results suggest that while the algorithm works well for smaller datasets, further development is required, particularly in the similarity metric used, to make the tool scalable to large libraries.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/MPY5WY54/Bruford et al. - 2019 - Groove Explorer An Intelligent Visual Interface f.pdf},
  journal = {Los Angeles},
  keywords = {music browsing},
  language = {en}
}

@misc{BullyingVictimDisability,
  title = {Bullying Victim Disability {{UK}} 2017},
  abstract = {This statistic shows self-reported bullying victims in the United Kingdom (UK) as of 2017, by disability.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FA7P4YIY/bullying-victim-disability-distribution-united-kingdom-uk.html},
  howpublished = {https://www.statista.com/statistics/474014/bullying-victim-disability-distribution-united-kingdom-uk/},
  journal = {Statista},
  language = {en}
}

@incollection{burgoyneMetaanalysisTimbrePerception2008,
  title = {A {{Meta}}-Analysis of {{Timbre Perception Using Nonlinear Extensions}} to {{CLASCAL}}},
  booktitle = {Computer {{Music Modeling}} and {{Retrieval}}. {{Sense}} of {{Sounds}}},
  author = {Burgoyne, John Ashley and McAdams, Stephen},
  editor = {{Kronland-Martinet}, Richard and Ystad, S{\o}lvi and Jensen, Kristoffer},
  year = {2008},
  volume = {4969},
  pages = {181--202},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-85035-9_12},
  abstract = {Seeking to identify the constituent parts of the multidimensional auditory attribute that musicians know as timbre, music psychologists have made extensive use of multidimensional scaling (mds), a statistical technique for visualising the geometric spaces implied by perceived dissimilarity. mds is also well known in the machine learning community, where it is used as a basic technique for dimensionality reduction. We adapt a nonlinear variant of mds that is popular in machine learning, Isomap, for use in analysing psychological data and re-analyse three earlier experiments on human perception of timbre. Isomap is designed to eliminate undesirable nonlinearities in the input data in order to reduce the overall dimensionality; our results show that it succeeds in these goals for timbre spaces, compressing the output onto well-known dimensions of timbre and highlighting the challenges inherent in quantifying differences in spectral shape.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/KY8L4KX6/Burgoyne and McAdams - 2008 - A Meta-analysis of Timbre Perception Using Nonline.pdf},
  isbn = {978-3-540-85034-2 978-3-540-85035-9},
  language = {en}
}

@article{caclinAcousticCorrelatesTimbre2005,
  title = {Acoustic Correlates of Timbre Space Dimensions: A Confirmatory Study Using Synthetic Tones.},
  shorttitle = {Acoustic Correlates of Timbre Space Dimensions},
  author = {Caclin, Anne and Mcadams, Stephen and Smith, Bennett K. and Winsberg, Suzanne},
  year = {2005},
  volume = {118},
  pages = {471--482},
  doi = {10.1121/1.1929229},
  abstract = {Timbre spaces represent the organization of perceptual distances, as measured with dissimilarity ratings, among tones equated for pitch, loudness, and perceived duration. A number of potential acoustic correlates of timbre-space dimensions have been proposed in the psychoacoustic literature, including attack time, spectral centroid, spectral flux, and spectrum fine structure. The experiments reported here were designed as direct tests of the perceptual relevance of these acoustical parameters for timbre dissimilarity judgments. Listeners presented with carefully controlled synthetic tones use attack time, spectral centroid, and spectrum fine structure in dissimilarity rating experiments. These parameters thus appear as major determinants of timbre. However, spectral flux appears as a less salient timbre parameter, its salience depending on the number of other dimensions varying concurrently in the stimulus set. Dissimilarity ratings were analyzed with two different multidimensional scaling models (CLASCAL and CONSCAL), the latter providing psychophysical functions constrained by the physical parameters. Their complementarity is discussed.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/R4MKNFSC/Caclin et al. - 2005 - Acoustic correlates of timbre space dimensions a .pdf},
  journal = {The Journal of the Acoustical Society of America},
  keywords = {Dimensions,Judgment,music perception,Population Parameter,Psychoacoustics},
  number = {1}
}

@article{caetanoMusicalInstrumentSound,
  title = {Musical {{Instrument Sound Morphing Guided}} by {{Perceptually Motivated Features}}},
  author = {Caetano, Marcelo and Rodet, Xavier},
  volume = {21},
  pages = {1666--1675},
  issn = {1558-7916},
  abstract = {\textemdash Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybrid musical},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/KR9DT96E/Musical_Instrument_Sound_Morphing_Guided_by_Perceptually_Motivated_Features.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  language = {en},
  number = {8}
}

@article{caetanoMusicalInstrumentSound2013,
  title = {Musical {{Instrument Sound Morphing Guided}} by {{Perceptually Motivated Features}}},
  author = {Caetano, Marcelo and Rodet, Xavier},
  year = {2013},
  month = aug,
  volume = {21},
  pages = {1666--1675},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2013.2260154},
  abstract = {Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybrid musical instruments. The ultimate goal of sound morphing is to perform perceptually linear transitions, which requires an appropriate model to represent the sounds being morphed and an interpolation function to obtain intermediate sounds. Typically, morphing techniques directly interpolate the parameters of the sound model without considering the perceptual impact or evaluating the results. Perceptual evaluations are cumbersome and not always conclusive. In this work, we seek parameters of a sound model that favor linear variation of perceptually motivated temporal and spectral features used to guide the morph towards more perceptually linear results. The requirement of linear variation of feature values gives rise to objective evaluation criteria for sound morphing. We investigate several spectral envelope morphing techniques to determine which spectral representation renders the most linear transformation in the spectral shape feature domain. We found that interpolation of line spectral frequencies gives the most linear spectral envelope morphs. Analogously, we study temporal envelope morphing techniques and we concluded that interpolation of cepstral coefficients results in the most linear temporal envelope morph.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/TCAU9TVM/Caetano and Rodet - 2013 - Musical Instrument Sound Morphing Guided by Percep.pdf},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  language = {en},
  number = {8}
}

@article{caetanoMusicalInstrumentSound2013a,
  title = {Musical {{Instrument Sound Morphing Guided}} by {{Perceptually Motivated Features}}},
  author = {Caetano, Marcelo and Rodet, Xavier},
  year = {2013},
  volume = {21},
  pages = {1666--1675},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  abstract = {Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybrid musical instruments. The ultimate goal of sound morphing is to perform perceptually linear transitions, which requires an appropriate model to represent the sounds being morphed and an interpolation function to obtain intermediate sounds. Typically, morphing techniques directly interpolate the parameters of the sound model without considering the perceptual impact or evaluating the results. Perceptual evaluations are cumbersome and not always conclusive. In this work, we seek parameters of a sound model that favor linear variation of perceptually motivated temporal and spectral features used to guide the morph towards more perceptually linear results. The requirement of linear variation of feature values gives rise to objective evaluation criteria for sound morphing. We investigate several spectral envelope morphing techniques to determine which spectral representation renders the most linear transformation in the spectral shape feature domain. We found that interpolation of line spectral frequencies gives the most linear spectral envelope morphs. Analogously, we study temporal envelope morphing techniques and we concluded that interpolation of cepstral coefficients results in the most linear temporal envelope morph.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Y6DFUYA8/hal-01106831.html},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  keywords = {NA},
  number = {8}
}

@article{caetanoMusicalInstrumentSound2013b,
  title = {Musical {{Instrument Sound Morphing Guided}} by {{Perceptually Motivated Features}}},
  author = {Caetano, Marcelo and Rodet, Xavier},
  year = {2013},
  volume = {8},
  pages = {1666--1675},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2013.2260154},
  abstract = {Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybrid musical instruments. The ultimate goal of sound morphing is to perform perceptually linear transitions, which requires an appropriate model to represent the sounds being morphed and an interpolation function to obtain intermediate sounds. Typically, morphing techniques directly interpolate the parameters of the sound model without considering the perceptual impact or evaluating the results. Perceptual evaluations are cumbersome and not always conclusive. In this work, we seek parameters of a sound model that favor linear variation of perceptually motivated temporal and spectral features used to guide the morph towards more perceptually linear results. The requirement of linear variation of feature values gives rise to objective evaluation criteria for sound morphing. We investigate several spectral envelope morphing techniques to determine which spectral representation renders the most linear transformation in the spectral shape feature domain. We found that interpolation of line spectral frequencies gives the most linear spectral envelope morphs. Analogously, we study temporal envelope morphing techniques and we concluded that interpolation of cepstral coefficients results in the most linear temporal envelope morph.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9BZWMCXN/bwmeta1.element.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  language = {English},
  number = {21}
}

@article{caetanoMusicalInstrumentSound2013c,
  title = {Musical {{Instrument Sound Morphing Guided}} by {{Perceptually Motivated Features}}},
  author = {Caetano, Marcelo and Rodet, Xavier},
  year = {2013},
  month = aug,
  volume = {21},
  pages = {1666--1675},
  issn = {1558-7916},
  doi = {10.1109/TASL.2013.2260154},
  abstract = {Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybrid musical instruments. The ultimate goal of sound morphing is to perform perceptually linear transitions, which requires an appropriate model to represent the sounds being morphed and an interpolation function to obtain intermediate sounds. Typically, morphing techniques directly interpolate the parameters of the sound model without considering the perceptual impact or evaluating the results. Perceptual evaluations are cumbersome and not always conclusive. In this work, we seek parameters of a sound model that favor linear variation of perceptually motivated temporal and spectral features used to guide the morph towards more perceptually linear results. The requirement of linear variation of feature values gives rise to objective evaluation criteria for sound morphing. We investigate several spectral envelope morphing techniques to determine which spectral representation renders the most linear transformation in the spectral shape feature domain. We found that interpolation of line spectral frequencies gives the most linear spectral envelope morphs. Analogously, we study temporal envelope morphing techniques and we concluded that interpolation of cepstral coefficients results in the most linear temporal envelope morph.},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {8}
}

@article{caetanoMusicalInstrumentSound2013d,
  title = {Musical {{Instrument Sound Morphing Guided}} by {{Perceptually Motivated Features}}},
  author = {Caetano, Marcelo and Rodet, Xavier},
  year = {2013},
  month = aug,
  volume = {21},
  pages = {1666--1675},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2013.2260154},
  abstract = {Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybrid musical instruments. The ultimate goal of sound morphing is to perform perceptually linear transitions, which requires an appropriate model to represent the sounds being morphed and an interpolation function to obtain intermediate sounds. Typically, morphing techniques directly interpolate the parameters of the sound model without considering the perceptual impact or evaluating the results. Perceptual evaluations are cumbersome and not always conclusive. In this work, we seek parameters of a sound model that favor linear variation of perceptually motivated temporal and spectral features used to guide the morph towards more perceptually linear results. The requirement of linear variation of feature values gives rise to objective evaluation criteria for sound morphing. We investigate several spectral envelope morphing techniques to determine which spectral representation renders the most linear transformation in the spectral shape feature domain. We found that interpolation of line spectral frequencies gives the most linear spectral envelope morphs. Analogously, we study temporal envelope morphing techniques and we concluded that interpolation of cepstral coefficients results in the most linear temporal envelope morph.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CIVNUV4M/Caetano and Rodet - 2013 - Musical Instrument Sound Morphing Guided by Percep.pdf},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  language = {en},
  number = {8}
}

@article{caetanoMusicalInstrumentSound2013e,
  title = {Musical {{Instrument Sound Morphing Guided}} by {{Perceptually Motivated Features}}},
  author = {Caetano, Marcelo and Rodet, Xavier},
  year = {2013},
  month = aug,
  volume = {21},
  pages = {1666--1675},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2013.2260154},
  abstract = {Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybrid musical instruments. The ultimate goal of sound morphing is to perform perceptually linear transitions, which requires an appropriate model to represent the sounds being morphed and an interpolation function to obtain intermediate sounds. Typically, morphing techniques directly interpolate the parameters of the sound model without considering the perceptual impact or evaluating the results. Perceptual evaluations are cumbersome and not always conclusive. In this work, we seek parameters of a sound model that favor linear variation of perceptually motivated temporal and spectral features used to guide the morph towards more perceptually linear results. The requirement of linear variation of feature values gives rise to objective evaluation criteria for sound morphing. We investigate several spectral envelope morphing techniques to determine which spectral representation renders the most linear transformation in the spectral shape feature domain. We found that interpolation of line spectral frequencies gives the most linear spectral envelope morphs. Analogously, we study temporal envelope morphing techniques and we concluded that interpolation of cepstral coefficients results in the most linear temporal envelope morph.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CAHEIHQP/Caetano and Rodet - 2013 - Musical Instrument Sound Morphing Guided by Percep.pdf},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  language = {en},
  number = {8}
}

@article{caetanoMusicalInstrumentSound2013f,
  title = {Musical {{Instrument Sound Morphing Guided}} by {{Perceptually Motivated Features}}},
  author = {Caetano, Marcelo and Rodet, Xavier},
  year = {2013},
  month = aug,
  volume = {21},
  pages = {1666--1675},
  issn = {1558-7924},
  doi = {10.1109/TASL.2013.2260154},
  abstract = {Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybrid musical instruments. The ultimate goal of sound morphing is to perform perceptually linear transitions, which requires an appropriate model to represent the sounds being morphed and an interpolation function to obtain intermediate sounds. Typically, morphing techniques directly interpolate the parameters of the sound model without considering the perceptual impact or evaluating the results. Perceptual evaluations are cumbersome and not always conclusive. In this work, we seek parameters of a sound model that favor linear variation of perceptually motivated temporal and spectral features used to guide the morph towards more perceptually linear results. The requirement of linear variation of feature values gives rise to objective evaluation criteria for sound morphing. We investigate several spectral envelope morphing techniques to determine which spectral representation renders the most linear transformation in the spectral shape feature domain. We found that interpolation of line spectral frequencies gives the most linear spectral envelope morphs. Analogously, we study temporal envelope morphing techniques and we concluded that interpolation of cepstral coefficients results in the most linear temporal envelope morph.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/VCIKJ74L/6508850.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {acoustic signal processing,Aerospace electronics,auditory illusion,Estimation,Instruments,interpolation,Interpolation,interpolation function,linear transitions,linear variation,musical instrument sound morphing,Musical instrument sounds,musical instruments,objective evaluation criteria,sound morphing,source sounds,source-filter model,spectral envelope morphing techniques,Spectral shape,spectral shape feature domain,target sounds,Timbre,timbre dimensions},
  number = {8}
}

@article{caetanoSoundMorphingFeature,
  title = {Sound Morphing by Feature Interpolation},
  author = {Caetano, Marcelo and Rodet, Xavier},
  abstract = {The goal of sound morphing by feature interpolation is to obtain sounds whose values of features are intermediate between those of the source and target sounds. In order to do this, we should be able to resynthesize sounds that present a set of},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PT2XS9AA/SOUND_MORPHING_BY_FEATURE_INTERPOLATION.html},
  journal = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  keywords = {sound morphing,timbre},
  language = {en}
}

@article{cakmakovESTIMATIONCURVESIMILARITY,
  title = {{{ESTIMATION OF CURVE SIMILARITY USING TURNING FUNCTIONS}}},
  author = {Cakmakov, Dusan and Celakoska, Emilija},
  pages = {14},
  abstract = {The process of classifying objects is a fundamental feature of most human pursuits, and the idea that people classify together those things that people find similar is both intuitive and popular across a wide range of disciplines. Estimation of difference between curves (curve matching) is an useful and often necessary technique in many applications, including: pattern recognition, image object recognition, robotic applications, computational geometry, etc.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6MRM77I7/Cakmakov and Celakoska - ESTIMATION OF CURVE SIMILARITY USING TURNING FUNCT.pdf},
  language = {en}
}

@article{camurriEyesWebGestureAffect2000,
  title = {{{EyesWeb}}: {{Toward Gesture}} and {{Affect Recognition}} in {{Interactive Dance}} and {{Music Systems}}},
  shorttitle = {{{EyesWeb}}},
  author = {Camurri, Antonio and Hashimoto, Shuji and Ricchetti, Matteo and Ricci, Andrea and Suzuki, Kenji and Trocca, Riccardo and Volpe, Gualtiero},
  year = {2000},
  volume = {24},
  pages = {57--69},
  issn = {0148-9267},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CCIKPXPW/Camurri et al. - 2000 - EyesWeb Toward Gesture and Affect Recognition in .pdf},
  journal = {Computer Music Journal},
  number = {1}
}

@article{carrGeneratingAlbumsSampleRNN2018,
  title = {Generating {{Albums}} with {{SampleRNN}} to {{Imitate Metal}}, {{Rock}}, and {{Punk Bands}}},
  author = {Carr, C. J. and Zukowski, Zack},
  year = {2018},
  month = nov,
  abstract = {This early example of neural synthesis is a proof-of-concept for how machine learning can drive new types of music software. Creating music can be as simple as specifying a set of music influences on which a model trains. We demonstrate a method for generating albums that imitate bands in experimental music genres previously unrealized by traditional synthesis techniques (e.g. additive, subtractive, FM, granular, concatenative). Raw audio is generated autoregressively in the time-domain using an unconditional SampleRNN. We create six albums this way. Artwork and song titles are also generated using materials from the original artists' back catalog as training data. We try a fully-automated method and a human-curated method. We discuss its potential for machine-assisted production.},
  archivePrefix = {arXiv},
  eprint = {1811.06633},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/QRIPKLC2/Carr and Zukowski - 2018 - Generating Albums with SampleRNN to Imitate Metal,.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/B4VYVCWQ/1811.html},
  journal = {arXiv:1811.06633 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@incollection{carthenMUSEMusicConducting2018,
  title = {{{MUSE}}: {{A Music Conducting Recognition System}}},
  shorttitle = {{{MUSE}}},
  booktitle = {Information {{Technology}} - {{New Generations}}},
  author = {Carthen, Chase D. and Kelley, Richard and Ruggieri, Cris and Dascalu, Sergiu M. and Colby, Justice and Harris, Frederick C.},
  editor = {Latifi, Shahram},
  year = {2018},
  volume = {558},
  pages = {363--369},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-54978-1_49},
  abstract = {In this paper, we introduce Music in a Universal Sound Environment(MUSE), a system for gesture recognition in the domain of musical conducting. Our system captures conductors' musical gestures to drive a MIDI-based music generation system allowing a human user to conduct a fully synthetic orchestra. Moreover, our system also aims to further improve a conductor's technique in a fun and interactive environment. We describe how our system facilitates learning through a intuitive graphical interface, and describe how we utilized techniques from machine learning and Conga, a finite state machine, to process inputs from a low cost Leap Motion sensor in which estimates the beats patterns that a conductor is suggesting through interpreting hand motions. To explore other beat detection algorithms, we also include a machine learning module that utilizes Hidden Markov Models (HMM) in order to detect the beat patterns of a conductor. An additional experiment was also conducted for future expansion of the machine learning module with Recurrent Neural Networks(rnn) and the results prove to be better than a set of HMMs. MUSE allows users to control the tempo of a virtual orchestra through basic conducting patterns used by conductors in real time. Finally, we discuss a number of ways in which our system can be used for educational and professional purposes.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GCDIK4Z8/Carthen et al. - 2018 - MUSE A Music Conducting Recognition System.pdf},
  isbn = {978-3-319-54977-4 978-3-319-54978-1},
  language = {en}
}

@misc{CasaPaganiniInfoMus,
  title = {Casa {{Paganini}} - {{InfoMus}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SV399ENX/eyesweb_ita.html},
  howpublished = {http://www.infomus.org/eyesweb\_ita.php}
}

@misc{cdcIncreasingPhysicalActivity2019,
  title = {Increasing {{Physical Activity}} among {{Adults}} with {{Disabilities}} | {{CDC}}},
  author = {CDC},
  year = {2019},
  month = sep,
  abstract = {Learn about resources and guidelines that help to increase physical activity among adults with disabilities.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/DF4CKW4D/pa.html},
  howpublished = {https://www.cdc.gov/ncbddd/disabilityandhealth/pa.html},
  journal = {Centers for Disease Control and Prevention},
  keywords = {Mobility issues},
  language = {en-us}
}

@article{chatterjeeAssessmentArtAttributes2010,
  title = {The {{Assessment}} of {{Art Attributes}}},
  author = {Chatterjee, Anjan and Widick, Page and Sternschein, Rebecca and Smith, William B. and Bromberger, Bianca},
  year = {2010},
  month = jul,
  volume = {28},
  pages = {207--222},
  publisher = {{SAGE Publications Inc}},
  issn = {0276-2374},
  doi = {10.2190/EM.28.2.f},
  abstract = {Neuropsychological investigations of art production and perception have the potential to offer critical insight into the biology of visual aesthetics. Thus far, however, investigations of art production in patients have been limited to anecdotal observations and investigations of art perception are non-existent. Progress in the field is hampered by the lack of an adequate instrument to provide basic quantification of artwork attributes. Motivated by the need to move neuropsychology of art beyond the fascinating anecdote, we present the Assessment of Art Attributes (AAA). The AAA is an instrument designed to assess six formal-perceptual and six conceptual-representational attributes using 24 paintings from the Western canon. Both artistically na\"ive and experienced participants were given the AAA. We found high degrees of agreement in the assessment of these attributes in both groups and few differences between the groups. We expect that the AAA's componential and quantitative approach will be useful in advancing neuropsychological studies as well as any investigations in which style and content of art works need to be quantified and compared.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/P64ZZS49/Chatterjee et al. - 2010 - The Assessment of Art Attributes.pdf},
  journal = {Empirical Studies of the Arts},
  keywords = {visual aesthetics},
  number = {2}
}

@inproceedings{chen2010thumbnaildj,
  title={ThumbnailDJ: Visual Thumbnails of Music Content.},
  author={Chen, Ya-Xi and Kl{\"u}ber, Ren{\'e}},
  booktitle={11th International Society for Music Information Retrieval Conference (ISMIR)},
  pages={565--570},
  year={2010}
}

@article{cherryQuantifyingCreativitySupport2014,
  title = {Quantifying the {{Creativity Support}} of {{Digital Tools}} through the {{Creativity Support Index}}},
  author = {Cherry, Erin and Latulipe, Celine},
  year = {2014},
  month = jun,
  volume = {21},
  pages = {21:1--21:25},
  issn = {1073-0516},
  doi = {10.1145/2617588},
  abstract = {Creativity support tools help people engage creatively with the world, but measuring how well a tool supports creativity is challenging since creativity is ill-defined. To this end, we developed the Creativity Support Index (CSI), which is a psychometric survey designed for evaluating the ability of a creativity support tool to assist a user engaged in creative work. The CSI measures six dimensions of creativity support: Exploration, Expressiveness, Immersion, Enjoyment, Results Worth Effort, and Collaboration. The CSI allows researchers to understand not just how well a tool supports creative work overall, but what aspects of creativity support may need attention. In this article, we present the CSI, along with scenarios for how it can be deployed in a variety of HCI research settings and how the CSI scores can help target design improvements. We also present the iterative, rigorous development and validation process used to create the CSI.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4XAVYPR2/Cherry and Latulipe - 2014 - Quantifying the Creativity Support of Digital Tool.pdf},
  journal = {ACM Transactions on Computer-Human Interaction},
  keywords = {Creativity support tools,evaluation,psychometrics,surveys,usability},
  number = {4}
}

@article{choiTutorialDeepLearning2017,
  title = {A {{Tutorial}} on {{Deep Learning}} for {{Music Information Retrieval}}},
  author = {Choi, Keunwoo and Fazekas, Gy{\"o}rgy and Cho, Kyunghyun and Sandler, Mark},
  year = {2017},
  month = sep,
  abstract = {Following their success in Computer Vision and other areas, deep learning techniques have recently become widely adopted in Music Information Retrieval (MIR) research. However, the majority of works aim to adopt and assess methods that have been shown to be effective in other domains, while there is still a great need for more original research focusing on music primarily and utilising musical knowledge and insight. The goal of this paper is to boost the interest of beginners by providing a comprehensive tutorial and reducing the barriers to entry into deep learning for MIR. We lay out the basic principles and review prominent works in this hard to navigate the field. We then outline the network structures that have been successful in MIR problems and facilitate the selection of building blocks for the problems at hand. Finally, guidelines for new tasks and some advanced topics in deep learning are discussed to stimulate new research in this fascinating field.},
  archivePrefix = {arXiv},
  eprint = {1709.04396},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/S5FX2KNY/Choi et al. - 2017 - A Tutorial on Deep Learning for Music Information .pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GP3TSFX8/1709.html},
  journal = {arXiv:1709.04396 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound,MIR Deep Learning},
  primaryClass = {cs}
}

@article{christopherKontrolHandGesture,
  title = {Kontrol: {{Hand Gesture Recognition}} for {{Music}} and {{Dance Interaction}}},
  author = {Christopher, Kameron and He, Jingyin and Kapur, Raakhi Sinha and Kapur, Ajay},
  pages = {4},
  abstract = {This paper describes Kontrol, a new hand interface that extends the intuitive control of electronic music to traditional instrumentalist and dancers. The goal of the authors has been to provide users with a device that is capable of detecting the highly intricate and expressive gestures of the master performer, in order for that information to be interpreted and used for control of electronic music. This paper discusses related devices, the architecture of Kontrol, its potential as a gesture recognition device, and several performance applications.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/HHBP47DT/Christopher et al. - Kontrol Hand Gesture Recognition for Music and Da.pdf},
  language = {en}
}

@article{conleyMetallAufMetall2008,
  title = {Metall Auf {{Metall}}: {{The Importance}} of the {{Kraftwerk Decision}} for the {{Sampling}} of {{Music}} in {{Germany Part III}}: {{Case Translation}}},
  shorttitle = {Metall Auf {{Metall}}},
  author = {Conley, Neil and Braegelmann, Tom},
  year = {2008},
  volume = {56},
  pages = {[i]-1038},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/UMFWYKBW/Conley and Braegelmann - 2008 - Metall auf Metall The Importance of the Kraftwerk.pdf},
  journal = {Journal of the Copyright Society of the U.S.A.},
  keywords = {Kraftwerk Sample},
  language = {eng},
  number = {4}
}

@incollection{cooperInmatesAreRunning1999,
  title = {The {{Inmates}} Are {{Running}} the {{Asylum}}},
  booktitle = {Software-{{Ergonomie}} '99: {{Design}} von {{Informationswelten}}},
  author = {Cooper, Alan},
  editor = {Arend, Udo and Eberleh, Edmund and Pitschke, Knut},
  year = {1999},
  pages = {17--17},
  publisher = {{Vieweg+Teubner Verlag}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-322-99786-9_1},
  abstract = {Are you an inmate? What if we switched the metaphor to, ``the building contractors are telling the architects where to put the windows?``Strike a little closer to home? The mechanics of building an application often end up taking precedence over the aims of the project, to the point where nobody\textemdash user, designer, programmer or manager\textemdash ends up getting what they want. Alan Cooper, the ``Father of Visual Basic``and author of About Face: The Essentials of User Interface Design, sees a cure for this craziness in a new way to design interaction. Applications created using his Goal-Directed \textregistered{} Design process provide users with power and pleasure. His keynote presentation will give you some much-needed perspective on design issues and show a case study of how a leading vendor has adopted Cooper's approach. He'll also offer tips on how you can make the business case for effective design to your managers. Alan is a motivating, thought-provoking, and original speaker. Come prepared to toss out some old ideas, hear some new ones and perhaps even escape from the asylum.},
  isbn = {978-3-322-99786-9},
  language = {en},
  series = {Berichte Des {{German Chapter}} of the {{ACM}}}
}

@misc{cooperInmatesAreRunning2004,
  title = {The {{Inmates Are Running}} the {{Asylum}}: {{Why High Tech Products Drive Us Crazy}} and {{How}} to {{Restore}} the {{Sanity}} (2nd {{Edition}})},
  shorttitle = {The {{Inmates Are Running}} the {{Asylum}}},
  author = {Cooper, Alan},
  year = {2004},
  abstract = {Imagine, at a terrifyingly aggressive rate, everything you regularly use is being equipped with computer technology. Think about your phone, cameras, cars-everything-being automated and programmed by people who in their rush to accept the many benefits of the silicon chip, have abdicated their responsibility to make these products easy to use. The Inmates Are Running the Asylum argues that the business executives who make the decisions to develop these products are not the ones in control of the technology used to create them. Insightful and entertaining, The Inmates Are Running the Asylum uses the author\&\#39;s experiences in corporate America to illustrate how talented people continuously design bad software-based products and why we need technology to work the way average people think. Somewhere out there is a happy medium that makes these types of products both user and bottom-line friendly; this book discusses why we need to quickly find that medium.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/EPUXTJTR/1ea2592c79f9fcd2e47c226614f45cdfa01b05eb.html},
  howpublished = {https://www.semanticscholar.org/paper/The-Inmates-Are-Running-the-Asylum\%3A-Why-High-Tech-Cooper/1ea2592c79f9fcd2e47c226614f45cdfa01b05eb},
  language = {en}
}

@article{cooperVisualizationAudioBasedMusic2006,
  title = {Visualization in {{Audio}}-{{Based Music Information Retrieval}}},
  author = {Cooper, Matthew and Foote, Jonathan and Pampalk, Elias and Tzanetakis, George},
  year = {2006},
  month = jul,
  volume = {30},
  pages = {42--62},
  issn = {1531-5169},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8FIJLC75/Cooper et al. - 2006 - Visualization in Audio-Based Music Information Ret.pdf},
  journal = {Computer Music Journal},
  language = {en},
  number = {2}
}

@article{corradiNewConceptionVisual,
  title = {A New Conception of Visual Aesthetic Sensitivity},
  author = {Corradi, Guido and Chuquichambi, Erick G. and Barrada, Juan Ram{\'o}n and Clemente, Ana and Nadal, Marcos},
  volume = {n/a},
  issn = {2044-8295},
  doi = {10.1111/bjop.12427},
  abstract = {Aesthetic sensitivity has been defined as the ability to recognize and appreciate beauty and compositional excellence, and to judge artistic merit according to standards of aesthetic value. The Visual Aesthetic Sensitivity Test (VAST) has often been used to assess this ability, but recent research has revealed it has several psychometric problems. Such problems are not easily remedied, because they reflect flawed assumptions inherent to the concept of aesthetic sensitivity as traditionally understood, and to the VAST itself. We introduce a new conception of aesthetic sensitivity defined as the extent to which someone's aesthetic valuation is influenced by a given feature. Experiment 1 aimed to characterize aesthetic sensitivity to four prominent features in visual aesthetics: complexity, symmetry, contour, and balance. Experiment 2 aimed to replicate the findings of Experiment 1 and to assess the test\textendash retest reliability of an instrument designed to measure aesthetic sensitivity to these features using an abridged set of stimuli. Our results reveal that people differ remarkably in the extent to which visual features influence their liking, highlighting the crucial role of individual variation when modelling aesthetic preferences. We did not find clear relations between the four measures of aesthetic sensitivity and personality, intelligence, and art interest and knowledge. Finally, our measurement instrument exhibited an adequate-to-good test\textendash retest reliability.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bjop.12427},
  copyright = {\textcopyright{} 2019 The British Psychological Society},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/33797YGS/Corradi et al. - A new conception of visual aesthetic sensitivity.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/QYAEZBLD/bjop.html},
  journal = {British Journal of Psychology},
  keywords = {aesthetic sensitivity,balance,complexity,contour,curvature,symmetry},
  language = {en},
  number = {n/a}
}

@article{crossPrivacyFeatureBodyWorn2020,
  title = {Privacy as a {{Feature}} for {{Body}}-{{Worn Cameras}} [{{In}} the {{Spotlight}}]},
  author = {Cross, M. S. and Cavallaro, A.},
  year = {2020},
  month = jul,
  volume = {37},
  pages = {145--148},
  issn = {1558-0792},
  doi = {10.1109/MSP.2020.2989686},
  abstract = {In this article, we discuss the threat to privacy that this passive data collection creates, along with opportunities to mitigate this risk. Furthermore, we argue that the use case of BWCs at work will stimulate the development of solutions that prevent the collection of data that could infringe upon the privacy of the wearer. Finally, we discuss the desirable properties of privacy-enhancing technologies (PETs) for BWC},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Y767IR5G/Cross and Cavallaro - 2020 - Privacy as a Feature for Body-Worn Cameras [In the.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JFQBZFUM/9127871.html},
  journal = {IEEE Signal Processing Magazine},
  keywords = {body-worn cameras,BWC,Cameras,data privacy,passive data collection,PET,Privacy,privacy-enhancing technologies,risk mitigation,Wearable computing},
  number = {4}
}

@misc{Dance,
  title = {Dance},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/D6WU5S5T/dance.html},
  howpublished = {http://erinmovement.com/dance},
  journal = {Erin Manning},
  language = {en-US}
}

@article{daviesHistorySampling1996,
  title = {A History of Sampling},
  author = {Davies, Hugh},
  year = {1996},
  month = apr,
  volume = {1},
  pages = {3--11},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S135577189600012X},
  abstract = {Since the mid-1980s commercial digital samplers have become widespread. The idea of musical instruments which have no sounds of their own is, however, much older, not just in the form of analogue samplers like the               Mellotron               , but in ancient myths and legends from China and elsewhere. This history of both digital and analogue samplers relates the latter to the early musique concr\`ete of Pierre Schaeffer and others, and also describes a variety of one-off systems devised by composers and performers.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4E7QADSQ/Davies - 1996 - A history of sampling.pdf},
  journal = {Organised Sound},
  keywords = {sampling},
  language = {en},
  number = {1}
}

@article{daviesHistorySampling1996a,
  title = {A History of Sampling},
  author = {Davies, Hugh},
  year = {1996},
  month = apr,
  volume = {1},
  pages = {3--11},
  issn = {1355-7718},
  doi = {10.1017/S135577189600012X},
  abstract = {Since the mid-1980s commercial digital samplers have become widespread. The idea of musical instruments which have no sounds of their own is, however, much older, not just in the form of analogue samplers like the Mellotron, but in ancient myths and legends from China and elsewhere. This history of both digital and analogue samplers relates the latter to the early musique concr\`ete of Pierre Schaeffer and others, and also describes a variety of one-off systems devised by composers and performers.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/KS8KUGXH/Davies - 1996 - A history of sampling.pdf},
  journal = {Organised Sound},
  number = {1}
}

@article{davisFitnessNamesDrawings1961,
  title = {The {{Fitness}} of {{Names}} to {{Drawings}}. a {{Cross}}-{{Cultural Study}} in {{Tanganyika}}},
  author = {Davis, R.},
  year = {1961},
  volume = {52},
  pages = {259--268},
  issn = {2044-8295},
  doi = {10.1111/j.2044-8295.1961.tb00788.x},
  abstract = {A set of experiments is described in which native children in the Mahali peninsula, Lake Tanganyika, were asked to match nonsense names with abstract drawings. The African children allotted names to drawings in a highly consistent manner and their choices were similar to those of a control group of English school children. It is argued that the results support the theory that purely structural similarities between sounds and shapes can be appreciated and used in naming.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8295.1961.tb00788.x},
  copyright = {1961 The British Psychological Society},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/B7PZC4I7/j.2044-8295.1961.tb00788.html},
  journal = {British Journal of Psychology},
  language = {en},
  number = {3}
}

@misc{DefinitionDisabilityEquality,
  title = {Definition of Disability under the {{Equality Act}} 2010},
  abstract = {You're disabled under the Equality Act 2010 if you have a physical or mental impairment that has a 'substantial' and 'long-term' negative effect on your ability to do daily activities},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9MVMHF23/definition-of-disability-under-equality-act-2010.html},
  howpublished = {https://www.gov.uk/definition-of-disability-under-equality-act-2010},
  journal = {GOV.UK},
  keywords = {disability},
  language = {en}
}

@article{dellemonacheEmbodiedSoundDesign2018,
  title = {Embodied Sound Design},
  author = {Delle Monache, Stefano and Rocchesso, Davide and Bevilacqua, Fr{\'e}d{\'e}ric and Lemaitre, Guillaume and Baldan, Stefano and Cera, Andrea},
  year = {2018},
  month = oct,
  volume = {118},
  pages = {47--59},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2018.05.007},
  abstract = {Embodied sound design is a process of sound creation that involves the designer's vocal apparatus and gestures. The possibilities of vocal sketching were investigated by means of an art installation. An artist\textendash designer interpreted several vocal self-portraits and rendered the corresponding synthetic sketches by using physics-based and concatenative sound synthesis. Both synthesis techniques afforded a broad range of artificial sound objects, from concrete to abstract, all derived from natural vocalisations. The vocal-to-synthetic transformation process was then automated in SEeD, a tool allowing to set and play interactively with physics- or corpus-based sound models. The voice-driven process and tool, developed and evaluated through design exercises, show how an embodied sound sketching system can work in supporting the externalisation of sonic concepts.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PWJ58Q86/Delle Monache et al. - 2018 - Embodied sound design.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/UF8J8QKE/S107158191830260X.html},
  journal = {International Journal of Human-Computer Studies},
  keywords = {Conceptual design,Sound design tool,Sound synthesis},
  language = {en}
}

@misc{DesigningEvaluatingVirtual,
  title = {Designing and Evaluating Virtual Musical Instruments: Facilitating Conversational User Interaction | {{Elsevier Enhanced Reader}}},
  shorttitle = {Designing and Evaluating Virtual Musical Instruments},
  doi = {10.1016/j.destud.2008.07.005},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CF2RT7UD/S0142694X08000665.html},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0142694X08000665?token=752EB1A2DA6E1F16B0DB25040782DCDFF35B3AF9E1476106C61EDC7905FCEEE7041A6528DD306A6000598BDE610C5AA3},
  language = {en}
}

@inproceedings{dumasDesignGuidelinesAdaptive2013,
  title = {Design Guidelines for Adaptive Multimodal Mobile Input Solutions},
  booktitle = {Proceedings of the 15th International Conference on {{Human}}-Computer Interaction with Mobile Devices and Services - {{MobileHCI}} '13},
  author = {Dumas, Bruno and Sol{\'o}rzano, Mar{\'i}a and Signer, Beat},
  year = {2013},
  pages = {285},
  publisher = {{ACM Press}},
  address = {{Munich, Germany}},
  doi = {10.1145/2493190.2493227},
  abstract = {The advent of advanced mobile devices in combination with new interaction modalities and methods for the tracking of contextual information, opens new possibilities in the field of context-aware user interface adaptation. One particular research direction is the automatic context-aware adaptation of input modalities in multimodal mobile interfaces. We present existing adaptive multimodal mobile input solutions and position them within closely related research fields. Based on a detailed analysis of the state of the art, we propose eight design guidelines for adaptive multimodal mobile input solutions. The use of these guidelines is further illustrated through the design and development of an adaptive multimodal calendar application.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/TNFTFLY5/Dumas et al. - 2013 - Design guidelines for adaptive multimodal mobile i.pdf},
  isbn = {978-1-4503-2273-7},
  keywords = {interface},
  language = {en}
}

@misc{EAREnhancedAugmented,
  title = {{{EAR}}: {{Enhanced Augmented Reality System}} for {{Sports Entertainment Applications}} {$<$} 논문상세 {$<$} 페이퍼서치},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Q7822YIK/article.html},
  howpublished = {http://www.papersearch.net/thesis/article.asp?key=3572209},
  keywords = {football ar}
}

@article{efratCurveMatchingTime,
  title = {Curve Matching, Time Warping, and Light Fields: {{New}} Algorithms for Computing Similarity between Curves},
  shorttitle = {Curve Matching, Time Warping, and Light Fields},
  author = {Efrat, Alon and Venkatasubramanian, Suresh and Fan, Quanfu},
  volume = {2007},
  abstract = {The problem of curve matching appears in many application domains, like time series analysis, shape matching, speech recognition, and signature verification, among others. Curve matching has been studied extensively by computational geometers, and many measures of similarity have been examined, among them being the Fr\'echet distance (sometimes referred in folklore as the ``dog-man '' distance). A measure that is very closely related to the Fr\'echet distance but has never been studied in a geometric context is the Dynamic Time Warping measure (DTW), first used in the context of speech recognition. This measure is ubiquitous across different domains, a surprising fact because notions of similarity usually vary significantly depending on the application. However, this measure suffers from some drawbacks, most importantly the fact that it is defined between sequences of points rather than curves. Thus, the way in which a curve is sampled to yield such a sequence can dramatically affect the quality of the result. Some attempts have been made to generalize the DTW to continuous domains, but the resulting algorithms have exponential complexity. In this paper we propose similarity measures that attempt to capture the ``spirit '' of dynamic time warping while being defined over continuous domains, and present efficient algorithms for computing them. Our formulation leads to a very interesting connection with finding short paths in a combinatorial manifold defined on the input chains, and in a deeper sense relates to the way light travels in a medium of variable refractivity. 1},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/MUPJYKJG/Efrat et al. - Curve matching, time warping, and light fields Ne.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/TZS8Y95A/summary.html},
  journal = {J. Mathematic Imaging and Vision},
  keywords = {curve matching}
}

@article{eitzSketchBasedImageRetrieval2011,
  title = {Sketch-{{Based Image Retrieval}}: {{Benchmark}} and {{Bag}}-of-{{Features Descriptors}}},
  shorttitle = {Sketch-{{Based Image Retrieval}}},
  author = {Eitz, Mathias and Hildebrand, Kristian and Boubekeur, Tamy and Alexa, Marc},
  year = {2011},
  month = nov,
  volume = {17},
  pages = {1624--1636},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2010.266},
  abstract = {We introduce a benchmark for evaluating the performance of large-scale sketch-based image retrieval systems. The necessary data are acquired in a controlled user study where subjects rate how well given sketch/image pairs match. We suggest how to use the data for evaluating the performance of sketch-based image retrieval systems. The benchmark data as well as the large image database are made publicly available for further studies of this type. Furthermore, we develop new descriptors based on the bag-of-features approach and use the benchmark to demonstrate that they significantly outperform other descriptors in the literature.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LFY6CJGN/Eitz et al. - 2011 - Sketch-Based Image Retrieval Benchmark and Bag-of.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/DYM3WKE3/5674030.html},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {bag-of-feature descriptor,benchmark data,Benchmark testing,benchmarking.,Correlation,Humans,image database,image databases,image matching,image pair matching,image retrieval,Image retrieval,Image/video retrieval,large scale sketch-based image retrieval system,performance evaluation,Shape,sketch pair matching,sketch recognition,visual databases},
  number = {11}
}

@inproceedings{ekmanUsingVocalSketching2010,
  title={Using vocal sketching for designing sonic interactions},
  author={Ekman, Inger and Rinott, Michal},
  booktitle={Proceedings of the 8th ACM conference on designing interactive systems},
  pages={123--131},
  year={2010}
}

@inproceedings{engelDDSPDifferentiableDigital2019,
  title = {{{DDSP}}: {{Differentiable Digital Signal Processing}}},
  shorttitle = {{{DDSP}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Engel, Jesse and Hantrakul, Lamtharn (Hanoi) and Gu, Chenjie and Roberts, Adam},
  year = {2019},
  month = sep,
  abstract = {Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not...},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/N8IURSYI/Engel et al. - 2019 - DDSP Differentiable Digital Signal Processing.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6XWFASUC/forum.html}
}

@inproceedings{engelDDSPDifferentiableDigital2019a,
  title = {{{DDSP}}: {{Differentiable Digital Signal Processing}}},
  shorttitle = {{{DDSP}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Engel, Jesse and Hantrakul, Lamtharn (Hanoi) and Gu, Chenjie and Roberts, Adam},
  year = {2019},
  month = sep,
  abstract = {Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not...},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PF7BGMFE/Engel et al. - 2019 - DDSP Differentiable Digital Signal Processing.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/XUI422LE/forum.html},
  keywords = {neural synthesis}
}

@article{engelDIFFERENTIABLEDIGITALSIGNAL2020,
  title = {{{DIFFERENTIABLE DIGITAL SIGNAL PROCESSING}}},
  author = {Engel, Jesse and Hantrakul, Lamtharn and Gu, Chenjie and Roberts, Adam},
  year = {2020},
  pages = {19},
  abstract = {Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library is publicly available1 and we welcome further contributions from the community and domain experts.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6H4DD56L/Engel et al. - 2020 - DIFFERENTIABLE DIGITAL SIGNAL PROCESSING.pdf},
  language = {en}
}


@article{engelnCoHEARenceAudibleShapes2020,
  title={CoHEARence of audible shapes—a qualitative user study for coherent visual audio design with resynthesized shapes},
  author={Engeln, Lars and Groh, Rainer},
  journal={Personal and Ubiquitous Computing},
  pages={1--11},
  year={2020},
  publisher={Springer}
}

@article{engelnCoHEARenceAudibleShapes2020a,
  title = {{{CoHEARence}} of Audible Shapes\textemdash a Qualitative User Study for Coherent Visual Audio Design with Resynthesized Shapes},
  author = {Engeln, Lars and Groh, Rainer},
  year = {2020},
  month = mar,
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-020-01392-5},
  abstract = {One way to achieve an intuitive sound design are visual approaches for synthesis and sound collages. Therefore, during spectral synthesis and editing, the sound is designed in a visualization of the frequency domain. In order to create a coherent workflow between visuals and the resulting audio, the stimuli should be matched to each other. In this work, a qualitative user study is presented, which is supposed to show the intuitive understanding from the shape to the sound. The shape is hereby the spectral envelope. The general aim is to find out whether there is a connection between the visual shape and the subsequent auditory impression.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/EVBV33GK/Engeln and Groh - 2020 - CoHEARence of audible shapes—a qualitative user st.pdf},
  journal = {Personal and Ubiquitous Computing},
  language = {en}
}

@inproceedings{engelnCoHEARenceQualitiveUser2019,
  title = {{{CoHEARence}}: A Qualitive {{User}}-({{Pre}}-){{Test}} on {{Resynthesized Shapes}} for Coherent Visual {{Sound Design}}},
  shorttitle = {{{CoHEARence}}},
  booktitle = {Proceedings of the 14th {{International Audio Mostly Conference}}: {{A Journey}} in {{Sound}}},
  author = {Engeln, Lars and Groh, Rainer},
  year = {2019},
  month = sep,
  pages = {98--102},
  publisher = {{Association for Computing Machinery}},
  address = {{Nottingham, United Kingdom}},
  doi = {10.1145/3356590.3356606},
  abstract = {To achieve an intuitive way of designing sound, visual approaches for synthesis or sound collages are used. In order to create a coherent workflow between visual and resulting audio, the stimuli should be matched to each other. Therefore, during spectral synthesis and editing, the sound is designed in a visualization of the frequency domain. In this work, a qualitative user pre-test is introduced, which is supposed to show the intuitive understanding from the shape to the sound. So, whether there is a connection between the visual shape and the subsequent auditory impression.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9ZUS2PXG/Engeln and Groh - 2019 - CoHEARence a qualitive User-(Pre-)Test on Resynth.pdf},
  isbn = {978-1-4503-7297-8},
  keywords = {audio-visual,coherent design,spectral editing},
  series = {{{AM}}'19}
}

@article{engelNeuralAudioSynthesis2017,
  title = {Neural {{Audio Synthesis}} of {{Musical Notes}} with {{WaveNet Autoencoders}}},
  author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
  year = {2017},
  month = apr,
  abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
  archivePrefix = {arXiv},
  eprint = {1704.01279},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/M5MRITXT/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Q7PEMQ26/1704.html},
  journal = {arXiv:1704.01279 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound,neural synthesis},
  primaryClass = {cs}
}

@inproceedings{engelnMultiTouchEnhancedVisual2018,
  title = {Multi-{{Touch Enhanced Visual Audio}}-{{Morphing}}},
  author = {Engeln, Lars and Kammer, Dietrich and Brandt, Leon and Groh, Rainer},
  year = {2018},
  month = jun,
  abstract = {Many digital interfaces for audio effects still resemble racks and cases of their hardware counterparts. For instance, DSP-algorithms are often adjusted via direct value input, sliders, or knobs. While recent research has started to experiment with the capabilities offered by modern interfaces, there are no examples for productive applications such as audio-morphing. Audio-morphing as a special field of DSP has a high complexity for the morph itself and for the parametrization of the transition between two sources. We propose a multi-touch enhanced interface for visual audio-morphing. This interface visualizes the internal processing and allows direct manipulation of the morphing parameters in the visualization. Using multi-touch gestures to manipulate audio-morphing in a visual way, sound design and music production becomes more unrestricted and creative.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/3H3L9QYT/Engeln et al. - 2018 - Multi-Touch Enhanced Visual Audio-Morphing.pdf}
}

@article{eslingBRIDGINGAUDIOANALYSIS2018,
  title = {{{BRIDGING AUDIO ANALYSIS}}, {{PERCEPTION AND SYNTHESIS WITH PERCEPTUALLY}}-{{REGULARIZED VARIATIONAL TIMBRE SPACES}}},
  author = {Esling, Philippe and Chemla, Axel and Bitton, Adrien},
  year = {2018},
  pages = {7},
  abstract = {Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/UASLE66L/Esling et al. - 2018 - BRIDGING AUDIO ANALYSIS, PERCEPTION AND SYNTHESIS .pdf},
  keywords = {music Ai},
  language = {en}
}

@article{eslingFlowSynthesizerUniversal2019,
  title = {Flow {{Synthesizer}}: {{Universal Audio Synthesizer Control}} with {{Normalizing Flows}}},
  shorttitle = {Flow {{Synthesizer}}},
  author = {Esling, Philippe and Masuda, Naotake and Bardet, Adrien and Despres, Romeo and {Chemla-Romeu-Santos}, Axel},
  year = {2019},
  volume = {10},
  pages = {302},
  issn = {2076-3417},
  doi = {10.3390/app10010302},
  abstract = {The ubiquity of sound synthesizers has reshaped modern music production, and novel music genres are now sometimes even entirely defined by their use. However, the increasing complexity and number of parameters in modern synthesizers make them extremely hard to master. Hence, the development of methods allowing to easily create and explore with synthesizers is a crucial need. Recently, we introduced a novel formulation of audio synthesizer control based on learning an organized latent audio space of the synthesizer's capabilities, while constructing an invertible mapping to the space of its parameters. We showed that this formulation allows to simultaneously address automatic parameters inference, macro-control learning, and audio-based preset exploration within a single model. We showed that this formulation can be efficiently addressed by relying on Variational Auto-Encoders (VAE) and Normalizing Flows (NF). In this paper, we extend our results by evaluating our proposal on larger sets of parameters and show its superiority in both parameter inference and audio reconstruction against various baseline models. Furthermore, we introduce disentangling flows, which allow to learn the invertible mapping between two separate latent spaces, while steering the organization of some latent dimensions to match target variation factors by splitting the objective as partial density evaluation. We show that the model disentangles the major factors of audio variations as latent dimensions, which can be directly used as macro-parameters. We also show that our model is able to learn semantic controls of a synthesizer, while smoothly mapping to its parameters. Finally, we introduce an open-source implementation of our models inside a real-time Max4Live device that is readily available to evaluate creative applications of our proposal.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/IVLGRZZC/Esling et al. - 2019 - Flow Synthesizer Universal Audio Synthesizer Cont.pdf},
  journal = {Applied Sciences},
  language = {en},
  number = {1}
}

@inproceedings{eslingFlowSynthSimplifyingComplex2020,
  title = {{{FlowSynth}}: {{Simplifying Complex Audio Generation Through Explorable Latent Spaces}} with {{Normalizing Flows}}},
  shorttitle = {{{FlowSynth}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Esling, Philippe and Masuda, Naotake and {Chemla--Romeu-Santos}, Axel},
    pages = {5273--5275},
  year = {2020},


  doi = {10.24963/ijcai.2020/767},
  abstract = {Audio synthesizers are pervasive in modern music production. These highly complex audio generation functions provide a unique diversity through their large sets of parameters. However, this feature also can make them extremely hard and obfuscated to use, especially for non-expert users with no formal knowledge on signal processing.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NCSMQJQG/Esling et al. - 2020 - FlowSynth Simplifying Complex Audio Generation Th.pdf},
  isbn = {978-0-9992411-6-5},
  language = {en}
}

@article{van2014does,
  title={Does the colour of the mug influence the taste of the coffee?},
  author={Van Doorn, George H and Wuillemin, Dianne and Spence, Charles},
  journal={Flavour},
  volume={3},
  number={1},
  pages={1--7},
  year={2014},
  publisher={Springer}
}

@article{clemente2020set,
  title={A Set of 200 musical stimuli varying in balance, contour, symmetry, and complexity: Behavioral and computational assessments},
  author={Clemente, Ana and Vila-Vidal, Manel and Pearce, Marcus T and Aguil{\'o}, Germ{\'a}n and Corradi, Guido and Nadal, Marcos},
  journal={Behavior Research Methods},
  volume={52},
  number={4},
  pages={1491--1509},
  year={2020},
  publisher={Springer}
}

@article{rousseeuw1987silhouettes,
  title={Silhouettes: a graphical aid to the interpretation and validation of cluster analysis},
  author={Rousseeuw, Peter J},
  journal={Journal of computational and applied mathematics},
  volume={20},
  pages={53--65},
  year={1987},
  publisher={Elsevier}
}

@inproceedings{eslingFlowSynthSimplifyingComplex2020a,
  title = {{{FlowSynth}}: {{Simplifying Complex Audio Generation Through Explorable Latent Spaces}} with {{Normalizing Flows}}},
  shorttitle = {{{FlowSynth}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Esling, Philippe and Masuda, Naotake and {Chemla--Romeu-Santos}, Axel},
  year = {2020},
  month = jul,
  pages = {5273--5275},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/767},
  abstract = {Audio synthesizers are pervasive in modern music production. These highly complex audio generation functions provide a unique diversity through their large sets of parameters. However, this feature also can make them extremely hard and obfuscated to use, especially for non-expert users with no formal knowledge on signal processing.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2QRJ2DSE/Esling et al. - 2020 - FlowSynth Simplifying Complex Audio Generation Th.pdf},
  isbn = {978-0-9992411-6-5},
  language = {en}
}

@misc{experience10HeuristicsUser,
  title = {10 {{Heuristics}} for {{User Interface Design}}: {{Article}} by {{Jakob Nielsen}}},
  shorttitle = {10 {{Heuristics}} for {{User Interface Design}}},
  author = {Experience, World Leaders in Research-Based User},
  abstract = {Jakob Nielsen's 10 general principles for interaction design. They are called "heuristics" because they are broad rules of thumb and not specific usability guidelines.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CJ3QZ4YN/ten-usability-heuristics.html},
  howpublished = {https://www.nngroup.com/articles/ten-usability-heuristics/},
  journal = {Nielsen Norman Group},
  language = {en}
}

@article{fazekasOverviewSemanticWeb2010,
  title = {An {{Overview}} of {{Semantic Web Activities}} in the {{OMRAS2 Project}}},
  author = {Fazekas, Gy{\"o}rgy and Raimond, Yves and Jacobson, Kurt and Sandler, Mark},
  year = {2010},
  month = dec,
  volume = {39},
  pages = {295--311},
  issn = {0929-8215, 1744-5027},
  doi = {10.1080/09298215.2010.536555},
  abstract = {The use of cultural information is becoming increasingly important in music information research, especially in music retrieval and recommendation. While this information is widely available on the Web, it is most commonly published using proprietary Web APIs. The Linked Data community is aiming at resolving the incompatibilities between these diverse data sources by building a Web of data using Semantic Web technologies. The OMRAS2 project has made several important contributions to this by developing an ontological framework and numerous software tools, as well as publishing music related data on the Semantic Web. These data and tools have found their use even beyond their originally intended scope. In this paper, we first provide a broad overview of the Semantic Web technologies underlying this work. We describe the Music Ontology, an open-ended framework for communicating musical information on the Web, and show how this framework can be extended to describe specific sub-domains such as music similarity, content-based audio features, musicological data and studio production. We describe several data-sets that have been published and data sources that have been adapted using this framework. Finally, we provide application examples ranging from software libraries to end user Web applications.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/UA2H7ZMX/Fazekas et al. - 2010 - An Overview of Semantic Web Activities in the OMRA.pdf},
  journal = {Journal of New Music Research},
  language = {en},
  number = {4}
}

@book{FeatureBasedSynthesisMapping,
  title = {Feature-{{Based Synthesis}}: {{Mapping Acoustic}} and {{Perceptual Features}} onto {{Synthesis Parameters}}},
  shorttitle = {Feature-{{Based Synthesis}}},
  abstract = {Substantial progress has been made recently in finding acoustic features that describe perceptually relevant aspects of sound. This paper presents a general framework for synthesizing audio manifesting arbitrary sets of quantifiable acoustic features of the sort used in music information retrieval and other sound analysis applications. The methods described have broad applications to the synthesis of novel musical timbres, processing by analysisresynthesis, efficient audio coding, generation of psychoacoustic stimuli, and music information retrieval research. 1},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/EBKFXQXQ/Feature-Based Synthesis Mapping Acoustic and Perc.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4A9ZHQJA/summary.html},
  keywords = {feature based synthesis}
}

@inproceedings{fiebrinkHumanModelEvaluation2011,
  title = {Human Model Evaluation in Interactive Supervised Learning},
  booktitle = {Proceedings of the 2011 Annual Conference on {{Human}} Factors in Computing Systems - {{CHI}} '11},
  author = {Fiebrink, Rebecca and Cook, Perry R. and Trueman, Dan},
  year = {2011},
  pages = {147},
  publisher = {{ACM Press}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1145/1978942.1978965},
  abstract = {Model evaluation plays a special role in interactive machine learning (IML) systems in which users rely on their assessment of a model's performance in order to determine how to improve it. A better understanding of what model criteria are important to users can therefore inform the design of user interfaces for model evaluation as well as the choice and design of learning algorithms. We present work studying the evaluation practices of end users interactively building supervised learning systems for real-world gesture analysis problems. We examine users' model evaluation criteria, which span conventionally relevant criteria such as accuracy and cost, as well as novel criteria such as unexpectedness. We observed that users employed evaluation techniques\textemdash including cross-validation and direct, real-time evaluation\textemdash not only to make relevant judgments of algorithms' performance and interactively improve the trained models, but also to learn to provide more effective training data. Furthermore, we observed that evaluation taught users about what types of models were easy or possible to build, and users sometimes used this information to modify the learning problem definition or their plans for using the trained models in practice. We discuss the implications of these findings with regard to the role of generalization accuracy in IML, the design of new algorithms and interfaces, and the scope of potential benefits of incorporating human interaction in the design of supervised learning systems.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/WFITMXSL/Fiebrink et al. - 2011 - Human model evaluation in interactive supervised l.pdf},
  isbn = {978-1-4503-0228-9},
  language = {en}
}

@inproceedings{fiocchiBeatTrackingUsing2018,
  title = {Beat {{Tracking}} Using {{Recurrent Neural Network}}: {{A Transfer Learning Approach}}},
  shorttitle = {Beat {{Tracking}} Using {{Recurrent Neural Network}}},
  booktitle = {2018 26th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Fiocchi, D. and Buccoli, M. and Zanoni, M. and Antonacci, F. and Sarti, A.},
  year = {2018},
  month = sep,
  pages = {1915--1919},
  doi = {10.23919/EUSIPCO.2018.8553059},
  abstract = {Deep learning networks have been successfully applied to solve a large number of tasks. The effectiveness of deep learning networks is limited by the amount and the variety of data used for the training. For this reason, deep-learning networks can be applied in scenarios where a huge amount of data are available. In music information retrieval, this is the case of popular genres due to the wider availability of annotated music pieces. Instead, to find sufficient and useful data is a hard task for non widespread genres, like, for instance, traditional and folk music. To address this issue, Transfer Learning has been proposed, i.e., to train a network using a large available dataset and then transfer the learned knowledge (the hierarchical representation) to another task. In this work, we propose an approach to apply transfer learning for beat tracking. We use a deep BLSTM-based RNN as the starting network trained on popular music, and we transfer it to track beats of Greek folk music. In order to evaluate the effectiveness of our approach, we collect a dataset of Greek folk music, and we manually annotate the pieces.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FDR8K8XV/8553059.html},
  keywords = {audio signal processing,beat tracking,deep learning networks,Europe,Feature extraction,Greek folk music,information retrieval,learning (artificial intelligence),Logic gates,music,recurrent neural nets,recurrent neural network,Recurrent neural networks,Task analysis,Training,transfer learning approach}
}

@inproceedings{fiocchiBeatTrackingUsing2018a,
  title = {Beat {{Tracking}} Using {{Recurrent Neural Network}}: {{A Transfer Learning Approach}}},
  shorttitle = {Beat {{Tracking}} Using {{Recurrent Neural Network}}},
  booktitle = {2018 26th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Fiocchi, D. and Buccoli, M. and Zanoni, M. and Antonacci, F. and Sarti, A.},
  year = {2018},
  month = sep,
  pages = {1915--1919},
  doi = {10.23919/EUSIPCO.2018.8553059},
  abstract = {Deep learning networks have been successfully applied to solve a large number of tasks. The effectiveness of deep learning networks is limited by the amount and the variety of data used for the training. For this reason, deep-learning networks can be applied in scenarios where a huge amount of data are available. In music information retrieval, this is the case of popular genres due to the wider availability of annotated music pieces. Instead, to find sufficient and useful data is a hard task for non widespread genres, like, for instance, traditional and folk music. To address this issue, Transfer Learning has been proposed, i.e., to train a network using a large available dataset and then transfer the learned knowledge (the hierarchical representation) to another task. In this work, we propose an approach to apply transfer learning for beat tracking. We use a deep BLSTM-based RNN as the starting network trained on popular music, and we transfer it to track beats of Greek folk music. In order to evaluate the effectiveness of our approach, we collect a dataset of Greek folk music, and we manually annotate the pieces.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/69YXGTIL/8553059.html},
  keywords = {audio signal processing,beat tracking,deep learning networks,Europe,Feature extraction,Greek folk music,information retrieval,learning (artificial intelligence),Logic gates,music,recurrent neural nets,recurrent neural network,Recurrent neural networks,Task analysis,Training,transfer learning approach}
}

@article{fiserShipShapeDrawingBeautification2015,
  title = {{{ShipShape}}: {{A Drawing Beautification Assistant}}},
  shorttitle = {{{ShipShape}}},
  author = {Fi{\v s}er, Jakub and Asente, Paul and S{\'y}kora, Daniel},
  year = {2015},
  pages = {9 pages},
  publisher = {{The Eurographics Association}},
  issn = {1812-3503},
  doi = {10.2312/EXP.20151178},
  abstract = {Sketching is one of the simplest ways to visualize ideas. Its key advantage is requiring the user to have neither deep knowledge of a particular drawing software nor any advanced drawing skills. In practice, however, all these skills become necessary to improve the visual fidelity of the resulting drawing. In this paper, we present ShipShape\textemdash a general beautification assistant that allows users to maintain the simplicity and speed of freehand sketching while still taking into account implicit geometric relations to automatically rectify the output image. In contrast to previous approaches ShipShape works with general B\'ezier curves, enables undo/redo operations, is scale independent, and is fully integrated into Adobe Illustrator. We demonstrate various results to demonstrate capabilities of the proposed method.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JTYJ7IPH/Fišer et al. - 2015 - ShipShape A Drawing Beautification Assistant.pdf},
  isbn = {9783905674903},
  journal = {Sketch-Based Interfaces and Modeling},
  keywords = {beautification,Enhancement,Geometric correction H.5.2 [User Interfaces],Graphics Utilities,I.3.3 [Computer Graphics],I.3.4 [Computer Graphics],I.3.6 [Computer Graphics],I.4.3 [Image Processing and Computer Vision],Input devices and strategies,Interaction techniques,Line and curve generation,Methodology and Techniques,Paint systems,Picture/Image Generation},
  language = {en}
}

@misc{FootballUnitedKingdom,
  title = {Football in the {{United Kingdom}} ({{UK}})},
  abstract = {Football in the United Kingdom (UK) - Get the report with graphs and tables on statista.com!},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/AVNU3L38/football-in-the-united-kingdom-uk-statista-dossier.html},
  howpublished = {https://www.statista.com/study/35351/football-in-the-united-kingdom-uk-statista-dossier/},
  journal = {Statista},
  language = {en}
}

@inproceedings{fordeInformationalVsControlling2015,
  title = {Informational vs. {{Controlling Gamification}}: {{A Study Design}}},
  shorttitle = {Informational vs. {{Controlling Gamification}}},
  booktitle = {Proceedings of the 2015 {{Annual Symposium}} on {{Computer}}-{{Human Interaction}} in {{Play}}},
  author = {Forde, Seamus F. and Mekler, Elisa D. and Opwis, Klaus},
  year = {2015},
  month = oct,
  pages = {517--522},
  publisher = {{Association for Computing Machinery}},
  address = {{London, United Kingdom}},
  doi = {10.1145/2793107.2810297},
  abstract = {Recent research suggests that gamification has the potential to increase intrinsic motivation, as well as decrease users' intrinsic motivation. However, the understanding of why gamification sometimes is successful and other times not, is still not fully understood. One reason for this is that applied research has been lacking a theoretical foundation. Therefore, we are currently designing a study in which we examine the underlying psychological mechanisms on how gamification works. Based on self determination theory, in our approach we compare how autonomy, competence and intrinsic motivation differ between an informational and a controlling condition.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/5CIRPSZX/Forde et al. - 2015 - Informational vs. Controlling Gamification A Stud.pdf},
  isbn = {978-1-4503-3466-2},
  keywords = {gamification,motivation,self-determination theory},
  series = {{{CHI PLAY}} '15}
}

@inproceedings{francoiseHierarchicalApproachDesign2012,
  title = {A {{Hierarchical Approach}} for the {{Design}} of {{Gesture}}-to-{{Sound Mappings}}},
  booktitle = {9th {{Sound}} and {{Music Computing Conference}}},
  author = {Fran{\c c}oise, Jules and Caramiaux, Baptiste and Bevilacqua, Fr{\'e}d{\'e}ric},
  year = {2012},
  month = jul,
  pages = {233--240},
  address = {{Copenhagen, Denmark}},
  abstract = {We propose a hierarchical approach for the design of gesture-to-sound mappings, with the goal to take into account multilevel time structures in both gesture and sound processes. This allows for the integration of temporal mapping strategies, complementing mapping systems based on instantaneous relationships between gesture and sound synthesis parameters. As an example, we propose the implementation of Hierarchical Hidden Markov Models to model gesture input, with a flexible structure that can be authored by the user. Moreover, some parameters can be adjusted through a learning phase. We show some examples of gesture segmentations based on this approach, considering several phases such as preparation, attack, sustain, release. Finally we describe an application, developed in Max/MSP, illustrating the use of accelerometer-based sen- sors to control phase vocoder synthesis techniques based on this approach.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/WHUFTD8P/Françoise et al. - 2012 - A Hierarchical Approach for the Design of Gesture-.pdf},
  keywords = {gesture,gesture recognition,hidden markov models,interaction,machine learning,mapping,music,music interfaces,sound}
}

@misc{FreeDownload20182018,
  title = {Free {{Download}}: 2018 {{Global Games Market Report}} by {{Newzoo}}},
  shorttitle = {Free {{Download}}},
  year = {2018},
  month = jun,
  abstract = {Gaming has evolved into an all-around entertainment phenomenon. If you add up all playing and viewing hours, gaming is the world's favorite pastime. And, with more than 2.3 billion active gamers in the world this year, this trend isn't going anywhere.~ And, for the first time, mobile gaming will contribute more than half of all \ldots},
  chapter = {Appstore},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/64TXHT5E/free-download-2018-global-games-market-report-by-newzoo.html},
  journal = {SmartNest Home Automation},
  language = {en-US}
}

@article{fridInteractiveSonificationFluid2018,
  title = {Interactive Sonification of a Fluid Dance Movement: An Exploratory Study},
  shorttitle = {Interactive Sonification of a Fluid Dance Movement},
  author = {Frid, Emma and Elblaus, Ludvig and Bresin, Roberto},
  year = {2018},
  month = nov,
  issn = {1783-7677, 1783-8738},
  doi = {10.1007/s12193-018-0278-y},
  abstract = {In this paper we present three different experiments designed to explore sound properties associated with fluid movement: (1) an experiment in which participants adjusted parameters of a sonification model developed for a fluid dance movement, (2) a vocal sketching experiment in which participants sketched sounds portraying fluid versus nonfluid movements, and (3) a workshop in which participants discussed and selected fluid versus nonfluid sounds. Consistent findings from the three experiments indicated that sounds expressing fluidity generally occupy a lower register and has less high frequency content, as well as a lower bandwidth, than sounds expressing nonfluidity. The ideal sound to express fluidity is continuous, calm, slow, pitched, reminiscent of wind, water or an acoustic musical instrument. The ideal sound to express nonfluidity is harsh, non-continuous, abrupt, dissonant, conceptually associated with metal or wood, unhuman and robotic. Findings presented in this paper can be used as design guidelines for future applications in which the movement property fluidity is to be conveyed through sonification.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8W9B7FXX/Frid et al. - 2018 - Interactive sonification of a fluid dance movement.pdf},
  journal = {Journal on Multimodal User Interfaces},
  language = {en}
}

@article{friedAudioQuilt2DArrangements,
  title = {{{AudioQuilt}}: {{2D Arrangements}} of {{Audio Samples}} Using {{Metric Learning}} and {{Kernelized Sorting}}},
  author = {Fried, Ohad and Jin, Zeyu and Finkelstein, Adam and Oda, Reid},
  pages = {6},
  abstract = {The modern musician enjoys access to a staggering number of audio samples. Composition software can ship with many gigabytes of data, and there are many more to be found online. However, conventional methods for navigating these libraries are still quite rudimentary, and often involve scrolling through alphabetical lists. We present AudioQuilt, a system for sample exploration that allows audio clips to be sorted according to user taste, and arranged in any desired 2D formation such that similar samples are located near each other. Our method relies on two advances in machine learning. First, metric learning allows the user to shape the audio feature space to match their own preferences. Second, kernelized sorting finds an optimal arrangement for the samples in 2D. We demonstrate our system with three new interfaces for exploring audio samples, and evaluate the technology qualitatively and quantitatively via a pair of user studies.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LPW75Y5F/Fried et al. - AudioQuilt 2D Arrangements of Audio Samples using.pdf},
  language = {en}
}

@article{friedCrossmodalSoundMapping,
  title = {Cross-Modal {{Sound Mapping Using Deep Learning}}},
  author = {Fried, Ohad and Fiebrink, Rebecca},
  pages = {4},
  abstract = {We present a method for automatic feature extraction and cross-modal mapping using deep learning. Our system uses stacked autoencoders to learn a layered feature representation of the data. Feature vectors from two (or more) different domains are mapped to each other, effectively creating a cross-modal mapping. Our system can either run fully unsupervised, or it can use high-level labeling to fine-tune the mapping according a user's needs. We show several applications for our method, mapping sound to or from images or gestures. We evaluate system performance both in standalone inference tasks and in cross-modal mappings.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/W8W232FU/Fried and Fiebrink - Cross-modal Sound Mapping Using Deep Learning.pdf},
  language = {en}
}

@article{garberAudioStellarOpenSource,
  title = {{{AudioStellar}}, an Open Source Corpus-Based Musical Instrument for Latent Sound Structure Discovery and Sonic Experimentation},
  author = {Garber, Leandro},
  pages = {6},
  abstract = {Generating a visual representation of short audio clips' similarities is not only useful for organizing and exploring an audio sample library but it also opens up a new range of possibilities for sonic experimentation. We present AudioStellar, an open source software that enables creative practitioners to create AI generated 2D visualizations (i.e latent space) of their own audio corpus without programming or machine learning knowledge. Sound artists can play their input corpus by interacting with this computer learned latent space using an user interface that provides built-in modes to experiment with. AudioStellar can interact with other software by MIDI syncing, sequencing, adding audio effects, and more. Creating novel forms of interaction is encouraged through OSC communication or writing custom C++ code using provided framework. AudioStellar has also proved useful as an educational strategy in courses and workshops for teaching concepts of programming, digital audio, machine learning and networks to young students in the digital art field.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/C2D2ITF6/Garber - AudioStellar, an open source corpus-based musical .pdf},
  language = {en}
}

@article{giannakisComparativeEvaluationAuditoryvisual2006,
  title = {A Comparative Evaluation of Auditory-Visual Mappings for Sound Visualisation},
  author = {Giannakis, Kostas},
  year = {2006},
  volume = {11},
  pages = {297--307},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771806001531},
  abstract = {The significant role of visual communication in modern computer applications is indisputable. In the case of music, various attempts have been made from time to time to translate non-visual ideas into visual codes (see Walters 1997 for a collection of graphic scores from the late computer music pioneer Iannis Xenakis, John Cage, Karlheinz Stockhausen, and others). In computer music research, most current sound design tools allow the direct manipulation of visual representations of sound such as time-domain and frequency-domain representations, with the most notable examples being the UPIC system (Xenakis 1992), Phonogramme (Lesbros 1996), Lemur (Fitz and Haken 1997), and MetaSynth (Wenger 1998), among others. Associations between auditory and visual dimensions have also been extensively studied in other scientific domains such as visual perception and cognitive psychology, as well as inspired new forms of artistic expression (see, for example, Wells 1980; Goldberg and Schrack 1986; Whitney 1991).},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YEVBUWWX/Giannakis - 2006 - A comparative evaluation of auditory-visual mappin.pdf},
  journal = {Organised Sound},
  language = {en},
  number = {3}
}

@phdthesis{giannakisSoundMosaicsGraphical2001,
  title = {Sound Mosaics: A Graphical User Interface for Sound Synthesis Based on Audio-Visual Associations.},
  shorttitle = {Sound Mosaics},
  author = {Giannakis, Konstantinos},
  year = {2001},
  abstract = {This thesis presents the design of a Graphical User Interface (GUI) for computer-based sound synthesis to support users in the externalisation of their musical ideas when interacting with the System in order to create and manipulate sound. The approach taken consisted of three research stages. The first stage was the formulation of a novel visualisation framework to display perceptual dimensions of sound in Visual terms. This framework was based on the findings of existing related studies and a series of empirical investigations of the associations between auditory and visual precepts that we performed for the first time in the area of computer-based sound synthesis. The results of our empirical investigations suggested associations between the colour dimensions of brightness and saturation with the auditory dimensions of pitch and loudness respectively, as well as associations between the multidimensional precepts of visual texture and timbre. The second stage of the research involved the design and implementation of Sound Mosaics, a prototype GUI for sound synthesis based on direct manipulation of visual representations that make use of the visualisation framework developed in the first stage.  We followed an iterative design approach that involved the design and evaluation of an initial Sound Mosaics prototype. The insights gained during this first iteration assisted us in revising various aspects of the original design and visualisation framework that led to a revised implementation of Sound Mosaics. The final stage of this research involved an evaluation study of the revised Sound Mosaics prototype that comprised two controlled experiments. First, a comparison experiment with the widely used frequency-domain representations of sound indicated that visual representations created with Sound Mosaics were more comprehensible and intuitive. Comprehensibility was measured as the level of accuracy in a series of sound image association tasks, while intuitiveness was related to subjects' response times and perceived levels of confidence. Second, we conducted a formative evaluation of Sound Mosaics, in which it was exposed to a number of users with and without musical background. Three usability factors were measured: effectiveness, efficiency, and subjective satisfaction. Sound Mosaics was demonstrated to perform satisfactorily in ail three factors for music subjects, although non-music subjects yielded less satisfactory results that can be primarily attributed to the subjects' unfamiliarity with the task of sound synthesis. Overall, our research has set the necessary groundwork for empirically derived and validated associations between auditory and visual dimensions that can be used in the design of cognitively useful GUIs for computer-based sound synthesis and related area.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/BCCGHGZ2/Giannakis - 2001 - Sound mosaics a graphical user interface for soun.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/3SEXLJWG/6634.html},
  language = {en},
  school = {Middlesex University}
}

@inproceedings{gilHumanguidedMachineLearning2019,
  title = {Towards Human-Guided Machine Learning},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}} - {{IUI}} '19},
  author = {Gil, Yolanda and Honaker, James and Gupta, Shikhar and Ma, Yibo and D'Orazio, Vito and Garijo, Daniel and Gadewar, Shruti and Yang, Qifan and Jahanshad, Neda},
  year = {2019},
  pages = {614--624},
  publisher = {{ACM Press}},
  address = {{Marina del Ray, California}},
  doi = {10.1145/3301275.3302324},
  abstract = {Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user's knowledge about the data available. We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/T69HBG8S/Gil et al. - 2019 - Towards human-guided machine learning.pdf},
  isbn = {978-1-4503-6272-6},
  language = {en}
}

@article{godoyImagesSonicObjects2010,
  title = {Images of {{Sonic Objects}}},
  author = {God{\o}y, Rolf Inge},
  year = {2010},
  month = apr,
  volume = {15},
  pages = {54},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771809990264},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FYQQYUZ5/Godøy - 2010 - Images of Sonic Objects.pdf},
  journal = {Organised Sound},
  language = {en},
  number = {01}
}

@misc{GraduatesUKLabour,
  title = {Graduates in the {{UK}} Labour Market - {{Office}} for {{National Statistics}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZXBQT8M9/2017.html},
  howpublished = {https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/articles/graduatesintheuklabourmarket/2017\#steady-increase-in-the-number-of-graduates-in-the-uk-over-the-past-decade}
}

@misc{GraduatesUKLaboura,
  title = {Graduates in the {{UK}} Labour Market - {{Office}} for {{National Statistics}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/V5XMYMFW/2017.html},
  howpublished = {https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/articles/graduatesintheuklabourmarket/2017},
  keywords = {graduate employment}
}

@article{gravesGeneratingSequencesRecurrent2014,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2014},
  month = jun,
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  archivePrefix = {arXiv},
  eprint = {1308.0850},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/DDBAX7UL/Graves - 2014 - Generating Sequences With Recurrent Neural Network.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/VUNVAZB4/1308.html},
  journal = {arXiv:1308.0850 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@inproceedings{grillIdentificationPerceptualQualities2011,
  title = {Identification of Perceptual Qualities in Textural Sounds Using the Repertory Grid Method},
  author = {Grill, Thomas and Flexer, Arthur and Cunningham, Stuart},
  year = {2011},
  month = sep,
  pages = {67--74},
  doi = {10.1145/2095667.2095677},
  abstract = {This paper is about exploring which perceptual qualities are relevant to people listening to textural sounds. Knowledge about those personal constructs shall eventually lead to more intuitive interfaces for browsing large sound libraries. By conducting mixed qualitative-quantitative interviews within the repertory grid framework ten bi-polar qualities are identified. A subsequent web-based study yields measures for inter-rater agreement and mutual similarity of the perceptual qualities based on a selection of 100 textural sounds. Additionally, some initial experiments are conducted to test standard audio descriptors for their correlation with the perceptual qualities.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4UDBC8MC/Grill et al. - 2011 - Identification of perceptual qualities in textural.pdf},
  keywords = {perception,timbre}
}

@inproceedings{grillVisualizationPerceptualQualities2012,
  title = {Visualization of Perceptual Qualities in Textural Sounds},
  author = {Grill, Thomas and Flexer, Arthur},
  booktitle={International Computer Music Conference (ICMC)},
  year = {2012},
  pages = {8},
  abstract = {We describe a visualization strategy that is capable of efficiently representing relevant perceptual qualities of textural sounds. The general aim is to develop intuitive screenbased interfaces representing large collections of sounds, where sound retrieval shall be much facilitated by the exploitation of cross-modal mechanisms of human perception. We propose the use of metaphoric sensory properties that are shared between sounds and graphics, constructing a meaningful mapping of auditory to visual dimensions. For this purpose, we have implemented a visualization using tiled maps, essentially combining low-dimensional projection and iconic representation. To prove the suitability we show detailed results of experiments having been conducted in the form of an online survey. Potential future use in music creation is illustrated by a prototype sound browser application.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6CPM7ED7/Grill and Flexer - 1929 - VISUALIZATION OF PERCEPTUAL QUALITIES IN TEXTURAL .pdf},
  journal = {. As},
  keywords = {perception,visualisation},
  language = {en}
}

@article{grueProblemInspirationPorn2016,
  title = {The Problem with Inspiration Porn: A Tentative Definition and a Provisional Critique},
  shorttitle = {The Problem with Inspiration Porn},
  author = {Grue, Jan},
  year = {2016},
  month = jul,
  volume = {31},
  pages = {838--849},
  publisher = {{Routledge}},
  issn = {0968-7599},
  doi = {10.1080/09687599.2016.1205473},
  abstract = {The term `inspiration porn' is associated with disability advocacy in general and the late activist and comedian Stella Young in particular. It has come into widespread usage over the last few years. I propose the following definition: `Inspiration porn is the representation of disability as a desirable but undesired characteristic, usually by showing impairment as a visually or symbolically distinct biophysical deficit in one person, a deficit that can and must be overcome through the display of physical prowess.' Inspiration porn superficially appears linked only to medical model/personal tragedy framings of disability, but on closer inspection resonates strongly with the disability movement's advocacy of empowerment and affirmation.},
  annotation = {\_eprint: https://doi.org/10.1080/09687599.2016.1205473},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RZUA4JA4/Grue - 2016 - The problem with inspiration porn a tentative def.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/HI7XR5GX/09687599.2016.html},
  journal = {Disability \& Society},
  keywords = {affirmation,deficit,inspiration porn,Inspiration porn,representation,stigma,symbol},
  number = {6}
}

@inproceedings{hamariDoesGamificationWork2014,
  title = {Does {{Gamification Work}}? \textendash{} {{A Literature Review}} of {{Empirical Studies}} on {{Gamification}}},
  shorttitle = {Does {{Gamification Work}}?},
  booktitle = {2014 47th {{Hawaii International Conference}} on {{System Sciences}}},
  author = {Hamari, Juho and Koivisto, Jonna and Sarsa, Harri},
  year = {2014},
  month = jan,
  pages = {3025--3034},
  issn = {1530-1605},
  doi = {10.1109/HICSS.2014.377},
  abstract = {This paper reviews peer-reviewed empirical studies on gamification. We create a framework for examining the effects of gamification by drawing from the definitions of gamification and the discussion on motivational affordances. The literature review covers results, independent variables (examined motivational affordances), dependent variables (examined psychological/behavioral outcomes from gamification), the contexts of gamification, and types of studies performed on the gamified systems. The paper examines the state of current research on the topic and points out gaps in existing literature. The review indicates that gamification provides positive effects, however, the effects are greatly dependent on the context in which the gamification is being implemented, as well as on the users using it. The findings of the review provide insight for further studies as well as for the design of gamified systems.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2Y3V4SCH/Hamari et al. - 2014 - Does Gamification Work – A Literature Review of E.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/X3PCJCZ6/6758978.html},
  keywords = {behavioural sciences,Context,Databases,dependent variables,Educational institutions,empirical studies,game theory,Games,gamification,gamification work,gamified systems,HCI,independent variables,Libraries,literature review,motivation,motivational affordance,persuasive technology,psychological-behavioral outcomes,Psychology}
}

@article{haNeuralRepresentationSketch2017,
  title = {A {{Neural Representation}} of {{Sketch Drawings}}},
  author = {Ha, David and Eck, Douglas},
  year = {2017},
  month = may,
  abstract = {We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.},
  archivePrefix = {arXiv},
  eprint = {1704.03477},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/25PTGG6U/Ha and Eck - 2017 - A Neural Representation of Sketch Drawings.pdf},
  journal = {arXiv:1704.03477 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,neural network,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{harmsGamificationOnlineSurveys2015,
  title={Gamification of online surveys: Design process, case study, and evaluation},
  author={Harms, Johannes and Biegler, Stefan and Wimmer, Christoph and Kappel, Karin and Grechenig, Thomas},
  booktitle={IFIP Conference on Human-Computer Interaction},
  pages={219--236},
  year={2015},
  organization={Springer}
}

@inproceedings{harrimanSolidNoiseMakingMusical2016,
  title = {{{SolidNoise}}: {{Making Musical Robots}}},
  shorttitle = {{{SolidNoise}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}} - {{CHI EA}} '16},
  author = {Harriman, Jiffer and Bethancourt, Matthew and Narula, Abhishek and Theodore, Michael and Gross, Mark},
  year = {2016},
  pages = {2504--2510},
  publisher = {{ACM Press}},
  address = {{Santa Clara, California, USA}},
  doi = {10.1145/2851581.2892539},
  abstract = {This late breaking work submission describes the development of tools and techniques aimed to simplify the development and use of musical robots. We describe these tools and techniques as utilized to produce an event known as SolidNoise. The event showcased a series of automated instruments and musical compositions created for the robotic ensemble. Our developments are motivated by historical examples of automated instruments and our vision for musical robots in the future.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JI7BKM3I/Harriman et al. - 2016 - SolidNoise Making Musical Robots.pdf},
  isbn = {978-1-4503-4082-3},
  language = {en}
}

@inproceedings{heiseSoundTorchQuickBrowsing2008,
  title = {{{SoundTorch}}: {{Quick Browsing}} in {{Large Audio Collections}}},
  shorttitle = {{{SoundTorch}}},
  author = {Heise, Sebastian and Hlatky, Michael and Loviscach, J{\"o}rn},
  year = {2008},
  abstract = {Musicians, sound engineers, and foley artists face the challenge of finding appropriate sounds in vast collections containing thousands of audio files. Imprecise naming and tagging forces users to review dozens of files in order to pick the right sound. Acoustic matching is not necessarily helpful here as it needs a sound exemplar to match with and may miss relevant files. Hence, we propose to combine acoustic content analysis with accelerated auditioning: Audio files are automatically arranged in 2D by psychoacoustic similarity. A user can shine a virtual flashlight onto this representation; all sounds in the light cone are played back simultaneously, their position indicated through surround sound. User tests show that this method can leverage the human brain's capability to single out sounds from a spatial mixture and enhance browsing in large collections of audio content.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZDS3B5IU/Heise et al. - 2008 - SoundTorch Quick Browsing in Large Audio Collecti.pdf},
  keywords = {Acoustic cryptanalysis,Audio Media,Audio signal processing,Browsing,Collections (publication),Engineering,Psychoacoustics,Surround sound}
}

@inproceedings{helenQueryExampleAudio2007,
  title = {Query by {{Example}} of {{Audio Signals}} Using {{Euclidean Distance Between Gaussian Mixture Models}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Helen, Marko and Virtanen, Tuomas},
  year = {2007},
  month = apr,
  pages = {I-225-I-228},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/ICASSP.2007.366657},
  abstract = {Query by example of multimedia signals aims at automatic retrieval of media samples from a database, which are similar to a userprovided example. This paper proposes a method for query by example of audio signals. The method calculates a set of acoustic features from the signals and models their probability density functions (pdfs) using Gaussian mixture models. The method measures the similarity between two samples using the Euclidian distance between their pdfs. A novel method for calculating the closed form solution of the distance is proposed. Simulation experiments show that proposed method enables higher retrieval accuracy than the reference methods.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/XMKFD8IT/Helen and Virtanen - 2007 - Query by Example of Audio Signals using Euclidean .pdf},
  isbn = {978-1-4244-0727-9},
  language = {en}
}

@article{henzeFreehandGesturesMusic,
  title = {Free-Hand Gestures for Music Playback: Deriving Gestures with a User-Centred Process},
  author = {Henze, Niels and L{\"o}cken, Andreas and Boll, Susanne and Hesselmann, Tobias and Pielot, Martin},
  pages = {10},
  abstract = {Music is a fundamental part of most cultures. Controlling music playback has commonly been used to demonstrate new interaction techniques and algorithm. In particular, controlling music playback has been used to demonstrate and evaluate gesture recognition algorithms. Previous work, however, used gestures that have been defined based on intuition, the developers' preferences, and the respective algorithm's capabilities. In this paper we propose a refined process for deriving gestures from constant user feedback. Along this process a set of free-hand gestures for controlling music playback is developed. The situational context is analyzed to shape the usage scenario and derive an initial set of necessary functions. In a successive user study the set of functions is validated. Furthermore, proposals for gestures are collected from the participants for each function. Two gesture sets containing static and dynamic gestures are derived and analyzed in a comparative evaluation. The evaluation shows that we developed an appropriate set of free-hand gestures for music playback. Our results indicate that the proposed process, that includes validation of each design decision, improves the final results.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/EPXHP5FJ/Henze et al. - Free-hand gestures for music playback deriving ge.pdf},
  keywords = {gesture},
  language = {en}
}

@inproceedings{hersheyApproximatingKullbackLeibler2007,
  title = {Approximating the {{Kullback Leibler Divergence Between Gaussian Mixture Models}}},
  booktitle = {Proceedings of {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), Vol 4},
  author = {Hershey, John and Olsen, Peder},
  year = {2007},
  month = may,
  volume = {4},
  pages = {IV-317},
  doi = {10.1109/ICASSP.2007.366913},
  abstract = {The Kullback Leibler (KL) divergence is a widely used tool in statistics and pattern recognition. The KL divergence between two Gaussian mixture models (GMMs) is frequently needed in the fields of speech and image recognition. Unfortunately the KL divergence between two GMMs is not analytically tractable, nor does any efficient computational algorithm exist. Some techniques cope with this problem by replacing the KL divergence with other functions that can be computed efficiently. We introduce two new methods, the variational approximation and the variational upper bound, and compare them to existing methods. We discuss seven different techniques in total and weigh the benefits of each one against the others. To conclude we evaluate the performance of each one through numerical experiments},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9XETN6ZF/Hershey and Olsen - 2007 - Approximating the Kullback Leibler Divergence Betw.pdf},
  isbn = {978-1-4244-0728-6}
}

@inproceedings{hersheyApproximatingKullbackLeibler2007a,
  title = {Approximating the {{Kullback Leibler Divergence Between Gaussian Mixture Models}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Hershey, John R. and Olsen, Peder A.},
  year = {2007},
  month = apr,
  volume = {4},
  pages = {IV-317-IV-320},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2007.366913},
  abstract = {The Kullback Leibler (KL) divergence is a widely used tool in statistics and pattern recognition. The KL divergence between two Gaussian mixture models (GMMs) is frequently needed in the fields of speech and image recognition. Unfortunately the KL divergence between two GMMs is not analytically tractable, nor does any efficient computational algorithm exist. Some techniques cope with this problem by replacing the KL divergence with other functions that can be computed efficiently. We introduce two new methods, the variational approximation and the variational upper bound, and compare them to existing methods. We discuss seven different techniques in total and weigh the benefits of each one against the others. To conclude we evaluate the performance of each one through numerical experiments.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/U7EKD4DE/Hershey and Olsen - 2007 - Approximating the Kullback Leibler Divergence Betw.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/WHNN4GFP/4218101.html},
  keywords = {Algorithm design and analysis,Entropy,gaussian mixture models,Gaussian mixture models,image recognition,Image recognition,Kullback Leibler divergence,Monte Carlo methods,Monte Carlo sampling,pattern recognition,Pattern recognition,Probability density function,Speech,speech recognition,Statistical distributions,Statistics,unscented transformation,Upper bound,variational methods}
}

@article{hinderksDevelopingUXKPI2019,
  title = {Developing a {{UX KPI}} Based on the User Experience Questionnaire},
  author = {Hinderks, Andreas and Schrepp, Martin and Dom{\'i}nguez Mayo, Francisco Jos{\'e} and Escalona, Mar{\'i}a Jos{\'e} and Thomaschewski, J{\"o}rg},
  year = {2019},
  month = jul,
  volume = {65},
  pages = {38--44},
  issn = {09205489},
  doi = {10.1016/j.csi.2019.01.007},
  abstract = {Decisions in Companies are made typically by using a number of entirely different key figures. A user experience key figure is one of many important key figures that represents one aspect of the success of the company or its products. What we aim in this article is to present to those responsible for a product a method of how a user experience key performance indicator (UX KPI) can be developed using a UX questionnaire. We have developed a UX KPI for use in organizations based on the User Experience Questionnaire (UEQ). To achieve this, we added six questions to the UEQ to measure the importance of the UEQ scales. Based on the UEQ scales and the scores given for importance, we then developed a User Experience Questionnaire KPI (UEQ KPI). In a first study with 882 participants, we calculated and discussed the UEQ KPI using Amazon and Skype. The results show that the six supplementary questions could be answered independently of the UEQ itself. In our opinion, the extension can be implemented without any problems. The resulting UEQ KPI can be used for communication within an organization as a key performance indicator.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/3VQ95NP8/Hinderks et al. - 2019 - Developing a UX KPI based on the user experience q.pdf},
  journal = {Computer Standards \& Interfaces},
  keywords = {User Experience Questionaire},
  language = {en}
}

@article{hochenbaumBricktableMusicalTangible,
  title = {Bricktable: {{A Musical Tangible Multi}}-{{Touch Interface}}},
  author = {Hochenbaum, Jordan and Vallis, Owen},
  pages = {8},
  abstract = {This paper describes how the development of a tangible multi-touch surface, Bricktable, was made possible through the use of open-source communities and tools. Subsequent work researching the musical applications of multitouch interfaces has allowed the Bricktable to now become a resource for others. Several Bricktable projects are provided as examples.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ASFW8ESJ/Hochenbaum and Vallis - Bricktable A Musical Tangible Multi-Touch Interfa.pdf},
  language = {en}
}

@inproceedings{hoffmanFeatureBasedSynthesisMapping,
  title={The Featsynth framework for feature-based synthesis: Design and applications},
  author={Hoffman, Matthew and Cook, Perry R},
  booktitle={ICMC},
  year={2007}
}

@incollection{houdeChapter16What1997,
  title = {Chapter 16 - {{What}} Do {{Prototypes Prototype}}?},
  booktitle = {Handbook of {{Human}}-{{Computer Interaction}} ({{Second Edition}})},
  author = {Houde, Stephanie and Hill, Charles},
  editor = {Helander, Marting G. and Landauer, Thomas K. and Prabhu, Prasad V.},
  year = {1997},
  month = jan,
  pages = {367--381},
  publisher = {{North-Holland}},
  address = {{Amsterdam}},
  doi = {10.1016/B978-044481862-1.50082-0},
  abstract = {Prototypes are widely recognized to be a core means of exploring and expressing designs for interactive computer artifacts. Choosing the right kind of more focused prototype to build is an art in itself, and communicating its limited purposes to its various audiences is a critical aspect of its use. Current terminology for describing prototypes centers on attributes of prototypes themselves that can be distracting. Tools can be used in many different ways, and detail is not a sure indicator of completion. This chapter proposes a change in the language used to talk about prototypes, to focus more on fundamental questions about the interactive system being designed. The goal of this chapter is to establish a model that describes any prototype in terms of the artifact being designed rather than the prototype's incidental attributes. By focusing on the purpose of the prototype\textemdash that is, what it prototypes\textemdash better decisions can be made about the kinds of prototypes to build. With a clear purpose for each prototype, prototypes can be better used to think and communicate about design. This chapter begins by describing current difficulties in communicating about prototypes: the complexity of interactive systems; issues of multi-disciplinary teamwork; and the audiences of prototypes. The chapter introduces the model and illustrates it with some initial examples of prototypes from real projects. The chapter concludes with a summary of the main implications of the model for prototyping practice.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GIM6KQXB/Houde and Hill - 1997 - Chapter 16 - What do Prototypes Prototype.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZY78N5JF/B9780444818621500820.html},
  isbn = {978-0-444-81862-1},
  language = {en}
}

@inproceedings{houixInnovativeToolsSound2016,
  title = {Innovative {{Tools}} for {{Sound Sketching Combining Vocalizations}} and {{Gestures}}},
  booktitle = {Proceedings of the {{Audio Mostly}} 2016 on - {{AM}} '16},
  author = {Houix, Olivier and Monache, Stefano Delle and Lachambre, H{\'e}l{\`e}ne and Bevilacqua, Fr{\'e}d{\'e}ric and Rocchesso, Davide and Lemaitre, Guillaume},
  year = {2016},
  pages = {12--19},
  publisher = {{ACM Press}},
  address = {{Norrkoping, Sweden}},
  doi = {10.1145/2986416.2986442},
  abstract = {Designers are used to produce a variety of physical and digital representations at different stages of the design process. These intermediary objects (IOs) do support the externalization of ideas and the mediation with the different stakeholders. In the same manner, sound designers deliver several intermediate sounds to their clients, through iteration and refinement. In fact, these preliminary sounds are sound sketches representing the intermediate steps of an evolving creation. In this paper we reflect on the method of sketching sounds through vocalizations and gestures, and how a technological support, grounded in the understanding of the design practice, can foster transparency and mediation in sound design-thinking. Three tools, under development in the scope of the EU project SkAT-VG (Sketching Audio Technologies using Vocalizations and Gestures) are introduced and discussed, based on the preliminary observations collected during a workshop involving professional sound designers.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2YRFYZ53/Houix et al. - 2016 - Innovative Tools for Sound Sketching Combining Voc.pdf},
  isbn = {978-1-4503-4822-5},
  language = {en}
}

@inproceedings{hsuKinesonicApproachesMapping2015,
  title = {Kinesonic Approaches to Mapping Movement and Music with the Remote Electroacoustic Kinesthetic Sensing ({{RAKS}}) System},
  booktitle = {Proceedings of the 2nd {{International Workshop}} on {{Movement}} and {{Computing}} - {{MOCO}} '15},
  author = {Hsu, Aurie and Kemper, Steven},
  year = {2015},
  pages = {45--47},
  publisher = {{ACM Press}},
  address = {{Vancouver, British Columbia, Canada}},
  doi = {10.1145/2790994.2791020},
  abstract = {Sensor technologies allow for a direct link between a dancer's kinetic and kinesthetic experience and musical expression. The Remote electroAcoustic Kinesthetic Sensing (RAKS) system, a wearable wireless sensor interface designed specifically for belly dance, enables such a link. Teka Mori (2013) for belly dancer, RAKS system, and computer-generated sound, explores a kinesonic approach to mapping belly dance movement to timbral control in electronic music.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7I7K48TS/Hsu and Kemper - 2015 - Kinesonic approaches to mapping movement and music.pdf},
  isbn = {978-1-4503-3457-0},
  language = {en}
}

@article{huangLocalBinaryPatterns2011,
  title = {Local {{Binary Patterns}} and {{Its Application}} to {{Facial Image Analysis}}: {{A Survey}}},
  shorttitle = {Local {{Binary Patterns}} and {{Its Application}} to {{Facial Image Analysis}}},
  author = {Huang, Di and Shan, Caifeng and Ardabilian, Mohsen and Wang, Yunhong and Chen, Liming},
  year = {2011},
  month = nov,
  volume = {41},
  pages = {765--781},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2011.2118750},
  abstract = {Local Binary Patterns (LBP) is a non-parametric descriptor whose aim is to efficiently summarize the local structures of images. In recent years, it has aroused increasing interest in many areas of image processing and computer vision, and has shown its effectiveness in a number of applications, in particular for facial image analysis, including tasks as diverse as face detection, face recognition, facial expression analysis, demographic classification, etc. This paper presents a comprehensive survey of LBP methodology including several more recent variations. As a typical application of the LBP approach, LBP-based facial image analysis is extensively reviewed, while its successful extensions in dealing with various tasks of facial image analysis are also highlighted.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Q9GZM2JE/Huang et al. - 2011 - Local Binary Patterns and Its Application to Facia.pdf},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  keywords = {LBP},
  language = {en},
  number = {6}
}

@book{huntExpertManipulation,
  title = {Expert {{Manipulation}}},
  author = {Hunt, Andy and W, Marcelo M. and Kirk, Ross},
  abstract = {This paper reviews models of the ways in which performer instrumental actions can be linked to sound synthesis parameters. We analyse available literature on both acoustical instrument simulation and mapping of input devices to sound synthesis in general human-computer interaction. We further demonstrate why a more complex mapping strategy is required to maximise human performance possibilities in expert manipulation situations by showing clear measurements of user performance improvement over time. We finally discuss a general model for instrumental mapping, by separating the mapping layer into two independent parts. This model allows the expressive use of different input devices within the same architecture, or conversely, the use of different synthesis algorithms, by only changing one part of the mapping layer.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LBBQ7H29/Hunt et al. - Expert Manipulation.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NIQLCMFN/summary.html}
}

@misc{IEEEXploreDigital,
  title = {{{IEEE Xplore Digital Library}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/5QLZZKKT/home.html},
  howpublished = {https://ieeexplore.ieee.org/Xplore/home.jsp}
}

@article{iezzoniMobilityDifficultiesAre2001,
  title = {Mobility {{Difficulties Are Not Only}} a {{Problem}} of {{Old Age}}},
  author = {Iezzoni, Lisa I and McCarthy, Ellen P and Davis, Roger B and Siebens, Hilary},
  year = {2001},
  month = apr,
  volume = {16},
  pages = {235--243},
  issn = {0884-8734},
  doi = {10.1046/j.1525-1497.2001.016004235.x},
  abstract = {BACKGROUND Lower extremity mobility difficulties often result from common medical conditions and can disrupt both physical and emotional well-being. OBJECTIVES To assess the national prevalence of mobility difficulties among noninstitutionalized adults and to examine associations with demographic characteristics and other physical and mental health problems. DESIGN Cross-sectional survey using the 1994\textendash 1995 National Health Interview Survey-Disability Supplement (NHIS-D). We constructed measures of minor, moderate, and major lower extremity mobility difficulties using questions about ability to walk, climb stairs, and stand, and use of mobility aids (e.g., canes, wheelchairs). Age and gender adjustment used direct standardization methods in Software for the Statistical Analysis of Correlated Data (SUDAAN). PARTICIPANTS Noninstitutionalized, civilian U.S. residents aged 18 years and older. National Health Interview Survey sampling weights with SUDAAN provided nationally representative population estimates. RESULTS An estimated 19 million people (10.1\%) reported some mobility difficulty. The mean age of those with minor, moderate, or major difficulty ranged from 59 to 67 years. Of those reporting major difficulties, 32\% said their problems began at aged 50 years or younger. Adjusted problem rates were higher among women (11.8\%) than men (8.8\%), and higher among African American (15.0\%) than whites (10.0\%). Persons with mobility difficulties were more likely to be poorly educated, living alone, impoverished, obese, and having problems conducting daily activities. Among persons with major mobility difficulties, 30.6\% reported being frequently depressed or anxious, compared to 3.8\% for persons without mobility difficulties. CONCLUSIONS Reports of mobility difficulties are common, including among middle-aged adults. Associations with poor performance of daily activities, depression, anxiety, and poverty highlight the need for comprehensive care for persons with mobility problems.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/N5TXI6RD/Iezzoni et al. - 2001 - Mobility Difficulties Are Not Only a Problem of Ol.pdf},
  journal = {Journal of General Internal Medicine},
  keywords = {mobility issues},
  number = {4},
  pmcid = {PMC1495195},
  pmid = {11318924}
}

@article{iversonIsolatingDynamicAttributes1993,
  title = {Isolating the Dynamic Attributes of Musical Timbre {\textsuperscript{a)}}},
  author = {Iverson, Paul and Krumhansl, Carol L.},
  year = {1993},
  month = nov,
  volume = {94},
  pages = {2595--2603},
  issn = {0001-4966},
  doi = {10.1121/1.407371},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6KV2A5CJ/Iverson and Krumhansl - 1993 - Isolating the dynamic attributes of musical timbre.pdf},
  journal = {The Journal of the Acoustical Society of America},
  keywords = {music perception},
  language = {en},
  number = {5}
}

@article{jacobsenAestheticJudgmentsNovel2002,
  title = {Aesthetic {{Judgments}} of {{Novel Graphic Patterns}}: {{Analyses}} of {{Individual Judgments}}},
  shorttitle = {Aesthetic {{Judgments}} of {{Novel Graphic Patterns}}},
  author = {Jacobsen, Thomas and H{\"o}fel, Lea},
  year = {2002},
  month = dec,
  volume = {95},
  pages = {755--766},
  publisher = {{SAGE Publications Inc}},
  issn = {0031-5125},
  doi = {10.2466/pms.2002.95.3.755},
  abstract = {Aesthetic judgments were investigated using a combined nomothetic and idiographic approach. Participants judged novel graphic patterns with respect to their own personal definitions of ``beauty'' Judgment analysis was employed to derive individual case models of judgment strategies as well as a group model. As predicted, symmetry had the highest correlations with aesthetic judgments of beauty. Stimulus complexity was the second-highest correlate of a positive evaluation. Thus, there was agreement at the group level. The judgment analyses, however, indicated substantial individual differences. These included use of symmetry or complexity cues that were contrary to the main group use, e.g., a few participants considered nonsymmetric patterns more beautiful. These findings suggest that exclusive consideration of the group model would have leveled the individual differences and been misleading. The group model is significant; however, the individual judgment analyses represent individual patterns of judgment in a notedly more accurate way.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GZUGEUDS/Jacobsen and Höfel - 2002 - Aesthetic Judgments of Novel Graphic Patterns Ana.pdf},
  journal = {Perceptual and Motor Skills},
  keywords = {shape preference},
  language = {en},
  number = {3}
}

@article{jenseniusActionsoundDevelopingMethods2007,
  title = {Action-Sound : Developing Methods and Tools to Study Music-Related Body Movement},
  shorttitle = {Action-Sound},
  author = {Jensenius, Alexander Refsum},
  year = {2007},
  abstract = {Body movement is integral to both performance and perception of music, and this dissertation suggests that we also think about music as movement. Based on ideas of embodied music cognition, it is argued that ecological knowledge of action-sound couplings guide our experience of music, both in perception and performance. Then follows a taxonomy of music-related body movements, before various observation studies of perceiver's music-movement correspondences are presented. Knowledge from the observation studies was used in the exploration of artificial action-sound relationships through the development of various prototype music controllers. The last part of the dissertation presents tools and methods that have been developed throughout the project, including the Musical Gestures Toolbox; techniques for creating motion history images and motiongrams of video material; and development of the Gesture Description Interchange Format (GDIF) for streaming and storing music-related movement data. These tools may be seen as an answer to many of the research questions posed in the dissertation, and have facilitated the analysis of music-related movement and creation of artificial action-sound relationships in the project.{$<$}br{$><$}br{$>$}    The zip-file contains support material for the dissertation, including sound and video examples, software, and source code.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CID8HY9V/27149.html},
  language = {eng}
}

@article{jeonRitualsMagicInteractive2019,
  title = {From Rituals to Magic: {{Interactive}} Art and {{HCI}} of the Past, Present, and Future},
  shorttitle = {From Rituals to Magic},
  author = {Jeon, Myounghoon and Rebecca Fiebrink and Edmonds, Ernest A. and Herath, Damith},
  year = {2019},
  month = nov,
  volume = {131},
  pages = {108--119},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2019.06.005},
  abstract = {The connection between art and technology is much tighter than is commonly recognized. The emergence of aesthetic computing in the early 2000s has brought renewed focus on this relationship. In this article, we articulate how art and Human\textendash Computer Interaction (HCI) are compatible with each other and actually essential to advance each other in this era, by briefly addressing interconnected components in both areas\textemdash interaction, creativity, embodiment, affect, and presence. After briefly introducing the history of interactive art, we discuss how art and HCI can contribute to one another by illustrating contemporary examples of art in immersive environments, robotic art, and machine intelligence in art. Then, we identify challenges and opportunities for collaborative efforts between art and HCI. Finally, we reiterate important implications and pose future directions. This article is intended as a catalyst to facilitate discussions on the mutual benefits of working together in the art and HCI communities. It also aims to provide artists and researchers in this domain with suggestions about where to go next.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SZATXSFA/Jeon et al. - 2019 - From rituals to magic Interactive art and HCI of .pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6FKLB8H8/S1071581919300758.html},
  journal = {International Journal of Human-Computer Studies},
  keywords = {Aesthetic computing,Computational creativity,Embodied interaction,History of HCI and Interactive Art,Interactive art,Robotic art},
  series = {50 Years of the {{International Journal}} of {{Human}}-{{Computer Studies}}. {{Reflections}} on the Past, Present and Future of Human-Centred Technologies}
}

@article{johnstonFluidSimulationFull,
  title = {Fluid {{Simulation}} as {{Full Body Audio}}-{{Visual Instrument}}},
  author = {Johnston, Andrew},
  pages = {4},
  abstract = {This paper describes an audio-visual performance system based on real-time fluid simulation. The aim is to provide a rich environment for works which blur the boundaries between dance and instrumental performance \textendash{} and sound and visuals \textendash{} while maintaining transparency for audiences and new performers. The system uses infra-red motion tracking to allow performers to manipulate a real-time fluid simulation, which in turn provides control data for computergenerated audio and visuals. It also provides a control and configuration system which allows the behaviour of the interactive system to be changed over time, enabling the structure within which interactions take place to be `composed'.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4BU2VFDE/Johnston - Fluid Simulation as Full Body Audio-Visual Instrum.pdf},
  language = {en}
}

@inproceedings{kapoorInteractiveOptimizationSteering2010,
  title = {Interactive {{Optimization}} for {{Steering Machine Classification}}},
  booktitle = {Conference on {{Human Factors}} in {{Computing Systems}} - {{Proceedings}}},
  author = {Kapoor, Ashish and Lee, Bongshin and Tan, Desney and Horvitz, Eric},
  year = {2010},
  month = jan,
  volume = {2},
  pages = {1343--1352},
  doi = {10.1145/1753326.1753529},
  abstract = {Interest has been growing within HCI on the use of machine learning and reasoning in applications to classify such hid- den states as user intentions, based on observations. HCI researchers with these interests typically have little exper- tise in machine learning and often employ toolkits as rela- tively fixed "black boxes" for generating statistical classifiers. However, attempts to tailor the performance of classifiers to specific application requirements may require a more sophisticated understanding and custom-tailoring of methods. We present ManiMatrix, a system that provides controls and visualizations that enable system builders to refine the behavior of classification systems in an intuitive manner. With ManiMatrix, users directly refine parameters of a confusion matrix via an interactive cycle of re- classification and visualization. We present the core me- thods and evaluate the effectiveness of the approach in a user study. Results show that users are able to quickly and effectively modify decision boundaries of classifiers to tai- lor the behavior of classifiers to problems at hand.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/B3CS6CMT/Kapoor et al. - 2010 - Interactive Optimization for Steering Machine Clas.pdf}
}

@inproceedings{kimNeuralMusicSynthesis2019,
  title = {Neural {{Music Synthesis}} for {{Flexible Timbre Control}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kim, Jong Wook and Bittner, Rachel and Kumar, Aparna and Bello, Juan Pablo},
  year = {2019},
  month = may,
  pages = {176--180},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8683596},
  abstract = {The recent success of raw audio waveform synthesis models like WaveNet motivates a new approach for music synthesis, in which the entire process - creating audio samples from a score and instrument information - is modeled using generative neural networks. This paper describes a neural music synthesis model with flexible timbre controls, which consists of a recurrent neural network conditioned on a learned instrument embedding followed by a WaveNet vocoder. The learned embedding space successfully captures the diverse variations in timbres within a large dataset and enables timbre control and morphing by interpolating between instruments in the embedding space. The synthesis quality is evaluated both numerically and perceptually, and an interactive web demo is presented.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/XCLD48WQ/Kim et al. - 2019 - Neural Music Synthesis for Flexible Timbre Control.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/HPVJUWUB/8683596.html},
  keywords = {audio signal processing,flexible timbre control,generative neural networks,instrument embedding,instrument information,interactive Web demo,interpolation,music,Music Synthesis,neural music synthesis model,raw audio waveform synthesis models,recurrent neural nets,recurrent neural network,Timbre Embedding,vocoders,WaveNet,WaveNet vocoder}
}

@inproceedings{kimNeuralMusicSynthesis2019a,
  title = {Neural {{Music Synthesis}} for {{Flexible Timbre Control}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kim, Jong Wook and Bittner, Rachel and Kumar, Aparna and Bello, Juan Pablo},
  year = {2019},
  month = may,
  pages = {176--180},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8683596},
  abstract = {The recent success of raw audio waveform synthesis models like WaveNet motivates a new approach for music synthesis, in which the entire process - creating audio samples from a score and instrument information - is modeled using generative neural networks. This paper describes a neural music synthesis model with flexible timbre controls, which consists of a recurrent neural network conditioned on a learned instrument embedding followed by a WaveNet vocoder. The learned embedding space successfully captures the diverse variations in timbres within a large dataset and enables timbre control and morphing by interpolating between instruments in the embedding space. The synthesis quality is evaluated both numerically and perceptually, and an interactive web demo is presented.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PGJU7TYQ/Kim et al. - 2019 - Neural Music Synthesis for Flexible Timbre Control.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CDYJZDEG/8683596.html},
  keywords = {audio signal processing,flexible timbre control,generative neural networks,instrument embedding,instrument information,interactive Web demo,interpolation,music,Music Synthesis,neural music synthesis model,raw audio waveform synthesis models,recurrent neural nets,recurrent neural network,Timbre Embedding,vocoders,WaveNet,WaveNet vocoder}
}

@article{klingbeilSOFTWARESPECTRALANALYSIS,
  title = {{{SOFTWARE FOR SPECTRAL ANALYSIS}}, {{EDITING}}, {{AND SYNTHESIS}}},
  author = {Klingbeil, Michael},
  pages = {4},
  abstract = {This paper describes the design and development of new software for spectral analysis, editing and resynthesis. Analysis is accomplished using a variation of the traditional McAulay-Quatieri technique of peak interpolation and partial tracking. Linear prediction of the partial amplitudes and frequencies is used to determine the best continuations for sinusoidal tracks. A high performance user interface supports flexible selection and immediate manipulation of analysis data, cut and paste, and unlimited undo/redo. Hundreds of simultaneous partials can be synthesized in real-time and documents may contain thousands of individual partials dispersed in time without degrading performance. A variety of standard file formats are supported for the import and export of analysis data.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GZ6I5MCL/Klingbeil - SOFTWARE FOR SPECTRAL ANALYSIS, EDITING, AND SYNTH.pdf},
  keywords = {inverse fft synthesis},
  language = {en}
}

@inproceedings{kneesIntelligentMusicInterfaces2019,
  title = {Intelligent {{Music Interfaces}} for {{Listening}} and {{Creation}}},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}: {{Companion}}},
  author = {Knees, Peter and Schedl, Markus and Fiebrink, Rebecca},
  year = {2019},
  pages = {135--136},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3308557.3313110},
  abstract = {Digital music technology plays a central role in all areas of the music ecosystem. For both, music consumers and music producers, intelligent user interfaces are a means to improve access to sound and music. The second workshop on Intelligent Music Interfaces for Listening and Creation (MILC) provides a forum for the latest developments and trends in intelligent interfaces for music consumption and production by bringing together researchers from areas such as music information retrieval, recommender systems, interactive machine learning, human-computer interaction, and composition.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YK7SE73L/Knees et al. - 2019 - Intelligent Music Interfaces for Listening and Cre.pdf},
  isbn = {978-1-4503-6673-1},
  keywords = {intelligent user interfaces,music creation,music listening},
  series = {{{IUI}} '19}
}

@article{krumhansl1992perceptual,
  title={Perceptual interactions between musical pitch and timbre.},
  author={Krumhansl, Carol L and Iverson, Paul},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={18},
  number={3},
  pages={739},
  year={1992},
  publisher={American Psychological Association}
}

@article{kohler1929gestalt,
  title={Gestalt Psychology.[Psychologische Probleme 1933]},
  author={K{\"o}hler, Wolfgang},
  journal={New York Horace Liveright},
  year={1929}
}

@inproceedings{kneesSearchingAudioSketching2016,
  title={Searching for audio by sketching mental images of sound: A brave new idea for audio retrieval in creative music production},
  author={Knees, Peter and Andersen, Kristina},
  booktitle={Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval},
  pages={95--102},
  year={2016}
}

@article{koelschMusicLanguageMeaning2004,
  title = {Music, Language and Meaning: Brain Signatures of Semantic Processing},
  shorttitle = {Music, Language and Meaning},
  author = {Koelsch, Stefan and Kasper, Elisabeth and Sammler, Daniela and Schulze, Katrin and Gunter, Thomas and Friederici, Angela D},
  year = {2004},
  month = mar,
  volume = {7},
  pages = {302--307},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1197},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/M424FEQY/Koelsch et al. - 2004 - Music, language and meaning brain signatures of s.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@inproceedings{kolhoffMusicIconsProcedural2006,
  title = {Music {{Icons}}: {{Procedural Glyphs}} for {{Audio Files}}},
  shorttitle = {Music {{Icons}}},
  booktitle = {2006 19th {{Brazilian Symposium}} on {{Computer Graphics}} and {{Image Processing}}},
  author = {Kolhoff, Philipp and Preub, Jacqueline and Loviscach, Jorn},
  year = {2006},
  month = oct,
  pages = {289--296},
  issn = {2377-5416},
  doi = {10.1109/SIBGRAPI.2006.30},
  abstract = {Nowadays, a personal music collection may comprise thousands of MP3 files. Visualization can help the user to gain an overview and to find similar songs inside so large a set. We describe a method to create icons from audio files in such a way that songs which the user considers similar receive similar icons. This allows visual data mining in standard directory listings of window-based operating systems. The icons consist of bloom-like shapes, whose form and color depend on eight parameters. These parameters are controlled through a neural net, the input of which are audio features that are extracted algorithmically from the MP3 files. To adapt the system to the user's perception and interests, the neural net is initially trained with a small set of songs and icons. User studies done on the system demonstrate a strong perceptual relation between music and icons},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Y9ETIQFA/Kolhoff et al. - 2006 - Music Icons Procedural Glyphs for Audio Files.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FNBRXPQK/4027079.html},
  keywords = {audio feature,audio file,Auditory displays,data mining,Data mining,data visualisation,Data visualization,Digital audio players,feature extraction,Feature extraction,graphical user interfaces,MP3 file,music icon,neural net,neural nets,Neural networks,Operating systems,operating systems (computers),procedural glyph,Prototypes,Shape,sound visualisation,Two dimensional displays,visual data mining,window-based operating system}
}

@article{kolliasOverviewingFieldSelf,
  title = {Overviewing ~a ~{{Field}} ~of ~{{Self}}-\-{{Organising}} ~{{Music}} ~{{Interfaces}}: ~ {{Autonomous}}, ~{{Distributed}}, ~{{Environmentally}} ~{{Aware}}, ~ {{Feedback}} ~{{Systems}}},
  author = {Kollias, Phivos-Angelos},
  pages = {6},
  abstract = {This paper aims to identify and discuss the music field of self-organising music: an emerging field based on different forms of self-organising music interfaces, that is to say `intelligent' sound/music systems characterised among others by autonomy, distributed/decentralised feedback processes and of environmental awareness. A music field based on systems-oriented concepts (cybernetics, general systems theory, complexity) and which is formed spontaneously by individual cases of composers-researchers with unique yet converging approaches. We are describing the general context of self-organising music and presenting different cases of composers-researchers that deal with the subject both from a technical and a theoretical perspective. We conclude the paper suggesting the search for a systemoriented shared musical language in order to broaden and evolve the field's musical though.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2HDY2649/Kollias - Overviewing  a  Field  of  Self-­Organising  Music.pdf},
  language = {en}
}

@article{kostaSTUDYCULTURALDEPENDENCE,
  title = {A {{STUDY OF CULTURAL DEPENDENCE OF PERCEIVED MOOD IN GREEK MUSIC}}},
  author = {Kosta, Katerina and Song, Yading and Fazekas, Gyorgy and Sandler, Mark B},
  pages = {6},
  abstract = {Several algorithms have been developed in the music information retrieval community for predicting mood in music in order to facilitate organising and accessing large audio collections. Little attention has been paid however to how perceived emotion depends on cultural factors, such as listeners' acculturation or familiarity with musical background or language. In this study, we examine this dependence in the context of Greek music. A large representative database of Greek songs has been created and sampled observing predefined criteria such as the balance between Eastern and Western influenced musical genres. Listeners were then asked to rate songs according to their perceived mood. We collected continuous ratings of arousal and valence for short song excerpts and also asked participants to select a mood tag from a controlled mood vocabulary that best described the music. We analysed the consistency of ratings between Greek and non-Greek listeners and the relationships between the categorical and dimensional representations of emotions. Our results show that there is a greater agreement in listener's judgements with Greek background compared to the group with varying background. These findings suggest valuable implications on the future development of mood prediction systems.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4GYB994A/Kosta et al. - A STUDY OF CULTURAL DEPENDENCE OF PERCEIVED MOOD I.pdf},
  language = {en}
}

@article{kronland-martinetModellingNaturalSounds1997,
  title = {Modelling of Natural Sounds by Time\textendash Frequency and Wavelet Representations},
  author = {{Kronland-Martinet}, Richard and GUILLEMAIN, Ph. and Ystad, S{\o}lvi},
  year = {1997},
  month = dec,
  volume = {2},
  pages = {179--191},
  doi = {10.1017/S1355771898009030},
  abstract = {Sound modelling is an important part of the analysis\textendash synthesis process since it combines sound processing and algorithmic synthesis within the same formalism. Its aim is to make sound simulators by synthesis methods based on signal models or physical models, the parameters of which are directly extracted from the analysis of natural sounds. In this article the successive steps for making such systems are described. These are numerical synthesis and sound generation methods, analysis of natural sounds, particularly time\textendash frequency and time\textendash scale (wavelet)  representations, extraction of pertinent parameters, and the determination of the correspondence between these parameters and those corresponding to the synthesis models. Additive synthesis, nonlinear synthesis, and waveguide synthesis are discussed.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/44LPCTJK/Kronland-Martinet et al. - 1997 - Modelling of natural sounds by time–frequency and .pdf},
  journal = {Organised Sound},
  keywords = {sound synthesis}
}

@inproceedings{krumhanslWhyMusicalTimbre1989,
  title = {Why Is Musical Timbre so Hard to Understand},
  author = {Krumhansl, Carol},
  year = {1989},
  abstract = {Fabrics of which the warp is sized with water-soluble polymers of acrylic acid and/or their alkali metal salts or ammonium salts, are desized by treating the sized fabric with from 30 to 300 percent by weight of water, based on the dry weight of the fabric, and separating the resulting size solution from the fabric. The recovered size solution can be directly re-used for sizing.},
  keywords = {music perception}
}

@article{kussnerMusiciansAreMore2014,
  title = {Musicians Are More Consistent: {{Gestural}} Cross-Modal Mappings of Pitch, Loudness and Tempo in Real-Time},
  shorttitle = {Musicians Are More Consistent},
  author = {K{\"u}ssner, Mats B. and Tidhar, Dan and Prior, Helen M. and {Leech-Wilkinson}, Daniel},
  year = {2014},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00789},
  abstract = {Cross-modal mappings of auditory stimuli reveal valuable insights into how humans make sense of sound and music. Whereas researchers have investigated cross-modal mappings of sound features varied in isolation within paradigms such as speeded classification and forced-choice matching tasks, investigations of representations of concurrently varied sound features (e.g., pitch, loudness and tempo) with overt gestures\textemdash accounting for the intrinsic link between movement and sound\textemdash are scant. To explore the role of bodily gestures in cross-modal mappings of auditory stimuli we asked sixty-four musically trained and untrained participants to represent pure tones\textemdash continually sounding and concurrently varied in pitch, loudness and tempo\textemdash with gestures while the sound stimuli were played. We hypothesised musical training to lead to more consistent mappings between pitch and height, loudness and distance/height, and tempo and speed of hand movement and muscular energy. Our results corroborate previously reported pitch vs. height (higher pitch leading to higher elevation in space) and tempo vs. speed (increasing tempo leading to increasing speed of hand movement) associations, but also reveal novel findings pertaining to musical training which influenced consistency of pitch mappings, annulling a commonly observed bias for convex (i.e. rising-falling) pitch contours. Moreover, we reveal effects of interactions between musical parameters on cross-modal mappings (e.g., pitch and loudness on speed of hand movement), highlighting the importance of studying auditory stimuli concurrently varied in different musical parameters. Results are discussed in light of cross-modal cognition, with particular emphasis on studies within (embodied) music cognition. Implications for theoretical refinements and potential clinical applications are provided.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RF9YFEQD/Küssner et al. - 2014 - Musicians are more consistent Gestural cross-moda.pdf},
  journal = {Frontiers in Psychology},
  keywords = {cross-modal mappings,embodied music cognition,Gesture,musical training,real-time mappings,sound shape associations},
  language = {English}
}

@article{kussnerShapeDrawingGesture2014,
  title = {Shape, Drawing and Gesture: {{Cross}}-Modal Mappings of Sound and Music.},
  shorttitle = {Shape, Drawing and Gesture},
  author = {K{\"u}ssner, Mats B.},
  year = {2014},
  publisher = {{Unpublished}},
  doi = {10.13140/RG.2.1.2213.3600},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4GW5YJ4Z/Küssner - 2014 - Shape, drawing and gesture Cross-modal mappings o.pdf},
  language = {en}
}

@inproceedings{legrouxPerceptsynthMappingPerceptual2008,
  title = {Perceptsynth: Mapping Perceptual Musical Features to Sound Synthesis Parameters},
  shorttitle = {Perceptsynth},
  booktitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Le Groux, Sylvain and Verschure, Paul FMJ},
  year = {2008},
  month = mar,
  pages = {125--128},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2008.4517562},
  abstract = {This paper presents a new system that allows for intuitive control of an additive sound synthesis model from perceptually relevant high-level sonic features. We suggest a general framework for the extraction, abstraction, reproduction and transformation of timbral characteristics of a sound analyzed from recordings. We propose a method to train, tune and evaluate our system in an automatic, consistent and reproducible fashion, and show that this system yields various original audio and musical applications.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9NN5I6I8/Le Groux and Verschure - 2008 - Perceptsynth mapping perceptual musical features .pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/HN4KMNNL/4517562.html},
  keywords = {acoustic signal processing,additive sound synthesis model,Additives,Control system synthesis,Feature extraction,Feature Extraction,Frequency,intuitive control,learning (artificial intelligence),Machine learning,machine learning approach,Matrix decomposition,musical acoustics,Network synthesis,perceptsynth method,perceptual musical feature mapping,Principal component analysis,signal synthesis,Signal synthesis,sound generation,Sound Synthesis,SVM,timbral characteristics,Timbre,training method}
}

@article{lehmannSearchingSoundsStudy,
  title = {Searching for {{Sounds}} - {{A Study}} on {{Interface Design}} and {{Enhancement}} of a {{Web Application}}},
  author = {Lehmann, Eric},
  pages = {123},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/BZHSLDKT/Lehmann - Searching for Sounds - A Study on Interface Design.pdf},
  language = {en}
}

@article{lehmannSearchingSoundsStudya,
  title = {Searching for {{Sounds}} - {{A Study}} on {{Interface Design}} and {{Enhancement}} of a {{Web Application}}},
  author = {Lehmann, Eric},
  pages = {123},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/V5JHKILA/Lehmann - Searching for Sounds - A Study on Interface Design.pdf},
  language = {en}
}

@article{lehmannSearchingSoundsStudyb,
  title = {Searching for {{Sounds}} - {{A Study}} on {{Interface Design}} and {{Enhancement}} of a {{Web Application}}},
  author = {Lehmann, Eric},
  pages = {123},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/KASWRVAJ/Lehmann - Searching for Sounds - A Study on Interface Design.pdf},
  language = {en}
}

@article{lehmannSearchingSoundsStudyc,
  title = {Searching for {{Sounds}} - {{A Study}} on {{Interface Design}} and {{Enhancement}} of a {{Web Application}}},
  author = {Lehmann, Eric},
  pages = {123},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NFCFMBH5/Lehmann - Searching for Sounds - A Study on Interface Design.pdf},
  language = {en}
}

@article{lehmannSearchingSoundsStudyd,
  title = {Searching for {{Sounds}} - {{A Study}} on {{Interface Design}} and {{Enhancement}} of a {{Web Application}}},
  author = {Lehmann, Eric},
  pages = {123},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2E259RMB/Lehmann - Searching for Sounds - A Study on Interface Design.pdf},
  language = {en}
}

@article{lehmannSearchingSoundsStudye,
  title = {Searching for {{Sounds}} - {{A Study}} on {{Interface Design}} and {{Enhancement}} of a {{Web Application}}},
  author = {Lehmann, Eric},
  pages = {123},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8WTV2DSI/Lehmann - Searching for Sounds - A Study on Interface Design.pdf},
  language = {en}
}

@article{lehmannSearchingSoundsStudyf,
  title = {Searching for {{Sounds}} - {{A Study}} on {{Interface Design}} and {{Enhancement}} of a {{Web Application}}},
  author = {Lehmann, Eric},
  pages = {123},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6RASF4WR/Lehmann - Searching for Sounds - A Study on Interface Design.pdf},
  language = {en}
}

@article{lembkeHearingTrianglesPerceptual2018,
  title = {Hearing Triangles: {{Perceptual}} Clarity, Opacity, and Symmetry of Spectrotemporal Sound Shapes},
  shorttitle = {Hearing Triangles},
  author = {Lembke, Sven-Amin},
  year = {2018},
  month = aug,
  volume = {144},
  pages = {608--619},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.5048130},
  abstract = {In electroacoustic music, the spectromorphological approach commonly employs analogies to non-sonic phenomena like shapes, gestures, or textures. In acoustical terms, sound shapes can concern simple geometries on the spectrotemporal plane, for instance, a triangle that widens in frequency over time. To test the auditory relevance of such triangular sound shapes, two psychoacoustic experiments assessed if and how these shapes are perceived. Triangular sound-shape stimuli, created through granular synthesis, varied across the factors grain density, frequency and amplitude scales, and widening vs narrowing orientations. The perceptual investigation focused on three auditory qualities, derived in analogy to the visual description of a triangle: the clarity of the triangular outline, the opacity of the area enclosed by the outline, and the symmetry along the vertical dimension. These morphological qualities seemed to capture distinct perceptual aspects, each linked to different acoustical factors. Clarity of shape was conveyed even for sparse grain densities, while also exhibiting a perceptual bias for widening orientations. Opacity varied as a function of grain texture, whereas symmetry strongly depended on frequency and amplitude scales. The perception of sound shapes could relate to common perceptual cross-modal correspondences and share the same principles of perceptual grouping with vision.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/94KFTH49/Lembke - 2018 - Hearing triangles Perceptual clarity, opacity, an.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/M4JIEGYV/1.html},
  journal = {The Journal of the Acoustical Society of America},
  keywords = {Sound shape},
  number = {2}
}

@article{lewisSystemUsabilityScale2018,
  title = {The {{System Usability Scale}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {The {{System Usability Scale}}},
  author = {Lewis, James R.},
  year = {2018},
  month = jul,
  volume = {34},
  pages = {577--590},
  publisher = {{Taylor \& Francis}},
  issn = {1044-7318},
  doi = {10.1080/10447318.2018.1455307},
  abstract = {The System Usability Scale (SUS) is the most widely used standardized questionnaire for the assessment of perceived usability. This review of the SUS covers its early history from inception in the 1980s through recent research and its future prospects. From relatively inauspicious beginnings, when its originator described it as a ``quick and dirty usability scale,'' it has proven to be quick but not ``dirty.'' It is likely that the SUS will continue to be a popular measurement of perceived usability for the foreseeable future. When researchers and practitioners need a measure of perceived usability, they should strongly consider using the SUS.},
  annotation = {\_eprint: https://doi.org/10.1080/10447318.2018.1455307},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YFXNDGXN/Lewis - 2018 - The System Usability Scale Past, Present, and Fut.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/WKYMIBG6/10447318.2018.html},
  journal = {International Journal of Human\textendash Computer Interaction},
  keywords = {Perceived usability,standardized usability scale,SUS,System Usability,usability},
  number = {7}
}

@article{lewisSystemUsabilityScale2018a,
  title = {The {{System Usability Scale}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {The {{System Usability Scale}}},
  author = {Lewis, James R.},
  year = {2018},
  month = jul,
  volume = {34},
  pages = {577--590},
  issn = {1044-7318, 1532-7590},
  doi = {10.1080/10447318.2018.1455307},
  abstract = {The System Usability Scale (SUS) is the most widely used standardized questionnaire for the assessment of perceived usability. This review of the SUS covers its early history from inception in the 1980s through recent research and its future prospects. From relatively inauspicious beginnings, when its originator described it as a ``quick and dirty usability scale,'' it has proven to be quick but not ``dirty.'' It is likely that the SUS will continue to be a popular measurement of perceived usability for the foreseeable future. When researchers and practitioners need a measure of perceived usability, they should strongly consider using the SUS.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZX7HK46X/Lewis - 2018 - The System Usability Scale Past, Present, and Fut.pdf},
  journal = {International Journal of Human\textendash Computer Interaction},
  language = {en},
  number = {7}
}

@article{limpaecherRealtimeDrawingAssistance2013,
  title = {Real-Time Drawing Assistance through Crowdsourcing},
  author = {Limpaecher, Alex and Feltman, Nicolas and Treuille, Adrien and Cohen, Michael},
  year = {2013},
  month = jul,
  volume = {32},
  pages = {54:1--54:8},
  issn = {0730-0301},
  doi = {10.1145/2461912.2462016},
  abstract = {We propose a new method for the large-scale collection and analysis of drawings by using a mobile game specifically designed to collect such data. Analyzing this crowdsourced drawing database, we build a spatially varying model of artistic consensus at the stroke level. We then present a surprisingly simple stroke-correction method which uses our artistic consensus model to improve strokes in real-time. Importantly, our auto-corrections run interactively and appear nearly invisible to the user while seamlessly preserving artistic intent. Closing the loop, the game itself serves as a platform for large-scale evaluation of the effectiveness of our stroke correction algorithm.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/BQNG5DTK/Limpaecher et al. - 2013 - Real-time drawing assistance through crowdsourcing.pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {crowdsourcing,drawing correction,interactive drawings},
  number = {4}
}

@article{lindhSkeuomorphicRepresentationSubtractive,
  title = {Beyond a {{Skeuomorphic Representation}} of {{Subtractive Synthesis}}},
  author = {Lindh, Martin},
  pages = {5},
  abstract = {This proposition paper wants to raise the issue of design ideology within the field of music software that recreates vintage analogue synthesizers using subtractive synthesis. There is a clear dominance of a skeuomorphic design ideology regarding the Graphical User Interface (GUI) within this field. The suggested study aims to research if this is a correct choice in terms us usability, accessibility and intuitiveness. The suggestion is to conduct a series of A/B tests of custom prototype UIs on three predefined groups of users. Group A, with a previous knowledge of analogue hardware synthesizers, Group B, with a knowledge of music software but limited experience with analogue hardware synthesizers and finally Group C, with a limited knowledge and experience of both music software and analogue hardware synthesizers. The prototype GUIs will be made using a skeuomorphic design paradigm as well as a flat design paradigm that also incorporates a couple of new ideas in terms of input controls. The A/B tests will be complemented with semi-structured interviews with the participants.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/F9725E67/Lindh - Beyond a Skeuomorphic Representation of Subtractiv.pdf},
  language = {en}
}

@misc{londonEmbodiedAudioVisualInteraction,
  title = {Embodied {{AudioVisual Interaction Group}}},
  author = {of London, EAVI Goldsmiths University},
  abstract = {An embodied way of interacting with computers},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7LGC5527/eavi.goldsmithsdigital.com.html},
  howpublished = {http://eavi.goldsmithsdigital.com},
  journal = {Embodied AudioVisual Interaction Group},
  language = {en-US}
}

@article{lorhoEGaugeMeasureAssessor,
  title = {{{eGauge}} - a Measure of Assessor Expertise in Audio Quality Evaluations},
  author = {Lorho, Gaetan},
  pages = {10},
  abstract = {The eGauge measure for assessor expertise in listening tests is presented. This ANOVA-based approach aims to objectively qualify the goodness of assessors within an experiment in terms of their discrimination and reliability skill and their overall agreement with the panel. A number of datasets resulting from experiments using different standardized mean opinion score (MOS) methodologies are then tested to illustrate this approach.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/HFMYC6ZR/Lorho - eGauge - a measure of assessor expertise in audio .pdf},
  language = {en}
}

@article{lvTouchlessInteractiveAugmented2015,
  title = {Touch-Less Interactive Augmented Reality Game on Vision-Based Wearable Device},
  author = {Lv, Zhihan and Halawani, Alaa and Feng, Shengzhong and {ur R{\'e}hman}, Shafiq and Li, Haibo},
  year = {2015},
  month = jul,
  volume = {19},
  pages = {551--567},
  issn = {1617-4917},
  doi = {10.1007/s00779-015-0844-1},
  abstract = {There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/IZENAVAT/Lv et al. - 2015 - Touch-less interactive augmented reality game on v.pdf},
  journal = {Personal and Ubiquitous Computing},
  keywords = {football ar},
  language = {en},
  number = {3}
}

@article{marelliTimeFrequencySynthesis2010,
  title = {Time\textendash{{Frequency Synthesis}} of {{Noisy Sounds With Narrow Spectral Components}}},
  author = {Marelli, Dami{\'a}n and Aramaki, Mitsuko and {Kronland-Martinet}, Richard and Verron, Charles},
  year = {2010},
  month = nov,
  volume = {18},
  pages = {1929--1940},
  issn = {1558-7924},
  doi = {10.1109/TASL.2010.2040532},
  abstract = {The inverse fast Fourier transform (FFT) method was proposed to alleviate the computational complexity of the additive sound synthesis method in real-time applications, and consists in synthesizing overlapping blocks of samples in the frequency domain. However, its application is limited by its inherent tradeoff between time and frequency resolution. In this paper, we propose an alternative to the inverse FFT method for synthesizing colored noise. The proposed approach uses subband signal processing to generate time-frequency noise with an autocorrelation function such that the noise obtained after converting it to time domain has the desired power spectral density. We show that the inverse FFT method can be interpreted as a particular case of the proposed method, and therefore, the latter offers some extra design flexibility. Exploiting this property, we present experimental results showing that the proposed method can offer a better tradeoff between time and frequency resolution, at the expense of some extra computations.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4RBAK92Q/5443740.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Acoustic noise,acoustic signal processing,additive sound synthesis method,Additive synthesis,audio systems,Colored noise,colored noise synthesis,computational complexity,Computational complexity,fast Fourier transforms,Fast Fourier transforms,FFT method,Frequency domain analysis,frequency-domain synthesis,inverse fast Fourier transform,inverse fft synthesis,narrow spectral components,Noise generators,noise synthesis,Power generation,power spectral density,Signal processing,Signal resolution,signal synthesis,Signal synthesis,subband signal processing,time-frequency analysis,time-frequency synthesis,time–frequency analysis},
  number = {8}
}

@incollection{marengoInteractionCasualUsers2017,
  title = {The {{Interaction}} of {{Casual Users}} with {{Digital Collections}} of {{Visual Art}}. {{An Exploratory Study}} of the {{WikiArt Website}}},
  booktitle = {{{HCI International}} 2017 \textendash{} {{Posters}}' {{Extended Abstracts}}},
  author = {Marengo, Lucia and Fazekas, Gy{\"o}rgy and Tombros, Anastasios},
  editor = {Stephanidis, Constantine},
  year = {2017},
  volume = {714},
  pages = {583--590},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-58753-0_82},
  abstract = {As many cultural institutions are publishing digital heritage material on the web, a new type of user emerged, that casually interacts with the art collection in his/her free time, driven by intrinsic curiosity more than by a professional duty or an informational goal. Can choices in how the interaction with data is structured increase engagement of such users? In our exploratory study, we use the WikiArt project as a case study to analyse how users approach search interfaces for free exploration. Our preliminary results show that, despite the remarkable diversity of artworks available, users rely on familiarity as their main criterion to navigate the website; they stay within known topics and rarely discover new ones. Users show interest in heterogeneous datasets, but their engagement is rarely sustained, while the presence of slightly unrelated artworks in a set can increase curiosity and self-reflection. Finally, we discuss the role of the database's perceived size on users' expectations.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/XMP5C5XW/Marengo et al. - 2017 - The Interaction of Casual Users with Digital Colle.pdf},
  isbn = {978-3-319-58752-3 978-3-319-58753-0},
  language = {en}
}

@article{martinoSynesthesiaStrongWeak2001,
  title={Synesthesia: Strong and weak},
  author={Martino, Gail and Marks, Lawrence E},
  journal={Current Directions in Psychological Science},
  volume={10},
  number={2},
  pages={61--65},
  year={2001},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{maurerShapeBoubasSound2006,
  title = {The Shape of Boubas: Sound\textendash Shape Correspondences in Toddlers and Adults},
  shorttitle = {The Shape of Boubas},
  author = {Maurer, Daphne and Pathman, Thanujeni and Mondloch, Catherine J.},
  year = {2006},
  volume = {9},
  pages = {316--322},
  issn = {1467-7687},
  doi = {10.1111/j.1467-7687.2006.00495.x},
  abstract = {A striking demonstration that sound\textendash object correspondences are not completely arbitrary is that adults map nonsense words with rounded vowels (e.g. bouba) to rounded shapes and nonsense words with unrounded vowels (e.g. kiki) to angular shapes (K\"ohler, 1947; Ramachandran \& Hubbard, 2001). Here we tested the bouba/kiki phenomenon in 2.5-year-old children and a control group of adults (n =20 per age), using four pairs of rounded versus pointed shapes and four contrasting pairs of nonsense words differing in vowel sound. Overall, participants at both ages matched words with rounded vowels to the rounder shapes and words with unrounded vowels to the pointed shapes (both ps {$<$} .0005), with no significant difference between the two ages (p {$>$} .10). Such naturally biased correspondences between sound and shape may influence the development of language.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NVWENS25/Maurer et al. - 2006 - The shape of boubas sound–shape correspondences i.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NFL997GP/j.1467-7687.2006.00495.html},
  journal = {Developmental Science},
  language = {en},
  number = {3}
}

@article{mcadamsMetaanalysisAcousticCorrelates2006,
  title = {A Meta-Analysis of Acoustic Correlates of Timbre Dimensions},
  author = {McAdams, S. and Giordano, B. L. and Susini, P. and Peeters, G. and Rioux, V.},
  year = {2006},
  volume = {120},
  pages = {3275-3276(A)},
  issn = {0001-4966},
  abstract = {A meta-analysis of ten published timbre spaces was conducted using multidimensional scaling analyses (CLASCAL) of dissimilarity ratings on recorded, resynthesized, or synthesized musical instrument tones. A set of signal descriptors derived from the tones was drawn from a large set developed at IRCAM, including parameters derived from the long-term amplitude spectrum (slope, centroid, spread, deviation, skewness, kurtosis), from the waveform and amplitude envelope (attack time, fluctuation, roughness), and from variations in the short-term amplitude spectrum (flux). Relations among all descriptors across the 128 sounds were used to determine families of related descriptors and to reduce the number of descriptors tested as predictors. Subsequently multiple correlations between descriptors and the positions of timbres along perceptual dimensions determined by the CLASCAL analyses were computed. The aim was (1) to select the subset of acoustic descriptors (or their linear combinations) that provided the most generalizable prediction of timbral relations and (2) to provide a signal-based model of timbral description for musical instrument tones. Four primary classes of descriptors emerge: spectral centroid, spectral spread, spectral deviation, and temporal envelope (effective duration/attack time).},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4GFRM67W/68650.html},
  journal = {Journal of the Acoustical Society of America},
  language = {en-GB}
}

@incollection{mcadamsPerceptionMusicalTimbre2008,
  title = {The Perception of Musical Timbre},
  booktitle = {The {{Oxford Handbook}} of {{Music Psychology}}},
  author = {Mcadams, Stephen and Giordano, Bruno},
  year = {2008},
  month = jan,
  pages = {72--80},
  doi = {10.1093/oxfordhb/9780199298457.013.0007},
  abstract = {This article discusses musical-timbre perception. Musical timbre is a combination of continuous perceptual dimensions and discrete features to which listeners are differentially sensitive. The continuous dimensions often have quantifiable acoustic correlates. The timbre-space representation is a powerful psychological model that allows predictions to be made about timbre perception in situations beyond those used to derive the model in the first place. Timbre can play a role in larger-scale movements of tension and relaxation and thus contribute to the expression inherent in musical form. Under conditions of high blend among instruments composing a vertical sonority, timbral roughness is a major component of musical tension. However, it strongly depends on the way auditory grouping processes have parsed the incoming acoustic information into events and streams.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/U6UUUAKZ/Mcadams and Giordano - 2008 - The perception of musical timbre.pdf}
}

@article{mcadamsPerceptionMusicalTimbre2016,
  title={The perception of musical timbre},
  author={McAdams, Stephen and Giordano, Bruno L},
  journal={The Oxford handbook of music psychology},
  pages={72--80},
  year={2009}
}

@article{mcadamsTimbreStructuringForce2013,
  title = {Timbre as a Structuring Force in Music},
  author = {McAdams, Stephen},
  year = {2013},
  volume = {19},
  pages = {7},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/S9QSMZHJ/McAdams - 2013 - Timbre as a structuring force in music.pdf},
  language = {en}
}

@article{mehrabiVocalImitationSynthesised2017,
  title = {Vocal Imitation of Synthesised Sounds Varying in Pitch, Loudness and Spectral Centroid},
  author = {Mehrabi, Adib and Dixon, Simon and Sandler, Mark B.},
  year = {2017},
  month = feb,
  volume = {141},
  pages = {783--796},
  issn = {0001-4966},
  doi = {10.1121/1.4974825},
  abstract = {Vocal imitations are often used to convey sonic ideas [Lemaitre, Dessein, Susini, and Aura. (2011). Ecol. Psych. 23(4), 267\textendash 307]. For computer based systems to interpret these vocalisations, it is advantageous to apply knowledge of what happens when people vocalise sounds where the acoustic features have different temporal envelopes. In the present study, 19 experienced musicians and music producers were asked to imitate 44 sounds with one or two feature envelopes applied. The study addresses two main questions: (1) How accurately can people imitate ramp and modulation envelopes for pitch, loudness, and spectral centroid?; (2) What happens to this accuracy when people are asked to imitate two feature envelopes simultaneously? The results show that experienced musicians can imitate pitch, loudness, and spectral centroid accurately, and that imitation accuracy is generally preserved when the imitated stimuli combine two, non-necessarily congruent features. This demonstrates the viability of using the voice as a natural means of expressing time series of two features simultaneously.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/G6GWQ7QC/Mehrabi et al. - 2017 - Vocal imitation of synthesised sounds varying in p.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/J6HWNPBQ/1.html},
  journal = {The Journal of the Acoustical Society of America},
  keywords = {cross-modal mapping},
  number = {2}
}

@article{mehriSampleRNNUnconditionalEndtoEnd2017,
  title = {{{SampleRNN}}: {{An Unconditional End}}-to-{{End Neural Audio Generation Model}}},
  shorttitle = {{{SampleRNN}}},
  author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
  year = {2017},
  month = feb,
  abstract = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
  archivePrefix = {arXiv},
  eprint = {1612.07837},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/I8KIAA8W/Mehri et al. - 2017 - SampleRNN An Unconditional End-to-End Neural Audi.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YTRKXYHY/1612.html},
  journal = {arXiv:1612.07837 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound},
  primaryClass = {cs}
}

@misc{mererPerceptualCharacterizationMotion2013,
  title = {Perceptual Characterization of Motion Evoked by Sounds for Synthesis Control Purposes},
  author = {Merer, Adrien and Aramaki, Mitsuko and Ystad, S{\o}lvi and {Kronland-Martinet}, Richard},
  year = {2013},
  month = mar,
  publisher = {{Association for Computing Machinery}},
  abstract = {This article addresses the question of synthesis and control of sound attributes from a perceptual point of view. We focused on an attribute related to the general concept of motion evoked by sounds. To investigate this concept, we tested 40 monophonic abstract sounds on listeners via a questionnaire and drawings, using a parametrized custom interface. This original procedure, which was defined with synthesis and control perspectives in mind, provides an alternative means of determining intuitive control parameters for synthesizing sounds evoking motion. Results showed that three main shape categories (linear, with regular oscillations, and with circular oscillations) and three types of direction (rising, descending, and horizontal) were distinguished by the listeners. In addition, the subjects were able to perceive the low-frequency oscillations (below 8 Hz) quite accurately. Three size categories (small, medium, and large) and three levels of randomness (none, low amplitude irregularities, and high amplitude irregularities) and speed (constant speed and speeds showing medium and large variations) were also observed in our analyses of the participants' drawings. We further performed a perceptual test to confirm the relevance of the contribution of some variables with synthesized sounds combined with visual trajectories. Based on these results, a general typology of evoked motion was drawn up and an intuitive control strategy was designed, based on a symbolic representation of continuous trajectories (provided by devices such as motion capture systems, pen tablets, etc.). These generic tools could be used in a wide range of applications such as sound design, virtual reality, sonification, and music.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/DMJ4XJ3S/Merer et al. - 2013 - Perceptual characterization of motion evoked by so.pdf},
  keywords = {Description,drawing,mapping,motion,perception,sound perception,sound shape association,synthesis control,trajectories}
}

@article{mererPerceptualCharacterizationMotion2013a,
  title = {Perceptual Characterization of Motion Evoked by Sounds for Synthesis Control Purposes},
  author = {Merer, Adrien and Aramaki, Mitsuko and Ystad, S{\o}lvi and {Kronland-Martinet}, Richard},
  year = {2013},
  month = mar,
  volume = {10},
  pages = {1:1--1:24},
  issn = {1544-3558},
  doi = {10.1145/2422105.2422106},
  abstract = {This article addresses the question of synthesis and control of sound attributes from a perceptual point of view. We focused on an attribute related to the general concept of motion evoked by sounds. To investigate this concept, we tested 40 monophonic abstract sounds on listeners via a questionnaire and drawings, using a parametrized custom interface. This original procedure, which was defined with synthesis and control perspectives in mind, provides an alternative means of determining intuitive control parameters for synthesizing sounds evoking motion. Results showed that three main shape categories (linear, with regular oscillations, and with circular oscillations) and three types of direction (rising, descending, and horizontal) were distinguished by the listeners. In addition, the subjects were able to perceive the low-frequency oscillations (below 8 Hz) quite accurately. Three size categories (small, medium, and large) and three levels of randomness (none, low amplitude irregularities, and high amplitude irregularities) and speed (constant speed and speeds showing medium and large variations) were also observed in our analyses of the participants' drawings. We further performed a perceptual test to confirm the relevance of the contribution of some variables with synthesized sounds combined with visual trajectories. Based on these results, a general typology of evoked motion was drawn up and an intuitive control strategy was designed, based on a symbolic representation of continuous trajectories (provided by devices such as motion capture systems, pen tablets, etc.). These generic tools could be used in a wide range of applications such as sound design, virtual reality, sonification, and music.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FMZ2WSPH/Merer et al. - 2013 - Perceptual characterization of motion evoked by so.pdf},
  journal = {ACM Transactions on Applied Perception},
  keywords = {Description,drawing,feature based synthesis,mapping,motion,perception,sound perception,synthesis control,trajectories},
  number = {1}
}

@book{mEvolutionPopularMusic2015,
  title = {The Evolution of Popular Music: {{USA}} 1960\textendash 2010},
  shorttitle = {The Evolution of Popular Music},
  author = {M, Levy and Of, Leroi Am The Evolution and Mauch, Matthias and Mauch, Matthias and Maccallum, Robert M. and Levy, Mark and Leroi, M.},
  year = {2015},
  abstract = {evolution/cognition/acoustics},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/B5WUPITU/M et al. - 2015 - The evolution of popular music USA 1960–2010.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ECRUEFS2/summary.html},
  keywords = {music evolution}
}

@book{modlerIntroductionNeuralNetworks,
  title = {Introduction {{Neural Networks}} for {{Mapping Hand Gestures}} to {{Sound Synthesis Parameters}}},
  author = {Modler, Paul},
  abstract = {null},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/99H6ELIX/Modler - Introduction Neural Networks for Mapping Hand Gest.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GWY3B6ZP/summary.html}
}

@article{moffatEvaluationAudioFeature2015,
  title = {An {{Evaluation}} of {{Audio Feature Extraction Toolboxes}}},
  author = {Moffat, David and Ronan, David and Reiss, Joshua D},
  year = {2015},
  pages = {7},
  abstract = {Audio feature extraction underpins a massive proportion of audio processing, music information retrieval, audio effect design and audio synthesis. Design, analysis, synthesis and evaluation often rely on audio features, but there are a large and diverse range of feature extraction tools presented to the community. An evaluation of existing audio feature extraction libraries was undertaken. Ten libraries and toolboxes were evaluated with the Cranfield Model for evaluation of information retrieval systems, reviewing the coverage, effort, presentation and time lag of a system. Comparisons are undertaken of these tools and example use cases are presented as to when toolboxes are most suitable. This paper allows a software engineer or researcher to quickly and easily select a suitable audio feature extraction toolbox.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/327M2MSQ/Moffat et al. - 2015 - An Evaluation of Audio Feature Extraction Toolboxe.pdf},
  language = {en}
}

@article{momeniDynamicIndependentMapping2006,
  title = {Dynamic {{Independent Mapping Layers}} for {{Concurrent Control}} of {{Audio}} and {{Video Synthesis}}},
  author = {Momeni, Ali and Henry, Cyrille},
  year = {2006},
  month = mar,
  volume = {30},
  pages = {49--66},
  issn = {1531-5169},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ASX9P3EH/Momeni and Henry - 2006 - Dynamic Independent Mapping Layers for Concurrent .pdf},
  journal = {Computer Music Journal},
  language = {en},
  number = {1}
}

@article{momeniDynamicIndependentMapping2006a,
  title = {Dynamic {{Independent Mapping Layers}} for {{Concurrent Control}} of {{Audio}} and {{Video Synthesis}}},
  author = {Momeni, Ali and Henry, Cyrille},
  year = {2006},
  volume = {30},
  pages = {49--66},
  issn = {0148-9267},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/KAEE4TU3/Momeni and Henry - 2006 - Dynamic Independent Mapping Layers for Concurrent .pdf},
  journal = {Computer Music Journal},
  number = {1}
}

@article{morrealeMusicRoom,
  title = {The {{Music Room}}},
  author = {Morreale, Fabio},
  abstract = {This paper presents The Music Room, an interactive installation where couples compose original music. The music is generated by Robin, an automatic composition system, according to relative distance between the users and the speed of their own},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/V6ZXJSRW/The_Music_Room.html},
  language = {en}
}

@inproceedings{morschheuserHowGamifyMethod2017,
  title = {How to {{Gamify}}? {{A Method For Designing Gamification}}},
  shorttitle = {How to {{Gamify}}?},
  booktitle = {Hawaii {{International Conference}} on {{System Sciences}}},
  author = {Morschheuser, Benedikt and Hamari, Juho and Werder, Karl and Abe, Julian},
  year = {2017},
  doi = {10.24251/HICSS.2017.155},
  abstract = {During recent years, gamification has become a popular method of enriching information technologies. Several business analysts have made promising predictions about penetration of gamification, however, it has also been estimated that most gamification efforts will fail due to poor understanding of how gamification should be designed and implemented. Therefore, in this paper we seek to advance the understanding of best practices related to the gamification design process. We approach this research problem via a design science research approach; firstly, by synthesizing the current body of literature on gamification design methods and interviewing 25 gamification experts. Secondly, we develop a method for gamification design, based on the gathered knowledge. Finally, we conduct an evaluation of the method via interviews of 10 gamification experts. The results indicate that the developed method is comprehensive, complete and provides practical utility. We deliver a comprehensive overview of gamification guidelines and shed novel insights into the overall nature of the gamification development and design discourse.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/86YV23D3/Morschheuser et al. - 2017 - How to Gamify A Method For Designing Gamification.pdf},
  keywords = {gamification},
  language = {en}
}

@misc{MostPopularSports,
  title = {The {{Most Popular Sports}} in the {{World}}},
  abstract = {\mbox It is estimated that more than half of the global population consider themselves association football (soccer) followers.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GYHDJF9L/what-are-the-most-popular-sports-in-the-world.html},
  howpublished = {https://www.worldatlas.com/articles/what-are-the-most-popular-sports-in-the-world.html},
  journal = {WorldAtlas},
  keywords = {Most popular sport},
  language = {en}
}

@article{mulderEmptyhandedGestureAnalysis,
  title = {Empty-Handed {{Gesture Analysis}} in {{Max}}/{{FTS}}},
  author = {Mulder, Axel},
  abstract = {Empty-handed Gesture Analysis in Max/FTS},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7IIQVY37/Empty-handed_Gesture_Analysis_in_Max_FTS.html},
  language = {en}
}

@article{mullensiefenGoldsmithsMusicalSophistication,
  title={The musicality of non-musicians: an index for assessing musical sophistication in the general population},
  author={M{\"u}llensiefen, Daniel and Gingras, Bruno and Musil, Jason and Stewart, Lauren},
  journal={PloS one},
  volume={9},
  number={2},
  pages={e89642},
  year={2014},
  publisher={Public Library of Science}
}

@article{mullnerModernHierarchicalAgglomerative2011,
  title = {Modern Hierarchical, Agglomerative Clustering Algorithms},
  author = {M{\"u}llner, Daniel},
  year = {2011},
  month = sep,
  abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
  archivePrefix = {arXiv},
  eprint = {1109.2378},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RCHV46YI/Müllner - 2011 - Modern hierarchical, agglomerative clustering algo.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/IH9XCLDR/1109.html},
  journal = {arXiv:1109.2378 [cs, stat]},
  keywords = {62H30,clustering,Computer Science - Data Structures and Algorithms,I.5.3,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{murray-browneBalancingCreativeFreedom,
  title = {Balancing Creative Freedom with Musical Development},
  author = {{Murray-Browne}, Tim},
  pages = {391},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NMGKQYKG/Murray-Browne - Balancing creative freedom with musical developmen.pdf},
  language = {en}
}

@article{murray-browneSerendiptichordWearableInstrument2010,
  title = {The {{Serendiptichord}}: {{A Wearable Instrument For Contemporary Dance Performance}}},
  author = {{Murray-Browne}, Tim and Mainstone, Di and {Bryan-Kinns}, Nick and Plumbley, Mark D},
  year = {2010},
  pages = {8},
  abstract = {We describe a novel musical instrument designed for use in contemporary dance performance. This instrument, the Serendiptichord, takes the form of a headpiece plus associated pods which sense movements of the dancer, together with associated audio processing software driven by the sensors. Movements such as translating the pods or shaking the trunk of the headpiece cause selection and modification of sampled sounds. We discuss how we have closely integrated physical form, sensor choice and positioning and software to avoid issues which otherwise arise with disconnection of the innate physical link between action and sound, leading to an instrument that non-musicians (in this case, dancers) are able to enjoy using immediately.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CMMSKDU7/Murray-Browne et al. - 2010 - The Serendiptichord A Wearable Instrument For Con.pdf},
  language = {en}
}

@inproceedings{naritaSketchBasedMangaRetrieval2017,
  title = {Sketch-{{Based Manga Retrieval Using Deep Features}}},
  booktitle = {2017 14th {{IAPR International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Narita, Rei and Tsubota, Koki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
  year = {2017},
  month = nov,
  volume = {03},
  pages = {49--53},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2017.291},
  abstract = {Manga, Japanese comics, are globally popular, and the digital manga (e-manga) market is growing year by year. E-manga has a limitation in its search methodology: it is currently restricted to a keyword search of authors and titles. In this paper, we present an intuitive sketch-based manga retrieval method using deep features. We propose a framework to extract feature vectors from sketches and manga images by two differently trained CNNs: The two CNNs are trained on a large number of manga face images with and without screentone. The deep features are switched according to a query, that is, a sketch drawing or a crop of a manga image. We built an interactive retrieval system that has a browser interface. We evaluated its retrieval accuracy by using sketches and manga images. The proposed method significantly outperformed state-of-the art sketch-based manga retrieval using handcrafted features.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8KW7K36M/Narita et al. - 2017 - Sketch-Based Manga Retrieval Using Deep Features.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/4XSFDR98/8270236.html},
  keywords = {Agriculture,art sketch,Browsers,CNN,content-based retrieval,database indexing,Databases,deep features,digital manga market,e-manga,edge detection,Face,Face recognition,feature extraction,Feature extraction,feature vectors,handcrafted features,Image edge detection,image representation,image retrieval,interactive retrieval system,intuitive sketch,Manga,manga face images,manga image,manga retrieval method,online front-ends,retrieval,sketch,sketch based retrieval,sketch drawing,trained CNNs,visual databases}
}

@misc{Newzoo2018Global,
  title = {Newzoo 2018 {{Global eSports Market Report}}},
  abstract = {Page created by Ana Griffin: Newzoo 2018 Global eSports Market Report},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RD7DQSYB/esports-free-2018-global-market-report-2682944.html},
  howpublished = {https://www.readkong.com/page/esports-free-2018-global-market-report-2682944},
  language = {en}
}

@article{nielsenParsingRoleConsonants2013,
  title = {Parsing the Role of Consonants versus Vowels in the Classic {{Takete}}-{{Maluma}} Phenomenon.},
  author = {Nielsen, Alan K. S. and Rendall, Drew},
  year = {2013},
  volume = {67},
  pages = {153--163},
  issn = {1878-7290, 1196-1961},
  doi = {10.1037/a0030553},
  abstract = {Wolfgang K\"ohler (1929, Gestalt psychology, New York, NY: Liveright) famously reported a bias in people's choice of nonsense words as labels for novel objects, pointing to possible na\"ive expectations about language structure. Two accounts have been offered to explain this bias, one focusing on the visuomotor effects of different vowel forms and the other focusing on variation in the acoustic structure and perceptual quality of different consonants. To date, evidence in support of both effects is mixed. Moreover, the veracity of either effect has often been doubted due to perceived limitations in methodologies and stimulus materials. A novel word-construction experiment is presented to test both proposed effects using randomized word- and image-generation techniques to address previous methodological concerns. Results show that participants are sensitive to both vowel and consonant content, constructing novel words of relatively sonorant consonants and rounded vowels to label curved object images, and of relatively plosive consonants and nonrounded vowels to label jagged object images. Results point to additional influences on word construction potentially related to the articulatory affordances or constraints accompanying different word forms.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZEZRFXJW/Nielsen and Rendall - 2013 - Parsing the role of consonants versus vowels in th.pdf},
  journal = {Canadian Journal of Experimental Psychology/Revue canadienne de psychologie exp\'erimentale},
  language = {en},
  number = {2}
}

@article{niewiadomskiAnalysisMovementQuality2019,
  title = {Analysis of {{Movement Quality}} in {{Full}}-{{Body Physical Activities}}},
  author = {Niewiadomski, Radoslaw and Kolykhalova, Ksenia and Piana, Stefano and Alborno, Paolo and Camurri, Antonio},
  year = {2019},
  month = feb,
  doi = {10.5281/zenodo.2564025},
  abstract = {Full-body human movement is characterized by fine-grain expressive qualities that humans are easily capable~of exhibiting and recognizing in others' movement. In sports (e.g., martial arts) and performing arts~(e.g., dance), the same sequence of movements can be performed in a wide range of ways characterized by~different qualities, often in terms of subtle (spatial and temporal) perturbations of the movement. Even a~non-expert observer can distinguish between a top-level and average performance by a dancer or martial~artist. The difference is not in the performed movements\textendash the same in both cases\textendash but in the ``quality'' of their~performance.~In this article, we present a computational framework aimed at an automated approximate measure of~movement quality in full-body physical activities. Starting from motion capture data, the framework computes~low-level (e.g., a limb velocity) and high-level (e.g., synchronization between different limbs) movement~features. Then, this vector of features is integrated to compute a value aimed at providing a quantitative assessment~of movement quality approximating the evaluation that an external expert observer would give of~the same sequence of movements.~Next, a system representing a concrete implementation of the framework is proposed. Karate is adopted~as a testbed.We selected two different katas (i.e., detailed choreographies of movements in karate) characterized~by different overall attitudes and expressions (aggressiveness, meditation), and we asked seven athletes,~having various levels of experience and age, to perform them. Motion capture data were collected from the~performances and were analyzed with the system. The results of the automated analysis were compared with~the scores given by 14 karate experts who rated the same performances. Results show that the movementquality scores computed by the system and the ratings given by the human observers are highly correlated (Pearson's correlations r = 0.84, p = 0.001 and r = 0.75, p = 0.005).},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9DWIFE9A/Niewiadomski et al. - 2019 - Analysis of Movement Quality in Full-Body Physical.pdf},
  keywords = {dance,full-body movement,gesture analysis,karate,movement quality},
  language = {eng}
}

@article{nykanenSketchingSoundsKinds2015,
  title = {Sketching Sounds \textendash{} {{Kinds}} of Listening and Their Functions in Designing},
  author = {Nyk{\"a}nen, Arne and Wingstedt, Johnny and Sundhage, Johan and Mohlin, Peter},
  year = {2015},
  month = jul,
  volume = {39},
  pages = {19--47},
  issn = {0142-694X},
  doi = {10.1016/j.destud.2015.04.002},
  abstract = {In this work, the use of sketching in sound design was studied. Based on Schon and Wiggins' model of how designers use sketching to see, move, and see again, we suggest that sound design evolves through a similar process requiring listening, moving, and listening again. This is facilitated by considering sounds as sketches. A case was followed in which six designers were asked to design a sound logotype. Processes and interactions were studied. The results suggest that sound design can be considered as a listen \textendash{} move \textendash{} listen process. Sound design is a conversation with sounding material, crucially dependent on listening. To assist in this, a computer interface was developed. Analysis of its use suggests that it supported co-designing.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/27LES8AF/Nykänen et al. - 2015 - Sketching sounds – Kinds of listening and their fu.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7TYF8JZ9/S0142694X15000277.html},
  journal = {Design Studies},
  keywords = {automotive design,design behaviour,design methodology,industrial design,sound sketching},
  language = {en}
}

@misc{ofcomTechnologyTracker2020,
  title = {Technology {{Tracker}}},
  author = {Ofcom},
  year = {2020},
  month = feb,
  copyright = {Open Government Licence},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GXGJXKRH/technology-tracker.html},
  howpublished = {https://data.gov.uk/dataset/eb673e35-1a59-47d3-b5f1-914a67d85baf/technology-tracker},
  language = {en}
}

@article{ojalaMultiresolutionGrayscaleRotation2002,
  title = {Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns},
  author = {Ojala, T. and Pietikainen, M. and Maenpaa, T.},
  year = {2002},
  month = jul,
  volume = {24},
  pages = {971--987},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2002.1017623},
  abstract = {\DH This paper presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \textordfeminine uniform,\textordmasculine{} are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \textordfeminine uniform\textordmasculine{} patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Excellent experimental results obtained in true problems of rotation invariance, where the classifier is trained at one particular rotation angle and tested with samples from other rotation angles, demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns. These operators characterize the spatial configuration of local image texture and the performance can be further improved by combining them with rotation invariant variance measures that characterize the contrast of local image texture. The joint distributions of these orthogonal measures are shown to be very powerful tools for rotation invariant texture analysis.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZWU6YUMS/Ojala et al. - 2002 - Multiresolution gray-scale and rotation invariant .pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {7}
}

@article{omodhrainFrameworkEvaluationDigital2011,
  title = {A {{Framework}} for the {{Evaluation}} of {{Digital Musical Instruments}}},
  author = {O'Modhrain, Sile},
  year = {2011},
  volume = {35},
  pages = {28--42},
  issn = {0148-9267},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/U5HRPZ6J/O'Modhrain - 2011 - A Framework for the Evaluation of Digital Musical .pdf},
  journal = {Computer Music Journal},
  number = {1}
}

@article{painterCanOutofcontextMusical,
  title = {Can Outofcontext Musical Sounds Convey Meaning? {{An ERP}} Study on the Processing of Meaning in Music},
  author = {Painter, Julia Grieser and Koelsch, Stefan},
  pages = {11},
  abstract = {There has been much debate over whether music can convey extra-musical meaning. The experiments presented here investigated whether low level musical features, specifically the timbre of a sound, have a direct access route to meaningful representations. Short musical sounds with varying timbres were investigated with regard to their ability to elicit meaningful associations, and the neural mechanisms underlying the meaningful processing of sounds were compared to those underlying the semantic processing of words. Two EEG experiments were carried out, and N400 effects were found for sound and word targets following sound and word primes in a semantic relatedness judgment task. No N400 effects were found in a memory task. The results show that even short musical sounds outside of a musical context are capable of conveying meaning information, but that sounds require more elaborate processing than other kinds of meaningful stimuli.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/VK5UELRY/Painter and Koelsch - Can outofcontext musical sounds convey meaning An.pdf},
  language = {en}
}

@patent{parisMultitouchGestureSearch2018,
  title = {Multi-Touch Gesture Search},
  author = {Paris, Jonathan and Rad, Leili Baghaei and Schillings, Benoit and Tang, Yuhuan},
  year = {2018},
  month = sep,
  assignee = {Verizon Media LLC},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/V8A47V88/Paris et al. - 2018 - Multi-touch gesture search.pdf},
  keywords = {area,interest,related content,search,user},
  language = {en},
  nationality = {US},
  number = {US10083238B2}
}

@inproceedings{parkWearableWirelessSensor2006,
  title = {A Wearable Wireless Sensor Platform for Interactive Art Performance},
  booktitle = {In to Appear, in {{Proc}}. {{Fourth Annual IEEE International Conference}} on {{Pervasive Computing}} and {{Communications}} ({{PerCom}}},
  author = {Park, Chulsung and Chou, Pai H.},
  year = {2006},
  pages = {13--17},
  abstract = {This paper reports on recent development of a wearable wireless sensor platform for interactive dance performances. At a fraction of a cubic-centimeter in volume, this platform is truly wearable and scalable in forming wireless networks. Integrated with a wide variety of different sensing devices, it is a real-time monitoring system for activities and physical conditions of the human body. The effectiveness of this platform is demonstrated with an interactive dance performance. 1},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/3C32X8WZ/Park and Chou - 2006 - A wearable wireless sensor platform for interactiv.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YS2KASES/summary.html}
}

@inproceedings{paulsonPaleoSketchAccuratePrimitive2008,
  title = {{{PaleoSketch}}: Accurate Primitive Sketch Recognition and Beautification},
  shorttitle = {{{PaleoSketch}}},
  booktitle = {Proceedings of the 13th International Conference on {{Intelligent}} User Interfaces},
  author = {Paulson, Brandon and Hammond, Tracy},
  year = {2008},
  month = jan,
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{Gran Canaria, Spain}},
  doi = {10.1145/1378773.1378775},
  abstract = {Sketching is a natural form of human communication and has become an increasingly popular tool for interacting with user interfaces. In order to facilitate the integration of sketching into traditional user interfaces, we must first develop accurate ways of recognizing users' intentions while providing feedback to catch recognition problems early in the sketching process. One approach to sketch recognition has been to recognize low-level primitives and then hierarchically construct higher-level shapes based on geometric constraints defined by the user, however, current low-level recognizers only handle a few number of primitive shapes. We propose a new low-level recognition and beautification system that can recognize eight primitive shapes, as well as combinations of these primitives, with recognition rates at 98.56\%. Our system also automatically generates beautified versions of these shapes to provide feedback early in the sketching process. In addition to looking at geometric perception, much of our recognition success can be attributed to two new features, along with a new ranking algorithm, which have proven to be significant in distinguishing polylines from curved segments.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/E34NRGTF/Paulson and Hammond - 2008 - PaleoSketch accurate primitive sketch recognition.pdf},
  isbn = {978-1-59593-987-6},
  keywords = {low-level processing,pen-based interfaces,shape beautification,sketch recognition},
  series = {{{IUI}} '08}
}

@misc{PDFGuidanceSurroundings,
  title = {({{PDF}}) {{Guidance}} and Surroundings Awareness in Outdoor Handheld Augmented Reality},
  doi = {http://dx.doi.org/10.1371/journal.pone.0230518},
  abstract = {PDF | Handheld and wearable devices are becoming ubiquitous in our lives and augmented reality technology is stepping out of the laboratory environment... | Find, read and cite all the research you need on ResearchGate},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FLSH3YWB/340049129_Guidance_and_surroundings_awareness_in_outdoor_handheld_augmented_reality.html},
  howpublished = {https://www.researchgate.net/publication/340049129\_Guidance\_and\_surroundings\_awareness\_in\_outdoor\_handheld\_augmented\_reality},
  journal = {ResearchGate},
  keywords = {augmented reality},
  language = {en}
}

@article{pearceModellingTimbralHardness2019,
  title = {Modelling {{Timbral Hardness}}},
  author = {Pearce, Andy and Brookes, Tim and Mason, Russell},
  year = {2019},
  volume = {9},
  pages = {466},
  issn = {2076-3417},
  doi = {10.3390/app9030466},
  abstract = {Hardness is the most commonly searched timbral attribute within freesound.org, a commonly used online sound effect repository. A perceptual model of hardness was developed to enable the automatic generation of metadata to facilitate hardness-based filtering or sorting of search results. A training dataset was collected of 202 stimuli with 32 sound source types, and perceived hardness was assessed by a panel of listeners. A multilinear regression model was developed on six features: maximum bandwidth, attack centroid, midband level, percussive-to-harmonic ratio, onset strength, and log attack time. This model predicted the hardness of the training data with R2 = 0.76. It predicted hardness within a new dataset with R2 = 0.57, and predicted the rank order of individual sources perfectly, after accounting for the subjective variance of the ratings. Its performance exceeded that of human listeners.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YFWYPG7Z/Pearce et al. - 2019 - Modelling Timbral Hardness.pdf},
  journal = {Applied Sciences},
  language = {en},
  number = {3}
}

@article{peerImplicitSPHFormulation2018,
  title = {An {{Implicit SPH Formulation}} for {{Incompressible Linearly Elastic Solids}}: {{Implicit Elastic SPH Solids}}},
  shorttitle = {An {{Implicit SPH Formulation}} for {{Incompressible Linearly Elastic Solids}}},
  author = {Peer, Andreas and Gissler, Christoph and Band, Stefan and Teschner, Matthias},
  year = {2018},
  month = sep,
  volume = {37},
  pages = {135--148},
  issn = {01677055},
  doi = {10.1111/cgf.13317},
  abstract = {We propose a novel SPH formulation for deformable solids. Key aspects of our method are implicit elastic forces and an adapted SPH formulation for the deformation gradient that\textemdash in contrast to previous work\textemdash allows a rotation extraction directly from the SPH deformation gradient. The proposed implicit concept is entirely based on linear formulations. As a linear strain tensor is used, a rotation-aware computation of the deformation gradient is required. In contrast to existing work, the respective rotation estimation is entirely realized within the SPH concept using a novel formulation with incorporated kernel gradient correction for first-order consistency.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/76ISABHL/Peer et al. - 2018 - An Implicit SPH Formulation for Incompressible Lin.pdf},
  journal = {Computer Graphics Forum},
  language = {en},
  number = {6}
}

@article{peetersTimbreToolboxExtracting2011,
  title = {The {{Timbre Toolbox}}: {{Extracting}} Audio Descriptors from Musical Signals},
  shorttitle = {The {{Timbre Toolbox}}},
  author = {Peeters, Geoffroy and Giordano, Bruno L. and Susini, Patrick and Misdariis, Nicolas and McAdams, Stephen},
  year = {2011},
  month = nov,
  volume = {130},
  pages = {2902--2916},
  issn = {0001-4966},
  doi = {10.1121/1.3642604},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZIK5KVDI/Peeters et al. - 2011 - The Timbre Toolbox Extracting audio descriptors f.pdf},
  journal = {The Journal of the Acoustical Society of America},
  language = {en},
  number = {5}
}

@inproceedings{pengWhyCanYou2018,
  title={Why Can You Hear a Difference between Pouring Hot and Cold Water? An Investigation of Temperature Dependence in Psychoacoustics},
  author={Peng, He and Reiss, Joshua D},
  booktitle={Audio Engineering Society Convention 145},
  year={2018},
  organization={Audio Engineering Society}
}

@misc{PeopleComputersVII,
  title = {People and {{Computers VII}} - {{British Computer Society}}. {{Human Computer Interaction Specialist Group}}. {{Conference}}, {{British Computer Society}} - {{Google Books}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CHBGRYLK/books.html},
  howpublished = {https://books.google.co.uk/books?hl=en\&lr=\&id=J83pppQhUbkC\&oi=fnd\&pg=PA391\&dq=\%22a+taxonomy+of+adaptive+user+interfaces\%22\&ots=Z1nAYdBZZY\&sig=3KxVi4sPpnMyfTaX5LUOQQc8NNM\#v=onepage\&q=\%22a\%20taxonomy\%20of\%20adaptive\%20user\%20interfaces\%22\&f=false}
}

@article{PhysicalActivityGuidelines,
  title = {Physical {{Activity Guidelines}} for {{Americans}}, 2nd Edition},
  pages = {118},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FU7FLF4M/Physical Activity Guidelines for Americans, 2nd ed.pdf},
  language = {en}
}

@article{pigremWeSpeakSensor,
  title = {Do {{We Speak Sensor}}? {{Cultural Constraints}} of {{Embodied Interaction}}},
  author = {Pigrem, Jon and McPherson, Andrew},
  pages = {4},
  abstract = {This paper explores the role of materiality in Digital Musical Instruments and questions the influence of tacit understandings of sensor technology. Existing research investigates the use of gesture, physical interaction and subsequent parameter mapping. We suggest that a tacit knowledge of the `sensor layer' brings with it definitions, understandings and expectations that forge and guide our approach to interaction. We argue that the influence of technology starts before a sound is made, and comes from not only intuition of material properties, but also received notions of what technology can and should do. On encountering an instrument with obvious sensors, a potential performer will attempt to predict what the sensors do and what the designer intends for them to do, becoming influenced by a machine centered understanding of interaction and not a solely material centred one. The paper presents an observational study of interaction using non-functional prototype instruments designed to explore fundamental ideas and understandings of instrumental interaction in the digital realm. We will show that this understanding influences both gestural language and ability to characterise an expected sonic/musical response.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FDD4G2QI/Pigrem and McPherson - Do We Speak Sensor Cultural Constraints of Embodi.pdf},
  language = {en}
}

@book{provenzanoAutoTuneLaborPopMusic2018,
  title = {Auto-{{Tune}}, {{Labor}}, and the {{Pop}}-{{Music Voice}}},
  author = {Provenzano, Catherine},
  year = {2018},
  month = oct,
  publisher = {{Oxford University Press}},
  abstract = {Long used in popular music to smooth vocal imperfections, Auto-Tune has become a much-discussed production tool since the early 2000s through artists including Cher, Daft Punk, and Kanye West. This chapter examines the relationship among artist skill, Auto-Tune, and reception. Artist T-Pain overtly used Auto-Tune to give his voice a synthetic, often robotic quality. Through T-Pain, overt use of Auto-Tune became associated with black music and was often reviled by the general public. T-Pain's acoustic performance on NPR's Tiny Desk Concert series redeemed him in the eyes of many listeners whose disdain for Auto-Tune arises from a belief that the technology erodes authenticity by making skillful singing irrelevant. In contrast, Taylor Swift's producers also use Auto-Tune as well, but rather than treating it as a special effect, they use it to correct intonation and amplify desirable vocal timbre. This use is also controversial, as Swift's recordings are often considered disingenuous.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LXZP9YAJ/oso-9780199985227-chapter-8.html},
  isbn = {978-0-19-090802-7},
  language = {en\_US}
}

@article{pucketteHandGestureTimbre,
  title = {Hand Gesture to Timbre Converter},
  author = {Puckette, Miller and Hagan, Kerry L},
  pages = {4},
  abstract = {In the design of a novel computer music instrument that uses the Leap Motion game controller to generate and play computer-generated sounds in real time, we consider the specific affordances that hand shapes bring to the control of time-varying timbres. The resulting instrument was used in a new piece, Who Was That Timbre I Saw You With?, performed as a live duet. Central to our approach are: a geometric exploration of the shape of hands themselves; a consideration of accuracy and speed of limb motion; and an appropriately designed sound generator and control parameter space.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SZFRMUES/Puckette and Hagan - Hand gesture to timbre converter.pdf},
  language = {en}
}

@misc{PythonKeyword,
  title = {Python Del {{Keyword}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PKVUIABC/ref_keyword_del.html},
  howpublished = {https://www.w3schools.com/python/ref\_keyword\_del.asp}
}

@misc{QuickStartGuide,
  title = {Quick Start Guide [{{Zotero Documentation}}]},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2CAWLUXN/quick_start_guide.html},
  howpublished = {https://www.zotero.org/support/quick\_start\_guide}
}

@misc{QuotencheckSportschau2018,
  title = {{Quotencheck: \guillemotleft Sportschau\guillemotright}},
  shorttitle = {{Quotencheck}},
  year = {2018},
  month = may,
  abstract = {Das Kult-Sportformat im Ersten zeigte sich 2018 stabil, obwohl die Reichweiten leicht sanken. Weiterhin bleibt die Samstags-\guillemotleft Sportschau\guillemotright{} aber eines der beliebtesten Vorabendprogramme.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZLGXR3XD/quotencheck-sportschau.html},
  howpublished = {http://www.quotenmeter.de/n/100964/quotencheck-sportschau},
  journal = {Quotenmeter},
  language = {de-DE}
}

@article{radziExtractionMovingObjects,
  title = {Extraction of {{Moving Objects Using Frame Differencing}}, {{Ghost}} and {{Shadow Removal}}},
  author = {Radzi, Syaimaa' Solehah Mohd and Yaakob, Shahrul Nizam and Kadim, Zulaikha and Woon, Hon Hock},
  pages = {6},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/U3B2UB8E/Radzi et al. - Extraction of Moving Objects Using Frame Differenc.pdf},
  language = {en}
}

@article{ramachandranSynaesthesiaWindowPerception,
  title={Synaesthesia--a window into perception, thought and language},
  author={Ramachandran, Vilayanur S and Hubbard, Edward M},
  journal={Journal of consciousness studies},
  volume={8},
  number={12},
  pages={3--34},
  year={2001},
  publisher={Imprint Academic}
}

@article{ramirezDeepLearningApproach2020,
  title = {A {{Deep Learning Approach}} to {{Intelligent Drum Mixing}} with the {{Wave}}-{{U}}-{{Net}}},
  author = {Ram{\i}rez, Marco A Mart{\i}nez and Stoller, Daniel and Moffat, David},
  year = {2020},
  volume = {1},
  pages = {9},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GLYHN8A7/Ramırez et al. - 2020 - A Deep Learning Approach to Intelligent Drum Mixin.pdf},
  language = {en},
  number = {1}
}

@article{rappStrengtheningGamificationStudies2019,
  title = {Strengthening Gamification Studies: {{Current}} Trends and Future Opportunities of Gamification Research},
  shorttitle = {Strengthening Gamification Studies},
  author = {Rapp, Amon and Hopfgartner, Frank and Hamari, Juho and Linehan, Conor and Cena, Federica},
  year = {2019},
  month = jul,
  volume = {127},
  pages = {1--6},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2018.11.007},
  abstract = {Gamification is now a well-established technique in Human-Computer Interaction. However, research on gamification still faces a variety of empirical and theoretical challenges. Firstly, studies of gamified systems typically focus narrowly on understanding individuals. short-term interactions with the system, ignoring more difficult to measure outcomes. Secondly, academic research on gamification has been slow to improve the techniques through which gamified applications are designed. Third, current gamification research lacks a critical lens capable of exploring unintended consequences of designs. The 14 articles published in this special issue face these challenges with great methodological rigor. We summarize them by identifying three main themes: the determination to improve the quality and usefulness of theory in the field of gamification, the improvements in design practice, and the adoption of a critical gaze to uncover side-effects of gamification designs. We conclude by providing an overview of the questions that we feel must be addressed by future work in gamification. Gamification studies would benefit from a wider use of theories to account for the complexity of human behavior, a more thorough exploration of the many opportunities coming from the world of games, and an ethical reflection on the use of game design elements in serious domains.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/I953X2CJ/Rapp et al. - 2019 - Strengthening gamification studies Current trends.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7T3AH8J2/S1071581918306712.html},
  journal = {International Journal of Human-Computer Studies},
  keywords = {Game elements,Gameful design,gamification,Gamification},
  language = {en},
  series = {Strengthening Gamification Studies: Critical Challenges and New Opportunities}
}

@inproceedings{rasmussenSketchingShapechangingInterfaces2016,
  title = {Sketching {{Shape}}-Changing {{Interfaces}}: {{Exploring Vocabulary}}, {{Metaphors Use}}, and {{Affordances}}},
  shorttitle = {Sketching {{Shape}}-Changing {{Interfaces}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rasmussen, Majken K. and Troiano, Giovanni M. and Petersen, Marianne G. and Simonsen, Jakob G. and Hornb{\ae}k, Kasper},
  year = {2016},
  month = may,
  pages = {2740--2751},
  publisher = {{ACM}},
  address = {{San Jose California USA}},
  doi = {10.1145/2858036.2858183},
  abstract = {Shape-changing interfaces allow designers to create user interfaces that physically change shape. However, presently, we lack studies of how such interfaces are designed, as well as what high-level strategies, such as metaphors and affordances, designers use. This paper presents an analysis of sketches made by 21 participants designing either a shape-changing radio or a shapechanging mobile phone. The results exhibit a range of interesting design elements, and the analysis points to a need to further develop or revise existing vocabularies for sketching and analyzing movement. The sketches show a prevalent use of metaphors, say, for communicating volume though big-is-on and small-is-off, as well as a lack of conventions. Furthermore, the affordances used were curiously asymmetrical compared to those offered by nonshape-changing interfaces. We conclude by offering implications on how our results can influence future research on shape-changing interfaces.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2J3RMCRR/Rasmussen et al. - 2016 - Sketching Shape-changing Interfaces Exploring Voc.pdf},
  isbn = {978-1-4503-3362-7},
  language = {en}
}

@article{redheadInteractiveMusicProducer,
  title = {The {{Interactive Music Producer}}},
  author = {Redhead, Tracy},
  abstract = {This paper introduces an investigation into the emerging role of the interactive music producer (IMP). This role has arisen as interactive technologies offer artists and audiences\&\#39; new tools and opportunities to completely reinvent forms of},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/L7HQCB5L/The_Interactive_Music_Producer.html},
  language = {en}
}

@article{redheadInteractiveMusicProducer2016,
  title = {The {{Interactive Music Producer}}},
  author = {Redhead, Tracy},
  year = {2016},
  pages = {9},
  abstract = {This paper introduces an investigation into the emerging role of the interactive music producer (IMP). This role has arisen as interactive technologies offer artists and audiences' new tools and opportunities to completely reinvent forms of music. The definition of an IMP's role will be clarified, evaluated and explored throughout this research. The research identifies the necessary skills underpinning cutting edge contemporary music practice. Focused research towards artistic approaches of interactive music making are needed due to the complex nature of developing interactive products, which usually involves custom-built software and expert interdisciplinary skills. This creates a huge problem for musicians and the music industry wanting to embrace changes in technology and communication. This research aims to address this problem by providing a new role that merges interactive technologies with traditional music making. The paper will introduce preliminary findings resulting from three creative works, utilising gestures, VR and performance. The creative practice involved building on existing skills in software and coding, producing interfaces and patches to experiment with while composing, recording, arranging and producing works that are fluid, adaptive and dynamic with audience interaction. By taking a bricoleur approach the researcher / artist is able to directly access potential impacts interactive technologies may have on contemporary music making and practice.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GNSKWEZE/Redhead - 2016 - The Interactive Music Producer.pdf},
  language = {en}
}

@inproceedings{reevesDesigningSpectatorExperience2005,
  title = {Designing the {{Spectator Experience}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Reeves, Stuart and Benford, Steve and O'Malley, Claire and Fraser, Mike},
  year = {2005},
  pages = {741--750},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1054972.1055074},
  abstract = {Interaction is increasingly a public affair, taking place in our theatres, galleries, museums, exhibitions and on the city streets. This raises a new design challenge for HCI - how should spectators experience a performer's interaction with a computer? We classify public interfaces (including examples from art, performance and exhibition design) according to the extent to which a performer's manipulations of an interface and their resulting effects are hidden, partially revealed, fully revealed or even amplified for spectators. Our taxonomy uncovers four broad design strategies: 'secretive,' where manipulations and effects are largely hidden; 'expressive,' where they tend to be revealed enabling the spectator to fully appreciate the performer's interaction; 'magical,' where effects are revealed but the manipulations that caused them are hidden; and finally 'suspenseful,' where manipulations are apparent but effects are only revealed as the spectator takes their turn.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Z5YQQJ6A/Reeves et al. - 2005 - Designing the Spectator Experience.pdf},
  isbn = {978-1-58113-998-3},
  keywords = {art,design framework,expression,galleries,magic,museums,performance,public experiences,spectators},
  series = {{{CHI}} '05}
}

@inproceedings{rematasSoccerYourTabletop2018,
  title = {Soccer on {{Your Tabletop}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rematas, Konstantinos and {Kemelmacher-Shlizerman}, Ira and Curless, Brian and Seitz, Steve},
  year = {2018},
  month = jun,
  pages = {4738--4747},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00498},
  abstract = {We present a system that transforms a monocular video of a soccer game into a moving 3D reconstruction, in which the players and field can be rendered interactively with a 3D viewer or through an Augmented Reality device. At the heart of our paper is an approach to estimate the depth map of each player, using a CNN that is trained on 3D player data extracted from soccer video games. We compare with state of the art body pose and depth estimation techniques, and show results on both synthetic ground truth benchmarks, and real YouTube soccer footage.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RYFGJYQJ/Rematas et al. - 2018 - Soccer on Your Tabletop.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{bresenham1965algorithm,
  title={Algorithm for computer control of a digital plotter},
  author={Bresenham, Jack E},
  journal={IBM Systems journal},
  volume={4},
  number={1},
  pages={25--30},
  year={1965},
  publisher={IBM}
}

@article{koo2016guideline,
  title={A guideline of selecting and reporting intraclass correlation coefficients for reliability research},
  author={Koo, Terry K and Li, Mae Y},
  journal={Journal of chiropractic medicine},
  volume={15},
  number={2},
  pages={155--163},
  year={2016},
  publisher={Elsevier}
}

@misc{ResearchGate,
  title = {{{ResearchGate}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/B8IQATEG/download.html},
  howpublished = {https://www.researchgate.net/publication/340049129\_Guidance\_and\_surroundings\_awareness\_in\_outdoor\_handheld\_augmented\_reality/link/5e74fc5d4585153370b9fc95/download}
}

@article{richanProposalEvaluationNew2020,
  title = {A Proposal and Evaluation of New Timbre Visualisation Methods for Audio Sample Browsers},
  author = {Richan, Etienne and Rouat, Jean},
  year = {2020},
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-020-01388-1},
  abstract = {Searching through vast libraries of sound samples can be a daunting and time-consuming task. Modern audio sample browsers use mappings between acoustic properties and visual attributes to visually differentiate displayed items. There are few studies focused on how well these mappings help users search for a specific sample. We propose new methods for generating textural labels and positioning samples based on perceptual representations of timbre. We perform a series of studies to evaluate the benefits of using shape, color or texture as labels in a known-item search task. We describe the motivation and implementation of the study, and present an in-depth analysis of results. We find that shape significantly improves task performance, while color and texture have little effect. We also compare results between in-person and online participants and propose research directions for further studies.},
  archivePrefix = {arXiv},
  eprint = {2011.15096},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/C4MKLN8B/Richan and Rouat - 2020 - A proposal and evaluation of new timbre visualisat.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/WLHG2K75/2011.html},
  journal = {Personal and Ubiquitous Computing},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,H.5.2,H.5.5}
}

@article{richanTranslatingPerceptualDistance2018,
  title = {Towards Translating Perceptual Distance from Hearing to Vision Using Neural Networks},
  author = {Richan, Etienne and Rouat, Jean},
  year = {2018},
  pages = {2},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7VZ94LGI/Richan and Rouat - 2018 - Towards translating perceptual distance from heari.pdf},
  language = {en}
}

@misc{RitualsMagicInteractive,
  title = {From Rituals to Magic\_ {{Interactive}} Art and {{HCI}} of the Past, Present, and Future | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.ijhcs.2019.06.005},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SHS8GAPP/S1071581919300758.html},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S1071581919300758?token=BAE140D7A247148E52E41A9CA59034079D31BDFAA1645E2BBB9729631F5042FB85FE8F9C02586E10740C2EEC27E3800D},
  language = {en}
}

@article{rodetSPECTRALENVELOPESINVERSE,
  title = {{{SPECTRAL ENVELOPES AND INVERSE FFT SYNTHESIS}}},
  author = {Rodet, X and Depalle, P},
  pages = {11},
  abstract = {We present a new additive synthesis method based on spectral envelopes and inverse Fast Fourier Transform (FFT-1). User control is facilitated by the use of spectral envelopes to describe the characteristics of the short term spectrum of the sound in terms of sinusoidal and noise components. Such characteristics can be given by users or obtained automatically from natural sounds. Use of the inverse FFT reduces the computation cost by a factor on the order of 15 compared to oscillators. We propose a low cost real-time synthesizer design allowing processing of recorded and live sounds, synthesis of instruments and synthesis of speech and the singing voice.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SW2BZ9PX/Rodet and Depalle - SPECTRAL ENVELOPES AND INVERSE FFT SYNTHESIS.pdf},
  keywords = {inverse fft synthesis},
  language = {en}
}

@inproceedings{rodetSpectralEnvelopesInverse1992,
  title = {Spectral {{Envelopes}} and {{Inverse FFT Synthesis}}},
  booktitle = {Audio {{Engineering Society Convention}} 93},
  author = {Rodet, X. and Depalle, P.},
  year = {1992},
  month = oct,
  publisher = {{Audio Engineering Society}},
  abstract = {Presented here is a new additive synthesis method based on spectral envelopes and inverse fast Fourier transform (FFT-1). User control is facilitated by the use of spectral envelopes to describe the characteristics of the short term spectrum of the sound in terms of sinusoidal and noise components. Such characteristics can be given by users or obtained automatically from natural sounds. Use of the inverse FFT reduces the computation cost by a factor on the order of 15 compared to oscillators....},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/I9H6MMY3/browse.html},
  keywords = {inverse ff},
  language = {English}
}

@inproceedings{romaMusicPerformanceDiscovering2015,
  title = {Music Performance by Discovering Community Loops},
  author = {Roma, Gerard and Serra, Xavier},
  year = {2015},
  abstract = {Technologies for discovering sounds in large databases can help breaking the boundary between exploration and music performance. In this paper, we present a system for exploring loops from Freesound. Sound files are grouped by their most common repetition periods, so that they can be played in sync. A graph layout algorithm is used to organize sounds in a two-dimensional plane so that loops with similar timbre are spatially close. The result is a system that can be used as a musical instrument: since sounds will always play in sync, the user can freely explore the variety of sounds uploaded by the Freesound community, while continuously producing a rhythmic music stream.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ULSMZD2Q/Roma and Serra - 2015 - Music performance by discovering community loops.pdf},
  keywords = {Algorithm,Database,Force-directed graph drawing,Graph - visual representation}
}

@inproceedings{rovanInstrumentalGesturalMapping1997,
  title = {Instrumental {{Gestural Mapping Strategies}} as {{Expressivity Determinants}} in {{Computer Music Performance}}},
  booktitle = {In {{Proceedings}} of {{Kansei}} - the {{Technology}} of {{Emotion Workshop}}},
  author = {Rovan, Joseph Butch and Wanderley, Marcelo M. and Dubnov, Shlomo and Depalle, Philippe},
  year = {1997},
  pages = {3--4},
  abstract = {This paper presents ongoing work on gesture mapping strategies and applications to sound synthesis by signal models controlled via a standard MIDI wind controller. Our approach consists in considering different mapping strategies in order to achieve "fine" (therefore in the authors' opinion, potentially expressive) control of additive synthesis by coupling originally independent outputs from the wind controller. These control signals are applied to nine different clarinet data files, obtained from analysis of clarinet sounds, which are arranged in an expressive timbral subspace and interpolated in real-time, using FTS 1.4, IRCAM's digital signal processing environment. An analysis of the resulting interpolation is also provided and topics related to sound morphing techniques are discussed.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FVSY4HSD/Rovan et al. - 1997 - Instrumental Gestural Mapping Strategies as Expres.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/KMKBC3Y5/summary.html}
}

@article{ryanWhatLifeReally2017,
  title = {What Is Life Really like for Disabled People? {{The}} Disability Diaries Reveal All},
  shorttitle = {What Is Life Really like for Disabled People?},
  author = {Ryan, Frances},
  year = {2017},
  month = nov,
  issn = {0261-3077},
  abstract = {We asked seven people to keep diaries for a month to document the reality of being disabled in Britain today. Frances Ryan reflects on the issues that arose \textendash{} public transport, employment, housing, attitudes \textendash{} and meets four of the diarists},
  chapter = {Inequality},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/IHUFUIER/whats-life-really-like-for-disabled-peopld-disability-diaries-reveal-all.html},
  journal = {The Guardian},
  keywords = {Disability,Inequality,mobility issues,Society,UK news},
  language = {en-GB}
}

@article{saitisTimbreSemanticsLens2018,
  title={Timbre semantics through the lens of crossmodal correspondences: A new way of asking old questions},
  author={Saitis, Charalampos and Weinzierl, Stefan and von Kriegstein, Katharina and Ystad, S{\o}lvi and Cuskley, Christine},
  journal={Acoustical Science and Technology},
  volume={41},
  number={1},
  pages={365--368},
  year={2020},
  publisher={ACOUSTICAL SOCIETY OF JAPAN}
}

@article{SantanderFirstTimeBuyer,
  title = {Santander {{First}}-{{Time Buyer Study}}. {{The}} Future of the Homeownership Dream.},
  pages = {25},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PBNEB3KK/Santander First-Time Buyer Study. The future of th.pdf},
  keywords = {life goals},
  language = {en}
}

@article{SantanderFirstTimeBuyera,
  title = {Santander {{First}}-{{Time Buyer Study}}. {{The}} Future of the Homeownership Dream.},
  pages = {25},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/WNPPNZX5/Santander First-Time Buyer Study. The future of th.pdf},
  language = {en}
}

@inproceedings{schedlIntelligentUserInterfaces2017,
  title = {Intelligent {{User Interfaces}} for {{Social Music Discovery}} and {{Exploration}} of {{Large}}-Scale {{Music Repositories}}},
  booktitle = {Proceedings of the 2017 {{ACM Workshop}} on {{Theory}}-{{Informed User Modeling}} for {{Tailoring}} and {{Personalizing Interfaces}}},
  author = {Schedl, Markus},
  year = {2017},
  pages = {7--11},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3039677.3039678},
  abstract = {In this position paper, we address the question of how to make music search and discovery more appealing, more exciting, and more joyful. In particular, we argue to research methods that foster serendipitous encounters with music items and to integrate ways for social interaction while exploring music collections and discovering the gems in today's huge catalogs available through online streaming platforms. We identify two major challenges here: the need for (i) highly efficient clustering and information visualization techniques that scale to these music catalogs and (ii) novel user interfaces that explain the clustering of music items and provide means to make the exploration of music a social event.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/U3FCTF6N/Schedl - 2017 - Intelligent User Interfaces for Social Music Disco.pdf},
  isbn = {978-1-4503-4905-5},
  keywords = {intelligent user interfaces,Intelligent User Interfaces,interaction,MIR,music browsing,music discovery,music search},
  series = {{{HUMANIZE}} '17}
}

@book{schedlMusicInformationRetrieval2014,
  title = {Music Information Retrieval: Recent Developments and Applications},
  shorttitle = {Music Information Retrieval},
  author = {Schedl, Markus and G{\'o}mez, Emilia and Urbano, Juli{\'a}n},
  year = {2014},
  publisher = {{Now Publ}},
  address = {{Boston, Mass.}},
  annotation = {OCLC: 931957204},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8RDXS6IK/Schedl et al. - 2014 - Music information retrieval recent developments a.pdf},
  isbn = {978-1-60198-806-5},
  language = {en},
  number = {8,2/3},
  series = {Foundations and Trends in Information Retrieval}
}

@inproceedings{schwarzDescriptorbasedSoundTexture2010,
  title = {Descriptor-Based {{Sound Texture Sampling}}},
  booktitle = {Sound and {{Music Computing}} ({{SMC}})},
  author = {Schwarz, Diemo and Schnell, Norbert},
  year = {2010},
  month = jul,
  pages = {510--515},
  address = {{Barcelona, Spain}},
  abstract = {Existing methods for sound texture synthesis are often concerned with the extension of a given recording, while keeping its overall properties and avoiding artefacts. However, they generally lack controllability of the resulting sound texture. After a review and classification of existing approaches, we propose two methods of statistical modeling of the audio descriptors of texture recordings using histograms and Gaussian mixture models. The models can be interpolated to steer the evolution of the sound texture between different target recordings (e.g. from light to heavy rain). Target descriptor values are stochastically drawn from the statistic models by inverse transform sampling to control corpus-based concatenative synthesis for the final sound generation, that can also be controlled interactively by navigation through the descriptor space. To better cover the target descriptor space, we expand the corpus by automatically generating variants of the source sounds with transformations applied, and storing only the resulting descriptors and the transformation parameters in the corpus.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZZ5VC427/Schwarz and Schnell - 2010 - Descriptor-based Sound Texture Sampling.pdf},
  keywords = {audio descriptors,concatenative synthesis,content-based retrieval,corpus-based synthesis,databases,feature based synthesis,Gaussian Mixture Models,Informatique musicale,sound textures,statistic modeling}
}

@article{schwarzSoundSpaceMusical,
  title = {The {{Sound Space}} as {{Musical Instrument}}: {{Playing Corpus}}-{{Based Concatenative Synthesis}}},
  author = {Schwarz, Diemo},
  pages = {6},
  abstract = {Corpus-based concatenative synthesis is a fairly recent sound synthesis method, based on descriptor analysis of any number of existing or live-recorded sounds, and synthesis by selection of sound segments from the database matching given sound characteristics. It is well described in the literature, but has been rarely examined for its capacity as a new interface for musical expression. The interesting outcome of such an examination is that the actual instrument is the space of sound characteristics, through which the performer navigates with gestures captured by various input devices. We will take a look at different types of interaction modes and controllers (positional, inertial, audio analysis) and the gestures they afford, and provide a critical assessment of their musical and expressive capabilities, based on several years of musical experience, performing with the CataRT system for real-time CBCS.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YAR9NNGF/Schwarz - The Sound Space as Musical Instrument Playing Cor.pdf},
  language = {en}
}

@inproceedings{schwarzSoundSpaceMusical2012,
  title = {The {{Sound Space}} as {{Musical Instrument}}: {{Playing Corpus}}-{{Based Concatenative Synthesis}}},
  shorttitle = {The {{Sound Space}} as {{Musical Instrument}}},
  booktitle = {New {{Interfaces}} for {{Musical Expression}} ({{NIME}})},
  author = {Schwarz, Diemo},
  year = {2012},
  month = may,
  pages = {250--253},
  address = {{Ann Arbour, United States}},
  abstract = {Corpus-based concatenative synthesis is a fairly recent sound synthesis method, based on descriptor analysis of any number of existing or live-recorded sounds, and synthesis by selection of sound segments from the database matching given sound characteristics. It is well described in the literature, but has been rarely examined for its capacity as a new interface for musical expression. The interesting outcome of such an examination is that the actual instrument is the space of sound characteristics, through which the performer navigates with gestures captured by various input devices. We will take a look at different types of interaction modes and controllers (positional, inertial, audio analysis) and the gestures they afford, and provide a critical assessment of their musical and expressive capabilities, based on several years of musical experience, performing with the CataRT system for real-time CBCS.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/BQGZT29M/Schwarz - 2012 - The Sound Space as Musical Instrument Playing Cor.pdf},
  keywords = {CataRT,concatenative synthesis,corpus-based concatenative synthesis,gesture,Informatique musicale}
}

@incollection{seagoNewInteractionStrategy2013,
  title = {A {{New Interaction Strategy}} for {{Musical Timbre Design}}},
  booktitle = {Music and {{Human}}-{{Computer Interaction}}},
  author = {Seago, Allan},
  editor = {Holland, Simon and Wilkie, Katie and Mulholland, Paul and Seago, Allan},
  year = {2013},
  pages = {153--169},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-2990-5_9},
  abstract = {Sound creation and editing in hardware and software synthesizers presents usability problems and a challenge for HCI research. Synthesis parameters vary considerably in their degree of usability, and musical timbre itself is a complex and multidimensional attribute of sound. This chapter presents a user-driven searchbased interaction style where the user engages directly with sound rather than with a mediating interface layer. Where the parameters of a given sound synthesis method do not readily map to perceptible sonic attributes, the search algorithm offers an alternative means of timbre specification and control. However, it is argued here that the method has wider relevance for interaction design in search domains which are generally well-ordered and understood, but whose parameters do not afford a useful or intuitive means of search.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PRKKHE5V/Seago - 2013 - A New Interaction Strategy for Musical Timbre Desi.pdf},
  isbn = {978-1-4471-2989-9 978-1-4471-2990-5},
  language = {en}
}

@article{sezginFeaturePointDetection,
  title = {Feature {{Point Detection}} and {{Curve Approximation}} for {{Early Processing}} of {{Free}}-{{Hand Sketches}}},
  author = {Sezgin, Tevfik Metin},
  pages = {77},
  abstract = {Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for feature detection in strokes and apply this technique using two approaches to signal processing, one using simple average based thresholding and a second using scale space.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8AH958ZF/Sezgin - Feature Point Detection and Curve Approximation fo.pdf},
  keywords = {sketch recognition},
  language = {en}
}

@inproceedings{sezginSketchBasedInterfaces2007,
  title = {Sketch Based Interfaces: Early Processing for Sketch Understanding},
  shorttitle = {Sketch Based Interfaces},
  booktitle = {{{ACM SIGGRAPH}} 2007 Courses},
  author = {Sezgin, Tevfik Metin and Stahovich, Thomas and Davis, Randall},
  year = {2007},
  month = aug,
  pages = {37--es},
  publisher = {{Association for Computing Machinery}},
  address = {{San Diego, California}},
  doi = {10.1145/1281500.1281548},
  abstract = {Freehand sketching is a natural and crucial part of everyday human interaction, yet is almost totally unsupported by current user interfaces. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer, to produce a user interface for design that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in a sketch into the intended geometric objects. In this paper we describe an implemented system that combines multiple sources of knowledge to provide robust early processing for freehand sketching.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/65UBI5VM/Sezgin et al. - 2007 - Sketch based interfaces early processing for sket.pdf},
  isbn = {978-1-4503-1823-5},
  keywords = {freehand sketching,multiple sources of knowledge,natural interaction,sketch recognition},
  series = {{{SIGGRAPH}} '07}
}

@misc{ShapeComparisonUsing,
  title = {Shape {{Comparison}} Using {{Turning Functions}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/W3K6LHF7/turningfunctions.html},
  howpublished = {https://sites.google.com/site/turningfunctions/},
  keywords = {shape comparison,turning function}
}

@article{shierUniversityVictoriaVictoria2017,
  title = {University of {{Victoria Victoria}}, {{British Columbia}}},
  author = {Shier, Jordie and McNally, Kirk and Tzanetakis, George},
  year = {2017},
  pages = {5},
  abstract = {The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. To be efficient, users of these large collections require new tools to assist them in sorting, selection and auditioning tasks. This paper presents a new plugin for working with a large collection of kick and snare samples within a music production context. A database of 4230 kick and snare samples, representing 250 individual electronic drum machines are analyzed by segmenting the audio samples into different sample lengths and characterizing these segments using audio feature analysis. The resulting multidimensional feature space is reduced using principle component analysis (PCA). Samples are mapped to a 2D grid interface within an audio plug-in built using the JUCE software framework.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NJWGXMNK/Shier et al. - 2017 - University of Victoria Victoria, British Columbia.pdf},
  language = {en}
}

@article{shinoharaTaketeMalumaAction2016,
  title = {Takete and {{Maluma}} in {{Action}}: {{A Cross}}-{{Modal Relationship}} between {{Gestures}} and {{Sounds}}},
  shorttitle = {Takete and {{Maluma}} in {{Action}}},
  author = {Shinohara, Kazuko and Yamauchi, Naoto and Kawahara, Shigeto and Tanaka, Hideyuki},
  year = {2016},
  month = sep,
  volume = {11},
  pages = {e0163525},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0163525},
  abstract = {Despite Saussure's famous observation that sound-meaning relationships are in principle arbitrary, we now have a substantial body of evidence that sounds themselves can have meanings, patterns often referred to as ``sound symbolism''. Previous studies have found that particular sounds can be associated with particular meanings, and also with particular static visual shapes. Less well studied is the association between sounds and dynamic movements. Using a free elicitation method, the current experiment shows that several sound symbolic associations between sounds and dynamic movements exist: (1) front vowels are more likely to be associated with small movements than with large movements; (2) front vowels are more likely to be associated with angular movements than with round movements; (3) obstruents are more likely to be associated with angular movements than with round movements; (4) voiced obstruents are more likely to be associated with large movements than with small movements. All of these results are compatible with the results of the previous studies of sound symbolism using static images or meanings. Overall, the current study supports the hypothesis that particular dynamic motions can be associated with particular sounds. Building on the current results, we discuss a possible practical application of these sound symbolic associations in sports instructions.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9AJ6KDBW/Shinohara et al. - 2016 - Takete and Maluma in Action A Cross-Modal Relatio.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/U9ZKMD7U/article.html},
  journal = {PLOS ONE},
  keywords = {Acceleration,cross-modal mapping,gesture,Hands,Kinematics,Motion,Musculoskeletal system,perception,Semiotics,Sensory perception,Vowels},
  language = {en},
  number = {9}
}

@article{sicchioHackingChoreographyDance2014,
  title = {Hacking {{Choreography}}: {{Dance}} and {{Live Coding}}},
  shorttitle = {Hacking {{Choreography}}},
  author = {Sicchio, Kate},
  year = {2014},
  month = apr,
  volume = {38},
  pages = {31--39},
  issn = {1531-5169},
  abstract = {This article explores the intersection of live coding and choreography, discussing the "practice as research" project Hacking Choreography. It examines the use of computer programming languages within dance scores, the creation of scores in real time, and the transparency of these scores to the audience during performance. Four pieces created by the author are discussed in terms of these elements and compared to live-coding practices for computer music. Through this, not only does live coding emerge as a performance practice related to sound or visuals, but it also continues its trajectory as a transdisciplinary approach to live performance events.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/TEHKJZXG/Sicchio - 2014 - Hacking Choreography Dance and Live Coding.pdf},
  journal = {Computer Music Journal},
  language = {en},
  number = {1}
}

@article{sicchioSoundChoreographyBody2017,
  title = {{\emph{Sound }}{{{\emph{Choreography}}}}{\emph{ {$<>$} }}{{{\emph{Body Code}}}} : {{Software Deployment}} and {{Notational Engagement}} without {{Trace}}},
  shorttitle = {{\emph{Sound }}{{{\emph{Choreography}}}}{\emph{ {$<>$} }}{{{\emph{Body Code}}}}},
  author = {Sicchio, Kate and McLean, Alex},
  year = {2017},
  month = jul,
  volume = {27},
  pages = {405--410},
  issn = {1048-6801, 1477-2264},
  doi = {10.1080/10486801.2017.1343244},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Y6BUXN4B/Sicchio and McLean - 2017 - iSound Choreography  Body Codei  Software .pdf},
  journal = {Contemporary Theatre Review},
  language = {en},
  number = {3}
}

@article{sidhuWhatNameSound2015,
  title = {What's in a {{Name}}? {{Sound Symbolism}} and {{Gender}} in {{First Names}}},
  shorttitle = {What's in a {{Name}}?},
  author = {Sidhu, David M. and Pexman, Penny M.},
  year = {2015},
  month = may,
  volume = {10},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0126809},
  abstract = {Although the arbitrariness of language has been considered one of its defining features, studies have demonstrated that certain phonemes tend to be associated with certain kinds of meaning. A well-known example is the Bouba/Kiki effect, in which nonwords like bouba are associated with round shapes while nonwords like kiki are associated with sharp shapes. These sound symbolic associations have thus far been limited to nonwords. Here we tested whether or not the Bouba/Kiki effect extends to existing lexical stimuli; in particular, real first names. We found that the roundness/sharpness of the phonemes in first names impacted whether the names were associated with round or sharp shapes in the form of character silhouettes (Experiments 1a and 1b). We also observed an association between femaleness and round shapes, and maleness and sharp shapes. We next investigated whether this association would extend to the features of language and found the proportion of round-sounding phonemes was related to name gender (Analysis of Category Norms). Finally, we investigated whether sound symbolic associations for first names would be observed for other abstract properties; in particular, personality traits (Experiment 2). We found that adjectives previously judged to be either descriptive of a figuratively `round' or a `sharp' personality were associated with names containing either round- or sharp-sounding phonemes, respectively. These results demonstrate that sound symbolic associations extend to existing lexical stimuli, providing a new example of non-arbitrary mappings between form and meaning.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8ZSW3CN8/Sidhu and Pexman - 2015 - What’s in a Name Sound Symbolism and Gender in Fi.pdf},
  journal = {PLoS ONE},
  number = {5},
  pmcid = {PMC4446333},
  pmid = {26016856}
}

@article{siedenburgComparisonApproachesTimbre2016,
  title={A comparison of approaches to timbre descriptors in music information retrieval and music psychology},
  author={Siedenburg, Kai and Fujinaga, Ichiro and McAdams, Stephen},
  journal={Journal of New Music Research},
  volume={45},
  number={1},
  pages={27--41},
  year={2016},
  publisher={Taylor \& Francis}
}

@misc{SketchBasedImageRetrieval,
  title = {Sketch-{{Based Image Retrieval}} by {{Salient Contour Reinforcement}} | {{IEEE Transactions}} on {{Multimedia}}},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/S3QAWJKN/TMM.2016.html},
  howpublished = {https://dl.acm.org/doi/abs/10.1109/TMM.2016.2568138}
}

@article{skoldCOMBININGSOUNDPITCHBASED,
  title = {{{COMBINING SOUND}}- {{AND PITCH}}-{{BASED NOTATION FOR TEACHING AND COMPOSITION}}},
  author = {Skold, Mattias},
  pages = {6},
  abstract = {My research is concerned with finding a common notation for pitch-based, sound-based and spatialized music in an attempt to bridge the gap between acoustic and electronic music, also working towards the possibility of a holistic system for algorithmic composition based on music representation. This paper describes the first step towards this goal, focusing on the combination of pitch-based and sound-based musical structures, introducing a graphical notation system that combines traditional music notation with electroacoustic music analysis notation. I present how this was tested in practice in a case study within the framework of composition education at the Royal College of Music in Stockholm, where composition students were working with, and reacting to, the system.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GPE4D4AY/Skold - COMBINING SOUND- AND PITCH-BASED NOTATION FOR TEAC.pdf},
  keywords = {music notation},
  language = {en}
}

@article{smalleyDefiningTimbreRefining1994,
  title = {Defining Timbre \textemdash{} {{Refining}} Timbre},
  author = {Smalley, Denis},
  year = {1994},
  month = jan,
  volume = {10},
  pages = {35--48},
  publisher = {{Routledge}},
  issn = {0749-4467},
  doi = {10.1080/07494469400640281},
  abstract = {Timbre is defined as the attribution of spectromorphological identity. Electroacoustic music experience, particularly acousmatic music, questions the viability of a notion of timbre. Of primary significance is the traditional linking of timbre to the source and cause of a sound: the concept of source-cause texture is introduced to define this link. The ambiguous relationship between pitch and timbre indicates that timbre cannot be defined as that part of the sound which is not pitch. Problems in establishing the existence of sonic identities within the musical context, and of maintaining the coherence of identities are discussed, opening up questions relating to musical discourse. Two types of discourse \textemdash{} transformational and typological \textemdash{} are defined, leading to the notion of generic timbre. In a spectromorphological music where defining identities becomes problematic it becomes impossible to disentangle timbre from discourse: here the notion of timbre has a limited viability.},
  annotation = {\_eprint: https://doi.org/10.1080/07494469400640281},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JVWK2MW8/Smalley - 1994 - Defining timbre — Refining timbre.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FRJ9BVCV/07494469400640281.html},
  journal = {Contemporary Music Review},
  keywords = {discourse,identity,registration,sources and causes,timbre,transformation,typology},
  number = {2}
}

@article{smalleySpectromorphologyExplainingSoundshapes1997,
  title = {Spectromorphology: Explaining Sound-Shapes},
  shorttitle = {Spectromorphology},
  author = {Smalley, Denis},
  year = {1997},
  month = aug,
  volume = {2},
  pages = {107--126},
  publisher = {{Cambridge University Press}},
  issn = {1469-8153, 1355-7718},
  doi = {10.1017/S1355771897009059},
  abstract = {The art of music is no longer limited to the sounding models of instruments and voices. Electoacoustic music opens access to all sounds, a bewildering sonic array ranging from the real to the surreal and beyond. For listeners the traditional links with physical sound-making are frequently ruptured: electroacoustic sound-shapes and qualities frequently do not indicate known sources and causes. Gone are the familiar articulations of instruments and vocal utterance: gone is the stability of note and interval: gone too is the reference of beat and metre. Composers also have problems: how to cut an aesthetic path and discover a stability in a wide-open sound world, how to develop appropriate sound-making methods, how to select technologies and software.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SUMQJ5TW/Smalley - 1997 - Spectromorphology explaining sound-shapes.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SSZLHM8G/A18EBE591592836FC22C20FB327D3232.html},
  journal = {Organised Sound},
  keywords = {sound shapes},
  language = {en},
  number = {2}
}

@article{sousaSketchbasedRetrievalDrawings2010,
  title = {Sketch-Based Retrieval of Drawings Using Spatial Proximity},
  author = {Sousa, Pedro and Fonseca, Manuel J.},
  year = {2010},
  volume = {21},
  pages = {69--80},
  issn = {1045926X},
  doi = {10.1016/j.jvlc.2009.12.001},
  abstract = {Currently, there are large collections of drawings from which users can select the desired ones to insert in their documents. However, to locate a particular drawing among thousands is not easy. In our prior work we proposed an approach to index and retrieve vector drawings by content, using topological and geometric information automatically extracted from figures. In this paper, we present a new approach to enrich the topological information by integrating spatial proximity in the topology graph, through the use of weights in adjacency links. Additionally, we developed a web search engine for clip art drawings, where we included the new technique. Experimental evaluation reveals that the use of topological proximity results in better retrieval results than topology alone. However, the increase in precision was not as high as we expected. To understand why, we analyzed sketched queries performed by users in previous experimental sessions and we present here the achieved conclusions.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JWNJT884/Sousa and Fonseca - 2010 - Sketch-based retrieval of drawings using spatial p.pdf},
  journal = {Journal of Visual Languages \& Computing},
  keywords = {sketch based retrieval},
  language = {en},
  number = {2}
}

@article{spenceCrossmodalCorrespondencesTutorial2011,
  title = {Crossmodal Correspondences: {{A}} Tutorial Review},
  shorttitle = {Crossmodal Correspondences},
  author = {Spence, Charles},
  year = {2011},
  month = may,
  volume = {73},
  pages = {971--95},
  doi = {10.3758/s13414-010-0073-7},
  abstract = {In many everyday situations, our senses are bombarded by many different unisensory signals at any given time. To gain the most veridical, and least variable, estimate of environmental stimuli/properties, we need to combine the individual noisy unisensory perceptual estimates that refer to the same object, while keeping those estimates belonging to different objects or events separate. How, though, does the brain "know" which stimuli to combine? Traditionally, researchers interested in the crossmodal binding problem have focused on the roles that spatial and temporal factors play in modulating multisensory integration. However, crossmodal correspondences between various unisensory features (such as between auditory pitch and visual size) may provide yet another important means of constraining the crossmodal binding problem. A large body of research now shows that people exhibit consistent crossmodal correspondences between many stimulus features in different sensory modalities. For example, people consistently match high-pitched sounds with small, bright objects that are located high up in space. The literature reviewed here supports the view that crossmodal correspondences need to be considered alongside semantic and spatiotemporal congruency, among the key constraints that help our brains solve the crossmodal binding problem.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/PSDSUHA9/Spence - 2011 - Crossmodal correspondences A tutorial review.pdf},
  journal = {Attention, perception \& psychophysics},
  keywords = {sound shape correspondance}
}

@inproceedings{stablesSemanticDescriptionTimbral2016,
  title = {Semantic {{Description}} of {{Timbral Transformations}} in {{Music Production}}},
  booktitle = {Proceedings of the 2016 {{ACM}} on {{Multimedia Conference}} - {{MM}} '16},
  author = {Stables, Ryan and De Man, Brecht and Enderby, Sean and Reiss, Joshua D. and Fazekas, Gy{\"o}rgy and Wilmering, Thomas},
  year = {2016},
  pages = {337--341},
  publisher = {{ACM Press}},
  address = {{Amsterdam, The Netherlands}},
  doi = {10.1145/2964284.2967238},
  abstract = {In music production, descriptive terminology is used to define perceived sound transformations. By understanding the underlying statistical features associated with these descriptions, we can aid the retrieval of contextually relevant processing parameters using natural language, and create intelligent systems capable of assisting in audio engineering. In this study, we present an analysis of a dataset containing descriptive terms gathered using a series of processing modules, embedded within a Digital Audio Workstation. By applying hierarchical clustering to the audio feature space, we show that similarity in term representations exists within and between transformation classes. Furthermore, the organisation of terms in low-dimensional timbre space can be explained using perceptual concepts such as size and dissonance. We conclude by performing Latent Semantic Indexing to show that similar groupings exist based on term frequency.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/DK5VETYI/Stables et al. - 2016 - Semantic Description of Timbral Transformations in.pdf},
  isbn = {978-1-4503-3603-1},
  language = {en}
}

@article{stuckeySecondStepData2015,
  title = {The Second Step in Data Analysis: {{Coding}} Qualitative Research Data},
  shorttitle = {The Second Step in Data Analysis},
  author = {Stuckey, Heather L.},
  year = {2015},
  month = jun,
  volume = {03},
  pages = {7--10},
  publisher = {{Thieme Medical and Scientific Publishers Private Ltd.}},
  issn = {2321-0656, 2321-0664},
  doi = {10.4103/2321-0656.140875},
  abstract = {Coding is a process used in the analysis of qualitative research, which takes time and creativity. Three steps will help facilitate this process:  1. Reading through the data and creating a storyline;  2. Categorizing the data into codes; and  3. Using memos for clarification and interpretation.  Remembering the research question or storyline, while coding will help keep the qualitative researcher focused on relevant codes. A data dictionary can be used to define the meaning of the codes and keep the process transparent. Coding is done using either predetermined (a priori) or emergent codes, and most often, a combination of the two. By using memos to help clarify how the researcher is constructing the codes and his/her interpretations, the analysis will be easier to write in the end and have more consistency. This paper describes the process of coding and writing memos in the analysis of qualitative data related to diabetes research.},
  copyright = {Thieme Medical and Scientific Publishers Private Ltd. A-12, Second Floor, Sector -2, NOIDA -201301, India},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7IDHJEEE/Stuckey - 2015 - The second step in data analysis Coding qualitati.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SRB3NC8X/2321-0656.html},
  journal = {Journal of Social Health and Diabetes},
  keywords = {Coding,interview coding,memos,methods,qualitative research},
  language = {en},
  number = {1}
}

@article{stuckeySecondStepData2015a,
  title = {The Second Step in Data Analysis: {{Coding}} Qualitative Research Data},
  shorttitle = {The Second Step in Data Analysis},
  author = {Stuckey, Heather},
  year = {2015},
  month = jun,
  volume = {03},
  pages = {007--010},
  issn = {2321-0656, 2321-0664},
  doi = {10.4103/2321-0656.140875},
  abstract = {Coding is a process used in the analysis of qualitative research, which takes time and creativity.Three steps will help facilitate this process: 1. Reading through the data and creating a storyline; 2. Categorizing the data into codes; and 3. Using memos for clarification and interpretation.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/TYMH9WRN/Stuckey - 2015 - The second step in data analysis Coding qualitati.pdf},
  journal = {Journal of Social Health and Diabetes},
  keywords = {interview coding},
  language = {en},
  number = {01}
}

@article{stylesWhenDoesMaluma2017,
  title = {When {{Does Maluma}}/{{Takete Fail}}? {{Two Key Failures}} and a {{Meta}}-{{Analysis Suggest That Phonology}} and {{Phonotactics Matter}}},
  shorttitle = {When {{Does Maluma}}/{{Takete Fail}}?},
  author = {Styles, Suzy J. and Gawne, Lauren},
  year = {2017},
  month = aug,
  volume = {8},
  issn = {2041-6695},
  doi = {10.1177/2041669517724807},
  abstract = {Eighty-seven years ago, K\"ohler reported that the majority of students picked the same answer in a quiz: Which novel word form (`maluma' or `takete') went best with which abstract line drawing (one curved, one angular). Others have consistently shown the effect in a variety of contexts, with only one reported failure by Rogers and Ross. In the spirit of transparency, we report our own failure in the same journal. In our study, speakers of Syuba, from the Himalaya in Nepal, do not show a preference when matching word forms `kiki' and `bubu' to spiky versus curvy shapes. We conducted a meta-analysis of previous studies to investigate the relationship between pseudoword legality and task effects. Our combined analyses suggest a common source for both of the failures: `wordiness' \textendash{} We believe these tests fail when the test words do not behave according to the sound structure of the target language.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LS9FUAC6/Styles and Gawne - 2017 - When Does MalumaTakete Fail Two Key Failures and.pdf},
  journal = {i-Perception},
  number = {4},
  pmcid = {PMC5574486},
  pmid = {28890777}
}

@inproceedings{suzukiShapeBotsShapechangingSwarm2019,
  title = {{{ShapeBots}}: {{Shape}}-Changing {{Swarm Robots}}},
  shorttitle = {{{ShapeBots}}},
  booktitle = {Proceedings of the 32nd {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Suzuki, Ryo and Zheng, Clement and Kakehi, Yasuaki and Yeh, Tom and Do, Ellen Yi-Luen and Gross, Mark D. and Leithinger, Daniel},
  year = {2019},
  month = oct,
  pages = {493--505},
  publisher = {{ACM}},
  address = {{New Orleans LA USA}},
  doi = {10.1145/3332165.3347911},
  abstract = {We introduce shape-changing swarm robots. A swarm of selftransformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6PM4XBSS/Suzuki et al. - 2019 - ShapeBots Shape-changing Swarm Robots.pdf},
  isbn = {978-1-4503-6816-2},
  language = {en}
}

@article{taruffiReviewMusicevokedVisual2019,
  title = {A Review of Music-Evoked Visual Mental Imagery: {{Conceptual}} Issues, Relation to Emotion, and Functional Outcome.},
  shorttitle = {A Review of Music-Evoked Visual Mental Imagery},
  author = {Taruffi, Liila and K{\"u}ssner, Mats B.},
  year = {2019},
  month = jun,
  volume = {29},
  pages = {62--74},
  issn = {2162-1535, 0275-3987},
  doi = {10.1037/pmu0000226},
  abstract = {Visual mental imagery has been characterized as an important aspect of our mental life, which consists of ``seeing'' in the absence of a sensory stimulus. However, the mechanisms underlying how visual mental images unfold during music listening have remained largely neglected. Here, we review the existing literature on the relation between music-evoked emotions and images and we draw attention on how visual mental imagery has been previously conceptualized in the music domain. We also propose to adopt a conceptual framework from research on spontaneous cognition, which will promote a more nuanced and comprehensive understanding of the different types of music-evoked visual mental imagery. Finally, we highlight how music's capability to trigger images can be harnessed in daily life as well as therapeutic practices to foster the benefits and minimize the costs of visual mental imagery.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZW2QGS9N/Taruffi and Küssner - 2019 - A review of music-evoked visual mental imagery Co.pdf},
  journal = {Psychomusicology: Music, Mind, and Brain},
  language = {en},
  number = {2-3}
}

@article{taydeFaceRecognitionLocal2013,
  title = {Face {{Recognition}} with {{Local Binary Patterns}}, {{Spatial Pyramid Histograms}} and {{Nearest Neighbor Classification}}},
  author = {Tayde, Abhijeet and Deshpande, A S},
  year = {2013},
  volume = {4},
  pages = {6},
  abstract = {Face recognition algorithms generally assume that face images are well aligned and have a similar pose yet in many different practical applications it is impossible to meet these certain conditions. Thus extending face recognition to unconstrained face images has become an active area for research. At this end, histograms of Local Binary Patterns (LBP) have proven to be highly discriminative descriptors for face recognition. Most LBP-based algorithms use a rigid descriptor matching strategy that's not robust against pose variation and misalignment. Here two algorithms are proposed for face recognition which are designed to deal with pose variations and misalignment. It also incorporate an illumination normalization step that increases robustness against lighting variations. The proposed algorithms use descriptors based on histograms of LBP and perform descriptor matching with spatial pyramid matching (SPM) and Naive Bayes Nearest Neighbor (NBNN) respectively. The main contribution is the inclusion of flexible spatial matching schemes; it uses an image-to-class relation to provide an improved robustness with respect to intra-class variations. The comparison is compulsory between the accuracy of the proposed algorithms against Ahonen's original LBP-based face recognition system and two baseline holistic classifiers on four standard datasets. Results indicate that the algorithm based on NBNN outperforms the other solutions and does so more markedly in presence of pose variations.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/BVNYA9SQ/Tayde and Deshpande - 2013 - Face Recognition with Local Binary Patterns, Spati.pdf},
  keywords = {LBP},
  language = {en},
  number = {7}
}

@article{taylorPhoneticSymbolismFour1962,
  title = {Phonetic Symbolism in Four Unrelated Languages},
  author = {Taylor, Insup Kim and Taylor, Maurice M.},
  year = {1962},
  volume = {16},
  pages = {344--356},
  publisher = {{University of Toronto Press}},
  issn = {0008-4255},
  doi = {10.1037/h0083261},
  abstract = {To test whether or not there is an intrinsic correspondence between sounds and meanings, consonant-vowel-consonant nonsense syllables were reated by monolingual students from the United States, Japan, Korea, and South India, on the following dimensions: big-small, active-passive, warm-cold, and pleasant-unpleasant. Phonetic symbolism occurred under all conditions tested, but the meanings associated with any particular sound were different from language to language. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YQNZ52LA/Taylor and Taylor - 1962 - Phonetic symbolism in four unrelated languages.pdf},
  journal = {Canadian Journal of Psychology/Revue canadienne de psychologie},
  keywords = {Language,monolingual students,Monolingualism,phonetic symbolism,Phonetics,sounds and meanings,Syllables,Symbolism,unrelated languages,Vowels},
  number = {4}
}

@article{thoretSeeingCirclesDrawing2016,
  title = {Seeing {{Circles}} and {{Drawing Ellipses}}: {{When Sound Biases Reproduction}} of {{Visual Motion}}},
  shorttitle = {Seeing {{Circles}} and {{Drawing Ellipses}}},
  author = {Thoret, Etienne and Aramaki, Mitsuko and Bringoux, Lionel and Ystad, S{\o}lvi and {Kronland-Martinet}, Richard},
  year = {2016},
  month = apr,
  volume = {11},
  pages = {e0154475},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0154475},
  abstract = {The perception and production of biological movements is characterized by the 1/3 power law, a relation linking the curvature and the velocity of an intended action. In particular, motions are perceived and reproduced distorted when their kinematics deviate from this biological law. Whereas most studies dealing with this perceptual-motor relation focused on visual or kinaesthetic modalities in a unimodal context, in this paper we show that auditory dynamics strikingly biases visuomotor processes. Biologically consistent or inconsistent circular visual motions were used in combination with circular or elliptical auditory motions. Auditory motions were synthesized friction sounds mimicking those produced by the friction of the pen on a paper when someone is drawing. Sounds were presented diotically and the auditory motion velocity was evoked through the friction sound timbre variations without any spatial cues. Remarkably, when subjects were asked to reproduce circular visual motion while listening to sounds that evoked elliptical kinematics without seeing their hand, they drew elliptical shapes. Moreover, distortion induced by inconsistent elliptical kinematics in both visual and auditory modalities added up linearly. These results bring to light the substantial role of auditory dynamics in the visuo-motor coupling in a multisensory context.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/V2TWN67E/Thoret et al. - 2016 - Seeing Circles and Drawing Ellipses When Sound Bi.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CFR375V6/article.html},
  journal = {PLOS ONE},
  keywords = {Curvature,Ellipses,Geometry,Kinematics,Motion,movement,Sensory perception,Velocity,Vision},
  language = {en},
  number = {4}
}

@article{thoretSoundShapeAuditory2014,
  title = {From Sound to Shape: {{Auditory}} Perception of Drawing Movements.},
  shorttitle = {From Sound to Shape},
  author = {Thoret, Etienne and Aramaki, Mitsuko and {Kronland-Martinet}, Richard and Velay, Jean-Luc and Ystad, S{\o}lvi},
  year = {2014},
  volume = {40},
  pages = {983--994},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/a0035441},
  abstract = {This study investigates the human ability to perceive biological movements through friction sounds produced by drawings and, furthermore, the ability to recover drawn shapes from the friction sounds generated. In a first experiment, friction sounds, real-time synthesized and modulated by the velocity profile of the drawing gesture, revealed that subjects associated a biological movement to those sounds whose timbre variations were generated by velocity profiles following the 1/3 power law. This finding demonstrates that sounds can adequately inform about human movements if their acoustic characteristics are in accordance with the kinematic rule governing actual movements. Further investigations of our ability to recognize drawn shapes were carried out in two association tasks in which both recorded and synthesized sounds had to be associated to both distinct and similar visual shapes. Results revealed that, for both synthesized and recorded sounds, subjects made correct associations for distinct shapes, while some confusion was observed for similar shapes. The comparisons made between recorded and synthesized sounds lead to conclude that the timbre variations induced by the velocity profile enabled the shape recognition. The results are discussed in the context of the ecological and ideomotor frameworks.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JTCAU5JW/Thoret et al. - 2014 - From sound to shape Auditory perception of drawin.pdf},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  keywords = {sound shape},
  language = {en},
  number = {3}
}

@inproceedings{troianoDeformableInterfacesPerforming2015,
  title = {Deformable {{Interfaces}} for {{Performing Music}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '15},
  author = {Troiano, Giovanni Maria and Pedersen, Esben Warming and Hornb{\ae}k, Kasper},
  year = {2015},
  pages = {377--386},
  publisher = {{ACM Press}},
  address = {{Seoul, Republic of Korea}},
  doi = {10.1145/2702123.2702492},
  abstract = {Deformable interfaces offer new possibilities for gestures, some of which have been shown effective in controlled laboratory studies. Little work, however, has attempted to match deformable interfaces to a demanding domain and evaluate them out of the lab. We investigate how musicians use deformable interfaces to perform electronic music. We invited musicians to three workshops, where they explored 10 deformable objects and generated ideas on how to use these objects to perform music. Based on the results from the workshops, we implemented sensors in the five preferred objects and programmed them for controlling sounds. Next, we ran a performance study where six musicians performed music with these objects at their studios. Our results show that (1) musicians systematically map deformations to certain musical parameters, (2) musicians use deformable interfaces especially to filter and modulate sounds, and (3) musicians think that deformable interfaces embody the parameters that they control. We discuss what these results mean to research in deformable interfaces.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GKNVWF7I/Troiano et al. - 2015 - Deformable Interfaces for Performing Music.pdf},
  isbn = {978-1-4503-3145-6},
  keywords = {deformable interface},
  language = {en}
}

@article{tubbCreativityExplorationControl,
  title = {Creativity, {{Exploration}} and {{Control}} in {{Musical Parameter Spaces}}},
  author = {Tubb, Robert H},
  pages = {496},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/KL5BPXVY/Tubb - Creativity, Exploration and Control in Musical Par.pdf},
  language = {en}
}

@article{tubbDivergentInterfaceSupporting,
  title = {The {{Divergent Interface}}: {{Supporting Creative Exploration}} of {{Parameter Spaces}}},
  author = {Tubb, Robert and Dixon, Simon},
  pages = {6},
  abstract = {This paper outlines a theoretical framework for creative technology based on two contrasting processes: divergent exploration and convergent optimisation. We claim that these two cases require different gesture-to-parameter mapping properties. We present results from a user experiment that motivates this theory. The experiment was conducted using a publicly available iPad app: ``Sonic Zoom''. Participants were encouraged to conduct an open ended exploration of synthesis timbre using a combination of two different interfaces. The first was a standard interface with ten sliders, hypothesised to be suited to the ``convergent'' stage of creation. The second was a mapping of the entire 10-D combinatorial space to a 2-D surface using a space filling curve. This novel interface was intended to support the ``divergent'' aspect of creativity. The paths of around 250 users through both 2-D and 10-D space were logged and analysed. Both the interaction data and questionnaire results show that the different interfaces tended to be used for different aspects of sound creation, and a combination of these two navigation styles was deemed to be more useful than either individually. The study indicates that the predictable, separate parameters found in most music technology are more appropriate for convergent tasks.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZMQJXCGI/Tubb and Dixon - The Divergent Interface Supporting Creative Explo.pdf},
  keywords = {interface},
  language = {en}
}

@article{tullisHowManyUsers,
  title = {How {{Many Users Are Enough}} for a {{Card}}-{{Sorting Study}}?},
  author = {Tullis, Tom and Investments, Fidelity and Wood, Larry and University, Brigham Young},
  pages = {10},
  abstract = {A study was conducted to assess the minimum number of participants needed for a card-sorting study. Similarity matrices and tree structures from various sample sizes were compared to those based on a set of 168 participants. Results indicate that reasonable structures are obtained from 20-30 participants.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/BSWYWLGF/Tullis et al. - How Many Users Are Enough for a Card-Sorting Study.pdf},
  language = {en}
}

@misc{VideoGamesWorldwide,
  title = {Video {{Games}} - Worldwide | {{Statista Market Forecast}}},
  abstract = {worldwide: Revenue in the Video Games segment amounts to US\$87,441m in 2020. Video Games are defined as fee-based video games distributed over the internet.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CHCCDCW5/worldwide.html},
  howpublished = {https://www.statista.com/outlook/203/100/video-games/worldwide},
  journal = {Statista},
  keywords = {video games},
  language = {en}
}

@article{voglIntelligentInterfaceDrum2016,
  title = {An {{Intelligent Interface}} for {{Drum Pattern Variation}} and {{Comparative Evaluation}} of {{Algorithms}}},
  author = {Vogl, Richard},
  year = {2016},
  volume = {64},
  pages = {503--513},
  doi = {http://dx.doi.org/10.17743/jaes.2016.0016},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/KGYZ6RW8/Vogl - 2016 - An Intelligent Interface for Drum Pattern Variatio.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/V3N9P4FY/3645.html},
  journal = {Journal of the Audio Engineering Society},
  number = {7/8}
}

@inproceedings{waldeckMobileLiquid2D2004,
  title = {Mobile Liquid {{2D}} Scatter Space ({{ML2DSS}})},
  booktitle = {Proceedings. {{Eighth International Conference}} on {{Information Visualisation}}, 2004. {{IV}} 2004.},
  author = {Waldeck, C. and Balfanz, D.},
  year = {2004},
  month = jul,
  pages = {494--498},
  issn = {1093-9547},
  doi = {10.1109/IV.2004.1320190},
  abstract = {This (video-) paper describes a user interface concept, which facilitates browsing of comparably large amounts of information on small screens. The viewing concept is based on a visually optimized star field display (Ahlberg \& Shneiderman 1994) and is introducing new concepts like liquid browsing (an expansion lens with pressure controlled magnetic force simulation), selection based filtering and representation manipulation, multimotion behavior tagging, (interaction-) transparent spaces and continuous state animation. All these create a very versatile information space with a liquid-like look and feel, which can be used in a very intuitive way, providing all the advantages of a mature interactive scatter plot and can easily be scaled to different screen sizes without any adjustment efforts. This paper focuses on visual design and interaction details and emphasizes the importance of the visual interactive quality for mobile information visualization.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LY4L9R5M/Waldeck and Balfanz - 2004 - Mobile liquid 2D scatter space (ML2DSS).pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/BVFYLBYB/1320190.html},
  keywords = {computer animation,continuous state animation,Filtering,Force control,interaction transparent spaces,interactive systems,Large screen displays,Lenses,liquid browsing,Magnetic forces,Magnetic liquids,Magnetic separation,mobile computing,mobile information visualization,mobile liquid 2D scatter space,multimotion behavior tagging,optimized star field display,Pressure control,pressure controlled magnetic force simulation,representation manipulation,Scattering,selection based filtering,user interface,user interfaces,User interfaces,vision-based knowledge discovery,visual design,visual interactive information space}
}

@article{waltonWhatAbstractArt1988,
  title = {What {{Is Abstract}} about the {{Art}} of {{Music}}?},
  author = {Walton, Kendall L.},
  year = {1988},
  volume = {46},
  pages = {351--364},
  publisher = {{[Wiley, American Society for Aesthetics]}},
  issn = {0021-8529},
  doi = {10.2307/431106},
  journal = {The Journal of Aesthetics and Art Criticism},
  number = {3}
}

@inproceedings{wanderleyESCHERmodelingPerformingComposed1998,
  title = {{{ESCHER}}-Modeling and Performing Composed Instruments in Real-Time},
  booktitle = {{{SMC}}'98 {{Conference Proceedings}}. 1998 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{Cat}}. {{No}}.{{98CH36218}})},
  author = {Wanderley, M.M. and Schnell, N. and Rovan, J.},
  year = {1998},
  volume = {2},
  pages = {1080--1084},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/ICSMC.1998.727836},
  abstract = {This article presents ESCHER, a sound synthesis environment based on Ircam\~Os real-time audio environment jMax. ESCHER is a modular system providing synthesis-independent prototyping of gesturally-controlled instruments by means of parameter interpolation. The system divides into two components: gestural controller and synthesis engine. Mapping between components takes place on two independent levels, coupled by an intermediate abstract parameter layer. This separation allows a flexible choice of controllers and/or sound synthesis methods.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/D3X9725R/Wanderley et al. - 1998 - ESCHER-modeling and performing composed instrument.pdf},
  isbn = {978-0-7803-4778-6},
  language = {en}
}

@book{wigginsComputerRepresentationMusicResearch,
  title = {Computer-{{Representation}} of {{Music}} in the {{Research Environment}}},
  author = {Wiggins, Geraint A.},
  abstract = {Music representation systems have been a part of human culture for millennia. The primary purpose of early music representation systems (that is, from ancient times until the beginning of musicology in the 18th century) was to record music for the purpose of reproduction by performers other than the composer and/or as aides memoire to the},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Y9MF952D/Wiggins - Computer-Representation of Music in the Research E.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LCFQCZW3/summary.html},
  keywords = {music representation}
}

@inproceedings{witkinScalespaceFilteringNew1984,
  title = {Scale-Space Filtering: {{A}} New Approach to Multi-Scale Description},
  shorttitle = {Scale-Space Filtering},
  booktitle = {{{ICASSP}} '84. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Witkin, A.},
  year = {1984},
  volume = {9},
  pages = {150--153},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/ICASSP.1984.1172729},
  abstract = {The extrema in a signal and its first few derivatives provide a useful general purpose qualitative description for many kinds of signals. A fundamental problem in computing such descriptions is scale: a derivative must be taken over some neighborhood, but there is seldom a principled basis for choosing its size. Scale-space filtering is a method that describes signals qualitatively, managing the ambiguity of scale in an organized and natural way. The signal is first expanded by convolution with gaussian masks over a continuum of sizes. This "scale-space" image is then collapsed, using its qualitative structure, into a tree providing a concise but complete qualitative description covering all scales of observation. The description is further refined by applying a stability criterion, to identify events that persist of large changes in scale.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/65AMXJGQ/Witkin - 1984 - Scale-space filtering A new approach to multi-sca.pdf},
  keywords = {filtering},
  language = {en}
}

@inproceedings{wolinShortStrawSimpleEffective,
  title={ShortStraw: A Simple and Effective Corner Finder for Polylines.},
  author={Wolin, Aaron and Eoff, Brian and Hammond, Tracy},
  booktitle={SBM},
  pages={33--40},
  year={2008}
}

@inproceedings{xiongRevisitingShortStrawImproving2009,
  title={Revisiting shortstraw: improving corner finding in sketch-based interfaces},
  author={Xiong, Yiyan and LaViola Jr, Joseph J},
  booktitle={Proceedings of the 6th Eurographics Symposium on Sketch-Based Interfaces and Modeling},
  pages={101--108},
  year={2009}
}

@inproceedings{yuDomainindependentSystemSketch2003,
  title = {A Domain-Independent System for Sketch Recognition},
  booktitle = {Proceedings of the 1st International Conference on {{Computer}} Graphics and Interactive Techniques in {{Australasia}} and {{South East Asia}}},
  author = {Yu, Bo and Cai, Shijie},
  year = {2003},
  month = feb,
  pages = {141--146},
  publisher = {{Association for Computing Machinery}},
  address = {{Melbourne, Australia}},
  doi = {10.1145/604471.604499},
  abstract = {Freehand sketching is a natural and powerful means of interpersonal communication. But to date, it still cannot be supported effectively by human-computer interface. In this paper, we describe a domain-independent system for sketch recognition. Our system allows users to draw sketches as naturally as how they do on paper, and it recognizes the drawing through imprecise stroke approximation which is implemented in a unified and incremental procedure. This method can handle smooth curves and hybrid shapes as gracefully as it does to polylines. With a feature-area verification mechanism and the intelligent adjustment in the post-process, the system can produce user-intended results. Moreover, the output is organized in a hierarchical structure which includes syntactic and semantic information as well as raw data. Our system mainly utilizes low-level geometric features and does not rely on any domain-specific knowledge. Therefore, it will serve as a general and solid foundation for future high-level applications.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/XXDFC6PH/Yu and Cai - 2003 - A domain-independent system for sketch recognition.pdf},
  isbn = {978-1-58113-578-7},
  keywords = {graphics recognition,HCI,multimodal interface,sketch recognition},
  series = {{{GRAPHITE}} '03}
}

@article{zacharakisAdditiveSynthesisTechnique2011,
  title = {An Additive Synthesis Technique for Independent Modification of the Auditory Perceptions of Brightness and Warmth},
  author = {Zacharakis, Asteris and Reiss, Josh},
  year = {2011},
  pages = {9},
  abstract = {An algorithm that achieves independent modification of two low-level features that are correlated with the auditory perceptions of brightness and warmth was implemented. The perceptual validity of the algorithm was tested through a series of listening tests in order to examine whether the low-level modification was indeed perceived as independent and to investigate the influence of the fundamental frequency on the perceived modification. A Multidimensional Scaling analysis (MDS) on listener responses to pairwise dissimilarity comparisons accompanied by a verbal elicitation experiment, examined the perceptual significance and independence of the two low-level features chosen. This is a first step for the future development of a perceptually based control of an additive synthesizer.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SU3GRKQA/Zacharakis and Reiss - 2011 - An additive synthesis technique for independent mo.pdf},
  language = {en}
}

@article{zacharakisInterlanguageUnificationMusical2015,
  title = {An {{Interlanguage Unification}} of {{Musical Timbre}}: {{Bridging Semantic}}, {{Perceptual}}, and {{Acoustic Dimensions}}},
  shorttitle = {An {{Interlanguage Unification}} of {{Musical Timbre}}},
  author = {Zacharakis, Asterios and Pastiadis, Konstantinos and Reiss, Joshua D.},
  year = {2015},
  month = apr,
  volume = {32},
  pages = {394--412},
  issn = {0730-7829, 1533-8312},
  doi = {10.1525/mp.2015.32.4.394},
  abstract = {Skip to Next Section The current study expands our previous work on interlanguage musical timbre semantics by examining the relationship between semantics and perception of timbre. Following Zacharakis, Pastiadis, and Reiss (2014), a pairwise dissimilarity listening test involving participants from two separate linguistic groups (Greek and English) was conducted. Subsequent multidimensional scaling analysis produced a 3D perceptual timbre space for each language. The comparison between perceptual spaces suggested that timbre perception is unaffected by native language. Additionally, comparisons between semantic and perceptual spaces revealed substantial similarities which suggest that verbal descriptions can convey a considerable amount of perceptual information. The previously determined semantic labels ``auditory texture'' and ``luminance'' featured the highest associations with perceptual dimensions for both languages. ``Auditory mass'' failed to show any strong correlations. Acoustic analysis identified energy distribution of harmonic partials, spectral detail, temporal/spectrotemporal characteristics and the fundamental frequency as the most salient acoustic correlates of perceptual dimensions.},
  copyright = {\textcopyright{} 2015 by The Regents of the University of California},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/M8KAGBK9/394.html},
  journal = {Music Perception: An Interdisciplinary Journal},
  keywords = {acoustic correlates,multidimensional scaling,musical timbre perception,perception,semantic description,timbre spaces},
  language = {en},
  number = {4}
}

@inproceedings{lobbers2021sketching,
  title={Sketching sounds: an exploratory study on sound-shape associations},
  author={L{\"o}bbers, Sebastian and Barthet, Mathieu and Fazekas, Gy{\"o}rgy},
  booktitle={International Computer Music Conference (ICMC)},
  pages = {6},
  journal={arXiv preprint arXiv:2107.07360},
  year={2021}
}

@inproceedings{lobbers2021buildingsynth,
  title={Sketching Sounds: Using Sound-shape Associations to Build a Sketch-based Sound Synthesizer},
  author={L{\"o}bbers, Sebastian and Fazekas, Gy{\"o}rgy},
  booktitle={Digital Music Research Network (DMRN+16)},
  journal={https://www.qmul.ac.uk/dmrn/media/dmrn/00.-DMRN-16-Proceedings-Combined---V4-(draft-before-submission).pdf},
  year={2021}
}

@article{zacharakisInterlanguageUnificationMusical2015a,
  title = {An {{Interlanguage Unification}} of {{Musical Timbre}}: {{Bridging Semantic}}, {{Perceptual}}, and {{Acoustic Dimensions}}},
  shorttitle = {An {{Interlanguage Unification}} of {{Musical Timbre}}},
  author = {Zacharakis, Asterios and Pastiadis, Konstantinos and Reiss, Joshua D.},
  year = {2015},
  month = apr,
  volume = {32},
  pages = {394--412},
  issn = {07307829, 15338312},
  doi = {10.1525/mp.2015.32.4.394},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/GG3VXYFZ/Zacharakis et al. - 2015 - An Interlanguage Unification of Musical Timbre Br.pdf},
  journal = {Music Perception: An Interdisciplinary Journal},
  language = {en},
  number = {4}
}

@article{zbyszynskiGestureTimbreSpaceMultidimensional,
  title = {Gesture-{{Timbre Space}}: {{Multidimensional Feature Mapping Using Machine Learning}} \& {{Concatenative Synthesis}}},
  author = {Zbyszynski, Michael and Donato, Balandino Di and Tanaka, Atau},
  pages = {13},
  abstract = {This paper presents a method for mapping embodied gesture, acquired with electromyography and motion sensing, to a corpus of small sound units, organised by derived timbral features using concatenative synthesis. Gestures and sounds can be associated directly using individual units and static poses, or by using a sound tracing method that leverages our intuitive associations between sound and embodied movement. We propose a method for augmenting corporal density to enable expressive variation on the original gesture-timbre space.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YZ8D2PTE/Zbyszynski et al. - Gesture-Timbre Space Multidimensional Feature Map.pdf},
  language = {en}
}

@article{zhangSketchBasedImageRetrieval2016,
  title = {Sketch-{{Based Image Retrieval}} by {{Salient Contour Reinforcement}}},
  author = {Zhang, Yuting and Qian, Xueming and Tan, Xianglong and Han, Junwei and Tang, Yuanyan},
  year = {2016},
  volume = {18},
  pages = {1604--1615},
  issn = {1941-0077},
  doi = {10.1109/TMM.2016.2568138},
  abstract = {The paper presents a sketch-based image retrieval algorithm. One of the main challenges in sketch-based image retrieval (SBIR) is to measure the similarity between a sketch and an image. To tackle this problem, we propose an SBIR-based approach by salient contour reinforcement. In our approach, we divide the image contour into two types. The first is the global contour map. The second, called the salient contour map, is helpful to find out the object in images similar to the query. In addition, based on the two contour maps, we propose a new descriptor, namely an angular radial orientation partitioning (AROP) feature. It fully utilizes the edge pixels' orientation information in contour maps to identify the spatial relationships. Our AROP feature based on the two candidate contour maps is both efficient and effective to discover false matches of local features between sketches and images, and can greatly improve the retrieval performance. The application of the retrieval system based on this algorithm is established. The experiments on the image dataset with 0.3 million images show the effectiveness of the proposed method and comparisons with other algorithms are also given. Compared to baseline performance, the proposed method achieves 10\% higher precision in top 5.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/T4GFK68R/7469342.html},
  journal = {IEEE Transactions on Multimedia},
  keywords = {angular radial orientation partitioning feature,AROP feature,Contour matching,edge pixel orientation information,false matches,feature extraction,Feature extraction,global contour map,Histograms,Image color analysis,image contour,image dataset,Image edge detection,image matching,image resolution,image retrieval,Image retrieval,Indexes,local features,salient contour,salient contour map,salient contour reinforcement,SBIR-based approach,Shape,similarity measure,sketch based image retrieval (SBIR),sketch-based image retrieval algorithm,spatial relationships},
  number = {8}
}

@article{zhangSketchBasedImageRetrieval2016a,
  title = {Sketch-{{Based Image Retrieval}} by {{Salient Contour Reinforcement}}},
  author = {Zhang, Yuting and Qian, Xueming and Tan, Xianglong and Han, Junwei and Tang, Yuanyan},
  year = {2016},
  month = aug,
  volume = {18},
  pages = {1604--1615},
  issn = {1941-0077},
  doi = {10.1109/TMM.2016.2568138},
  abstract = {The paper presents a sketch-based image retrieval algorithm. One of the main challenges in sketch-based image retrieval (SBIR) is to measure the similarity between a sketch and an image. To tackle this problem, we propose an SBIR-based approach by salient contour reinforcement. In our approach, we divide the image contour into two types. The first is the global contour map. The second, called the salient contour map, is helpful to find out the object in images similar to the query. In addition, based on the two contour maps, we propose a new descriptor, namely an angular radial orientation partitioning (AROP) feature. It fully utilizes the edge pixels' orientation information in contour maps to identify the spatial relationships. Our AROP feature based on the two candidate contour maps is both efficient and effective to discover false matches of local features between sketches and images, and can greatly improve the retrieval performance. The application of the retrieval system based on this algorithm is established. The experiments on the image dataset with 0.3 million images show the effectiveness of the proposed method and comparisons with other algorithms are also given. Compared to baseline performance, the proposed method achieves 10\% higher precision in top 5.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/W2KN3YF4/Zhang et al. - 2016 - Sketch-Based Image Retrieval by Salient Contour Re.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RTJLJQEU/7469342.html},
  journal = {IEEE Transactions on Multimedia},
  keywords = {angular radial orientation partitioning feature,AROP feature,Contour matching,edge pixel orientation information,false matches,feature extraction,Feature extraction,global contour map,Histograms,Image color analysis,image contour,image dataset,Image edge detection,image matching,image resolution,image retrieval,Image retrieval,Indexes,local features,salient contour,salient contour map,salient contour reinforcement,SBIR-based approach,Shape,similarity measure,sketch based image retrieval (SBIR),Sketch Retrieval,sketch-based image retrieval algorithm,spatial relationships},
  number = {8}
}

@inproceedings{zhangSymphonyCreativeParticipation2018,
  title = {Symphony : {{Creative Participation}} for {{Audiences}} of {{Live Music}}},
  shorttitle = {Symphony},
  author = {Zhang, Leshao and {Bryan-Kinns}, Nick and Barthet, Mathieu},
  year = {2018},
  abstract = {Most contemporary Western performing arts practices restrict creative interactions from audiences. Open Symphony is designed to explore audience-performer interaction in live music performance assisted by digital technology. Audiences can conduct improvising performers by voting for various musical `modes'. Technological components include a web-based mobile application, a visual client displaying generated symbolic scores, and a server service for the exchange of creative data. The interaction model, app and visualisation were designed through an iterative participatory design process. The visualisation communicates audience directions to performers upon which to improvise music, and enables the audience to get feedback on their voting. The system was experienced by about 120 audience and performer participants (35 completed surveys) in controlled (lab) and ``real world'' settings. Feedback on usability and user experience was overall positive and live interactions demonstrate significant levels of audience creative engagement. We identified further design challenges around audience sense of control, learnability and compositional structure.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2KU4BMRR/Zhang et al. - 2018 - Symphony  Creative Participation for Audiences of.pdf},
  keywords = {Digital electronics,Interaction,Iteration,Learnability,Mobile app,Server (computing),Symphony,Usability,User experience,Web application}
}

@article{zhangyutingSketchBasedImageRetrieval2016,
  title = {Sketch-{{Based Image Retrieval}} by {{Salient Contour Reinforcement}}},
  author = {ZhangYuting and QianXueming and TanXianglong and HanJunwei and TangYuanyan},
  year = {2016},
  month = aug,
  publisher = {{IEEE Press}},
  abstract = {The paper presents a sketch-based image retrieval algorithm. One of the main challenges in sketch-based image retrieval (SBIR) is to measure the similarity between a sketch and an image. To tackle ...},
  annotation = {PUB767 		Piscataway, NJ, USA},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/42RWZHMK/TMM.2016.html},
  journal = {IEEE Transactions on Multimedia},
  language = {EN}
}

@article{lobbers2021sketching,
  title={Sketching sounds: an exploratory study on sound-shape associations},
  author={L{\"o}bbers, Sebastian and Barthet, Mathieu and Fazekas, Gy{\"o}rgy},
  booktitle={International Computer Music Conference (ICMC)},
  year={2021}
}

@incollection{engeln2021similarity,
  title={Similarity Analysis of Visual Sketch-based Search for Sounds},
  author={Engeln, Lars and Le, Nhat Long and McGinity, Matthew and Groh, Rainer},
  booktitle={Audio Mostly 2021},
  pages={101--108},
  year={2021}
}

@inproceedings{wan2019towards,
  title={Towards audio to scene image synthesis using generative adversarial network},
  author={Wan, Chia-Hung and Chuang, Shun-Po and Lee, Hung-Yi},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={496--500},
  year={2019},
  organization={IEEE}
}

@inproceedings{ishibashi2020investigating,
  title={Investigating audio data visualization for interactive sound recognition},
  author={Ishibashi, Tatsuya and Nakao, Yuri and Sugano, Yusuke},
  booktitle={Proceedings of the 25th International Conference on Intelligent User Interfaces},
  pages={67--77},
  year={2020}
}

@inproceedings{das2021cloud2curve,
  title={Cloud2curve: Generation and vectorization of parametric sketches},
  author={Das, Ayan and Yang, Yongxin and Hospedales, Timothy M and Xiang, Tao and Song, Yi-Zhe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7088--7097},
  year={2021}
}

@article{duffy2018makes,
  title={What makes rhythms hard to perform? An investigation using Steve Reich’s Clapping Music},
  author={Duffy, Sam and Pearce, Marcus},
  journal={Plos one},
  volume={13},
  number={10},
  pages={e0205847},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{martin2020data,
  title={Data-Driven Analysis of Tiny Touchscreen Performance with MicroJam},
  author={Martin, Charles Patrick and Torresen, Jim},
  journal={Computer Music Journal},
  volume={43},
  number={4},
  pages={41--57},
  year={2020},
  publisher={MIT Press}
}

@book{norman2013design,
  title={The design of everyday things: Revised and expanded edition},
  author={Norman, Don},
  year={2013},
  publisher={Basic books}
}

@article{paea2018information,
  title={Information Architecture (IA): Using Multidimensional Scaling (MDS) and K-Means Clustering Algorithm for Analysis of Card Sorting Data.},
  author={Paea, Sione and Baird, Ross},
  journal={Journal of Usability Studies},
  volume={13},
  number={3},
  year={2018}
}

@article{hayes2020there,
  title={There’s more to timbre than musical instruments: semantic dimensions of FM sounds},
  author={Hayes, Ben and Saitis, Charalampos and others},
  year={2020},
  publisher={Timbre 2020}
}

@article{hayes2021neural,
  title={Neural waveshaping synthesis},
  author={Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
  journal={arXiv preprint arXiv:2107.05050},
  year={2021}
}

@article{hayes2021perceptual,
    title={Perceptual and semantic scaling of FM synthesis timbres: Common dimensions and the role of expertise},
    author={Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
    journal="ICMPC-ESCOM",
    year={2021}
}

@article{chatterjee2010assessment,
  title={The assessment of art attributes},
  author={Chatterjee, Anjan and Widick, Page and Sternschein, Rebecca and Smith, William B and Bromberger, Bianca},
  journal={Empirical Studies of the Arts},
  volume={28},
  number={2},
  pages={207--222},
  year={2010},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{vahidi2021modulation,
  title={A Modulation Front-End for Music Audio Tagging},
  author={Vahidi, Cyrus and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2021},
  organization={IEEE}
}