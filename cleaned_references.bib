@article{adeliAudiovisualCorrespondenceMusical2014,
  title={Audiovisual correspondence between musical timbre and visual shapes},
  author={Adeli, Mohammad and Rouat, Jean and Molotchnikoff, St{\'e}phane},
  journal={Frontiers in human neuroscience},
  volume={8},
  pages={352},
  year={2014},
  publisher={Frontiers}
}

@article{bottiniSoundSymbolismSighted2019,
  title={Sound symbolism in sighted and blind. The role of vision and orthography in sound-shape correspondences},
  author={Bottini, Roberto and Barilari, Marco and Collignon, Olivier},
  journal={Cognition},
  volume={185},
  pages={62--70},
  year={2019},
  publisher={Elsevier}
}

@article{peeters2004large,
  title={A large set of audio features for sound description (similarity and classification) in the CUIDADO project},
  author={Peeters, Geoffroy},
  journal={CUIDADO Ist Project Report},
  volume={54},
  number={0},
  pages={1--25},
  year={2004}
}

@article{braunUsingThematicAnalysis2006,
  title = {Using Thematic Analysis in Psychology},
  author = {Braun, Virginia and Clarke, Victoria},
  year = {2006},
  volume = {3},
  pages = {77--101},
  issn = {1478-0887, 1478-0895},
  doi = {10.1191/1478088706qp063oa},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7WBTBUAR/Braun and Clarke - 2006 - Using thematic analysis in psychology.pdf},
  journal = {Qualitative Research in Psychology},
  language = {en},
  number = {2}
}

@article{bremnerBoubaKikiNamibia2013,
  title={“Bouba” and “Kiki” in Namibia? A remote culture make similar shape--sound matches, but different shape--taste matches to Westerners},
  author={Bremner, Andrew J and Caparos, Serge and Davidoff, Jules and de Fockert, Jan and Linnell, Karina J and Spence, Charles},
  journal={Cognition},
  volume={126},
  number={2},
  pages={165--172},
  year={2013},
  publisher={Elsevier}
}

@article{brufordGrooveExplorerIntelligent2019,
  title = {Groove {{Explorer}}: {{An Intelligent Visual Interface}} for {{Drum Loop Library Navigation}}},
  author = {Bruford, Fred and Barthet, Mathieu and McDonald, SKoT and Sandler, Mark},
  year = {2019},
  pages = {4},
  abstract = {Music producers nowadays rely on increasingly large libraries of loops, samples and virtual instrument sounds as part of the composition process. Intelligent interfaces are therefore useful in enabling navigation of these databases in a way that supports the production workflow. Within virtual drumming software, producers typically rely on large libraries of symbolic drum loops. Due to their large size, navigating and exploring these libraries can be a difficult process. To address this, preliminary work is presented into the Groove Explorer. Using Self-Organizing Maps, a large library of symbolic drum loops is automatically mapped on a 2D space according to rhythmic similarity. This space can then be explored via a Max/MSP prototype interface. Early results suggest that while the algorithm works well for smaller datasets, further development is required, particularly in the similarity metric used, to make the tool scalable to large libraries.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/MPY5WY54/Bruford et al. - 2019 - Groove Explorer An Intelligent Visual Interface f.pdf},
  journal = {Los Angeles},
  keywords = {music browsing},
  language = {en}
}

@inproceedings{chen2010thumbnaildj,
  title={ThumbnailDJ: Visual Thumbnails of Music Content.},
  author={Chen, Ya-Xi and Kl{\"u}ber, Ren{\'e}},
  booktitle={11th International Society for Music Information Retrieval Conference (ISMIR)},
  pages={565--570},
  year={2010}
}

@article{davisFitnessNamesDrawings1961,
  title = {The {{Fitness}} of {{Names}} to {{Drawings}}. a {{Cross}}-{{Cultural Study}} in {{Tanganyika}}},
  author = {Davis, R.},
  year = {1961},
  volume = {52},
  pages = {259--268},
  issn = {2044-8295},
  doi = {10.1111/j.2044-8295.1961.tb00788.x},
  abstract = {A set of experiments is described in which native children in the Mahali peninsula, Lake Tanganyika, were asked to match nonsense names with abstract drawings. The African children allotted names to drawings in a highly consistent manner and their choices were similar to those of a control group of English school children. It is argued that the results support the theory that purely structural similarities between sounds and shapes can be appreciated and used in naming.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8295.1961.tb00788.x},
  copyright = {1961 The British Psychological Society},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/B7PZC4I7/j.2044-8295.1961.tb00788.html},
  journal = {British Journal of Psychology},
  language = {en},
  number = {3}
}

@article{engelnCoHEARenceAudibleShapes2020,
  title={CoHEARence of audible shapes—a qualitative user study for coherent visual audio design with resynthesized shapes},
  author={Engeln, Lars and Groh, Rainer},
  journal={Personal and Ubiquitous Computing},
  pages={1--11},
  year={2020},
  publisher={Springer}
}

@article{engelNeuralAudioSynthesis2017,
  title = {Neural {{Audio Synthesis}} of {{Musical Notes}} with {{WaveNet Autoencoders}}},
  author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
  year = {2017},
  month = apr,
  abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
  archivePrefix = {arXiv},
  eprint = {1704.01279},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/M5MRITXT/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Q7PEMQ26/1704.html},
  journal = {arXiv:1704.01279 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound,neural synthesis},
  primaryClass = {cs}
}

@article{van2014does,
  title={Does the colour of the mug influence the taste of the coffee?},
  author={Van Doorn, George H and Wuillemin, Dianne and Spence, Charles},
  journal={Flavour},
  volume={3},
  number={1},
  pages={1--7},
  year={2014},
  publisher={Springer}
}

@article{clemente2020set,
  title={A Set of 200 musical stimuli varying in balance, contour, symmetry, and complexity: Behavioral and computational assessments},
  author={Clemente, Ana and Vila-Vidal, Manel and Pearce, Marcus T and Aguil{\'o}, Germ{\'a}n and Corradi, Guido and Nadal, Marcos},
  journal={Behavior Research Methods},
  volume={52},
  number={4},
  pages={1491--1509},
  year={2020},
  publisher={Springer}
}

@article{rousseeuw1987silhouettes,
  title={Silhouettes: a graphical aid to the interpretation and validation of cluster analysis},
  author={Rousseeuw, Peter J},
  journal={Journal of computational and applied mathematics},
  volume={20},
  pages={53--65},
  year={1987},
  publisher={Elsevier}
}

@article{friedAudioQuilt2DArrangements,
  title = {{{AudioQuilt}}: {{2D Arrangements}} of {{Audio Samples}} Using {{Metric Learning}} and {{Kernelized Sorting}}},
  author = {Fried, Ohad and Jin, Zeyu and Finkelstein, Adam and Oda, Reid},
  pages = {6},
  abstract = {The modern musician enjoys access to a staggering number of audio samples. Composition software can ship with many gigabytes of data, and there are many more to be found online. However, conventional methods for navigating these libraries are still quite rudimentary, and often involve scrolling through alphabetical lists. We present AudioQuilt, a system for sample exploration that allows audio clips to be sorted according to user taste, and arranged in any desired 2D formation such that similar samples are located near each other. Our method relies on two advances in machine learning. First, metric learning allows the user to shape the audio feature space to match their own preferences. Second, kernelized sorting finds an optimal arrangement for the samples in 2D. We demonstrate our system with three new interfaces for exploring audio samples, and evaluate the technology qualitatively and quantitatively via a pair of user studies.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LPW75Y5F/Fried et al. - AudioQuilt 2D Arrangements of Audio Samples using.pdf},
  language = {en}
}

@article{garberAudioStellarOpenSource,
  title = {{{AudioStellar}}, an Open Source Corpus-Based Musical Instrument for Latent Sound Structure Discovery and Sonic Experimentation},
  author = {Garber, Leandro},
  pages = {6},
  abstract = {Generating a visual representation of short audio clips' similarities is not only useful for organizing and exploring an audio sample library but it also opens up a new range of possibilities for sonic experimentation. We present AudioStellar, an open source software that enables creative practitioners to create AI generated 2D visualizations (i.e latent space) of their own audio corpus without programming or machine learning knowledge. Sound artists can play their input corpus by interacting with this computer learned latent space using an user interface that provides built-in modes to experiment with. AudioStellar can interact with other software by MIDI syncing, sequencing, adding audio effects, and more. Creating novel forms of interaction is encouraged through OSC communication or writing custom C++ code using provided framework. AudioStellar has also proved useful as an educational strategy in courses and workshops for teaching concepts of programming, digital audio, machine learning and networks to young students in the digital art field.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/C2D2ITF6/Garber - AudioStellar, an open source corpus-based musical .pdf},
  language = {en}
}

@inproceedings{grillVisualizationPerceptualQualities2012,
  title = {Visualization of Perceptual Qualities in Textural Sounds},
  author = {Grill, Thomas and Flexer, Arthur},
  booktitle={International Computer Music Conference (ICMC)},
  year = {2012},
  pages = {8},
  abstract = {We describe a visualization strategy that is capable of efficiently representing relevant perceptual qualities of textural sounds. The general aim is to develop intuitive screenbased interfaces representing large collections of sounds, where sound retrieval shall be much facilitated by the exploitation of cross-modal mechanisms of human perception. We propose the use of metaphoric sensory properties that are shared between sounds and graphics, constructing a meaningful mapping of auditory to visual dimensions. For this purpose, we have implemented a visualization using tiled maps, essentially combining low-dimensional projection and iconic representation. To prove the suitability we show detailed results of experiments having been conducted in the form of an online survey. Potential future use in music creation is illustrated by a prototype sound browser application.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/6CPM7ED7/Grill and Flexer - 1929 - VISUALIZATION OF PERCEPTUAL QUALITIES IN TEXTURAL .pdf},
  journal = {. As},
  keywords = {perception,visualisation},
  language = {en}
}

@article{haNeuralRepresentationSketch2017,
  title = {A {{Neural Representation}} of {{Sketch Drawings}}},
  author = {Ha, David and Eck, Douglas},
  year = {2017},
  month = may,
  abstract = {We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.},
  archivePrefix = {arXiv},
  eprint = {1704.03477},
  eprinttype = {arxiv},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/25PTGG6U/Ha and Eck - 2017 - A Neural Representation of Sketch Drawings.pdf},
  journal = {arXiv:1704.03477 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,neural network,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kohler1929gestalt,
  title={Gestalt Psychology.[Psychologische Probleme 1933]},
  author={K{\"o}hler, Wolfgang},
  journal={New York Horace Liveright},
  year={1929}
}

@inproceedings{kneesSearchingAudioSketching2016,
  title={Searching for audio by sketching mental images of sound: A brave new idea for audio retrieval in creative music production},
  author={Knees, Peter and Andersen, Kristina},
  booktitle={Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval},
  pages={95--102},
  year={2016}
}

@inproceedings{kolhoffMusicIconsProcedural2006,
  title = {Music {{Icons}}: {{Procedural Glyphs}} for {{Audio Files}}},
  shorttitle = {Music {{Icons}}},
  booktitle = {2006 19th {{Brazilian Symposium}} on {{Computer Graphics}} and {{Image Processing}}},
  author = {Kolhoff, Philipp and Preub, Jacqueline and Loviscach, Jorn},
  year = {2006},
  month = oct,
  pages = {289--296},
  issn = {2377-5416},
  doi = {10.1109/SIBGRAPI.2006.30},
  abstract = {Nowadays, a personal music collection may comprise thousands of MP3 files. Visualization can help the user to gain an overview and to find similar songs inside so large a set. We describe a method to create icons from audio files in such a way that songs which the user considers similar receive similar icons. This allows visual data mining in standard directory listings of window-based operating systems. The icons consist of bloom-like shapes, whose form and color depend on eight parameters. These parameters are controlled through a neural net, the input of which are audio features that are extracted algorithmically from the MP3 files. To adapt the system to the user's perception and interests, the neural net is initially trained with a small set of songs and icons. User studies done on the system demonstrate a strong perceptual relation between music and icons},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Y9ETIQFA/Kolhoff et al. - 2006 - Music Icons Procedural Glyphs for Audio Files.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/FNBRXPQK/4027079.html},
  keywords = {audio feature,audio file,Auditory displays,data mining,Data mining,data visualisation,Data visualization,Digital audio players,feature extraction,Feature extraction,graphical user interfaces,MP3 file,music icon,neural net,neural nets,Neural networks,Operating systems,operating systems (computers),procedural glyph,Prototypes,Shape,sound visualisation,Two dimensional displays,visual data mining,window-based operating system}
}

@article{kussnerMusiciansAreMore2014,
  title = {Musicians Are More Consistent: {{Gestural}} Cross-Modal Mappings of Pitch, Loudness and Tempo in Real-Time},
  shorttitle = {Musicians Are More Consistent},
  author = {K{\"u}ssner, Mats B. and Tidhar, Dan and Prior, Helen M. and {Leech-Wilkinson}, Daniel},
  year = {2014},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00789},
  abstract = {Cross-modal mappings of auditory stimuli reveal valuable insights into how humans make sense of sound and music. Whereas researchers have investigated cross-modal mappings of sound features varied in isolation within paradigms such as speeded classification and forced-choice matching tasks, investigations of representations of concurrently varied sound features (e.g., pitch, loudness and tempo) with overt gestures\textemdash accounting for the intrinsic link between movement and sound\textemdash are scant. To explore the role of bodily gestures in cross-modal mappings of auditory stimuli we asked sixty-four musically trained and untrained participants to represent pure tones\textemdash continually sounding and concurrently varied in pitch, loudness and tempo\textemdash with gestures while the sound stimuli were played. We hypothesised musical training to lead to more consistent mappings between pitch and height, loudness and distance/height, and tempo and speed of hand movement and muscular energy. Our results corroborate previously reported pitch vs. height (higher pitch leading to higher elevation in space) and tempo vs. speed (increasing tempo leading to increasing speed of hand movement) associations, but also reveal novel findings pertaining to musical training which influenced consistency of pitch mappings, annulling a commonly observed bias for convex (i.e. rising-falling) pitch contours. Moreover, we reveal effects of interactions between musical parameters on cross-modal mappings (e.g., pitch and loudness on speed of hand movement), highlighting the importance of studying auditory stimuli concurrently varied in different musical parameters. Results are discussed in light of cross-modal cognition, with particular emphasis on studies within (embodied) music cognition. Implications for theoretical refinements and potential clinical applications are provided.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RF9YFEQD/Küssner et al. - 2014 - Musicians are more consistent Gestural cross-moda.pdf},
  journal = {Frontiers in Psychology},
  keywords = {cross-modal mappings,embodied music cognition,Gesture,musical training,real-time mappings,sound shape associations},
  language = {English}
}

@article{lewisSystemUsabilityScale2018,
  title = {The {{System Usability Scale}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {The {{System Usability Scale}}},
  author = {Lewis, James R.},
  year = {2018},
  month = jul,
  volume = {34},
  pages = {577--590},
  publisher = {{Taylor \& Francis}},
  issn = {1044-7318},
  doi = {10.1080/10447318.2018.1455307},
  abstract = {The System Usability Scale (SUS) is the most widely used standardized questionnaire for the assessment of perceived usability. This review of the SUS covers its early history from inception in the 1980s through recent research and its future prospects. From relatively inauspicious beginnings, when its originator described it as a ``quick and dirty usability scale,'' it has proven to be quick but not ``dirty.'' It is likely that the SUS will continue to be a popular measurement of perceived usability for the foreseeable future. When researchers and practitioners need a measure of perceived usability, they should strongly consider using the SUS.},
  annotation = {\_eprint: https://doi.org/10.1080/10447318.2018.1455307},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YFXNDGXN/Lewis - 2018 - The System Usability Scale Past, Present, and Fut.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/WKYMIBG6/10447318.2018.html},
  journal = {International Journal of Human\textendash Computer Interaction},
  keywords = {Perceived usability,standardized usability scale,SUS,System Usability,usability},
  number = {7}
}

@article{maurerShapeBoubasSound2006,
  title = {The Shape of Boubas: Sound\textendash Shape Correspondences in Toddlers and Adults},
  shorttitle = {The Shape of Boubas},
  author = {Maurer, Daphne and Pathman, Thanujeni and Mondloch, Catherine J.},
  year = {2006},
  volume = {9},
  pages = {316--322},
  issn = {1467-7687},
  doi = {10.1111/j.1467-7687.2006.00495.x},
  abstract = {A striking demonstration that sound\textendash object correspondences are not completely arbitrary is that adults map nonsense words with rounded vowels (e.g. bouba) to rounded shapes and nonsense words with unrounded vowels (e.g. kiki) to angular shapes (K\"ohler, 1947; Ramachandran \& Hubbard, 2001). Here we tested the bouba/kiki phenomenon in 2.5-year-old children and a control group of adults (n =20 per age), using four pairs of rounded versus pointed shapes and four contrasting pairs of nonsense words differing in vowel sound. Overall, participants at both ages matched words with rounded vowels to the rounder shapes and words with unrounded vowels to the pointed shapes (both ps {$<$} .0005), with no significant difference between the two ages (p {$>$} .10). Such naturally biased correspondences between sound and shape may influence the development of language.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NVWENS25/Maurer et al. - 2006 - The shape of boubas sound–shape correspondences i.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NFL997GP/j.1467-7687.2006.00495.html},
  journal = {Developmental Science},
  language = {en},
  number = {3}
}

@article{mullensiefenGoldsmithsMusicalSophistication,
  title={The musicality of non-musicians: an index for assessing musical sophistication in the general population},
  author={M{\"u}llensiefen, Daniel and Gingras, Bruno and Musil, Jason and Stewart, Lauren},
  journal={PloS one},
  volume={9},
  number={2},
  pages={e89642},
  year={2014},
  publisher={Public Library of Science}
}

@article{nielsenParsingRoleConsonants2013,
  title = {Parsing the Role of Consonants versus Vowels in the Classic {{Takete}}-{{Maluma}} Phenomenon.},
  author = {Nielsen, Alan K. S. and Rendall, Drew},
  year = {2013},
  volume = {67},
  pages = {153--163},
  issn = {1878-7290, 1196-1961},
  doi = {10.1037/a0030553},
  abstract = {Wolfgang K\"ohler (1929, Gestalt psychology, New York, NY: Liveright) famously reported a bias in people's choice of nonsense words as labels for novel objects, pointing to possible na\"ive expectations about language structure. Two accounts have been offered to explain this bias, one focusing on the visuomotor effects of different vowel forms and the other focusing on variation in the acoustic structure and perceptual quality of different consonants. To date, evidence in support of both effects is mixed. Moreover, the veracity of either effect has often been doubted due to perceived limitations in methodologies and stimulus materials. A novel word-construction experiment is presented to test both proposed effects using randomized word- and image-generation techniques to address previous methodological concerns. Results show that participants are sensitive to both vowel and consonant content, constructing novel words of relatively sonorant consonants and rounded vowels to label curved object images, and of relatively plosive consonants and nonrounded vowels to label jagged object images. Results point to additional influences on word construction potentially related to the articulatory affordances or constraints accompanying different word forms.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/ZEZRFXJW/Nielsen and Rendall - 2013 - Parsing the role of consonants versus vowels in th.pdf},
  journal = {Canadian Journal of Experimental Psychology/Revue canadienne de psychologie exp\'erimentale},
  language = {en},
  number = {2}
}

@article{pearceModellingTimbralHardness2019,
  title = {Modelling {{Timbral Hardness}}},
  author = {Pearce, Andy and Brookes, Tim and Mason, Russell},
  year = {2019},
  volume = {9},
  pages = {466},
  issn = {2076-3417},
  doi = {10.3390/app9030466},
  abstract = {Hardness is the most commonly searched timbral attribute within freesound.org, a commonly used online sound effect repository. A perceptual model of hardness was developed to enable the automatic generation of metadata to facilitate hardness-based filtering or sorting of search results. A training dataset was collected of 202 stimuli with 32 sound source types, and perceived hardness was assessed by a panel of listeners. A multilinear regression model was developed on six features: maximum bandwidth, attack centroid, midband level, percussive-to-harmonic ratio, onset strength, and log attack time. This model predicted the hardness of the training data with R2 = 0.76. It predicted hardness within a new dataset with R2 = 0.57, and predicted the rank order of individual sources perfectly, after accounting for the subjective variance of the ratings. Its performance exceeded that of human listeners.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YFWYPG7Z/Pearce et al. - 2019 - Modelling Timbral Hardness.pdf},
  journal = {Applied Sciences},
  language = {en},
  number = {3}
}

@article{ramachandranSynaesthesiaWindowPerception,
  title={Synaesthesia--a window into perception, thought and language},
  author={Ramachandran, Vilayanur S and Hubbard, Edward M},
  journal={Journal of consciousness studies},
  volume={8},
  number={12},
  pages={3--34},
  year={2001},
  publisher={Imprint Academic}
}

@article{bresenham1965algorithm,
  title={Algorithm for computer control of a digital plotter},
  author={Bresenham, Jack E},
  journal={IBM Systems journal},
  volume={4},
  number={1},
  pages={25--30},
  year={1965},
  publisher={IBM}
}

@article{koo2016guideline,
  title={A guideline of selecting and reporting intraclass correlation coefficients for reliability research},
  author={Koo, Terry K and Li, Mae Y},
  journal={Journal of chiropractic medicine},
  volume={15},
  number={2},
  pages={155--163},
  year={2016},
  publisher={Elsevier}
}

@article{saitisTimbreSemanticsLens2018,
  title={Timbre semantics through the lens of crossmodal correspondences: A new way of asking old questions},
  author={Saitis, Charalampos and Weinzierl, Stefan and von Kriegstein, Katharina and Ystad, S{\o}lvi and Cuskley, Christine},
  journal={Acoustical Science and Technology},
  volume={41},
  number={1},
  pages={365--368},
  year={2020},
  publisher={ACOUSTICAL SOCIETY OF JAPAN}
}

@article{sezginFeaturePointDetection,
  title = {Feature {{Point Detection}} and {{Curve Approximation}} for {{Early Processing}} of {{Free}}-{{Hand Sketches}}},
  author = {Sezgin, Tevfik Metin},
  pages = {77},
  abstract = {Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for feature detection in strokes and apply this technique using two approaches to signal processing, one using simple average based thresholding and a second using scale space.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/8AH958ZF/Sezgin - Feature Point Detection and Curve Approximation fo.pdf},
  keywords = {sketch recognition},
  language = {en}
}

@article{shinoharaTaketeMalumaAction2016,
  title = {Takete and {{Maluma}} in {{Action}}: {{A Cross}}-{{Modal Relationship}} between {{Gestures}} and {{Sounds}}},
  shorttitle = {Takete and {{Maluma}} in {{Action}}},
  author = {Shinohara, Kazuko and Yamauchi, Naoto and Kawahara, Shigeto and Tanaka, Hideyuki},
  year = {2016},
  month = sep,
  volume = {11},
  pages = {e0163525},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0163525},
  abstract = {Despite Saussure's famous observation that sound-meaning relationships are in principle arbitrary, we now have a substantial body of evidence that sounds themselves can have meanings, patterns often referred to as ``sound symbolism''. Previous studies have found that particular sounds can be associated with particular meanings, and also with particular static visual shapes. Less well studied is the association between sounds and dynamic movements. Using a free elicitation method, the current experiment shows that several sound symbolic associations between sounds and dynamic movements exist: (1) front vowels are more likely to be associated with small movements than with large movements; (2) front vowels are more likely to be associated with angular movements than with round movements; (3) obstruents are more likely to be associated with angular movements than with round movements; (4) voiced obstruents are more likely to be associated with large movements than with small movements. All of these results are compatible with the results of the previous studies of sound symbolism using static images or meanings. Overall, the current study supports the hypothesis that particular dynamic motions can be associated with particular sounds. Building on the current results, we discuss a possible practical application of these sound symbolic associations in sports instructions.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/9AJ6KDBW/Shinohara et al. - 2016 - Takete and Maluma in Action A Cross-Modal Relatio.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/U9ZKMD7U/article.html},
  journal = {PLOS ONE},
  keywords = {Acceleration,cross-modal mapping,gesture,Hands,Kinematics,Motion,Musculoskeletal system,perception,Semiotics,Sensory perception,Vowels},
  language = {en},
  number = {9}
}

@article{siedenburgComparisonApproachesTimbre2016,
  title={A comparison of approaches to timbre descriptors in music information retrieval and music psychology},
  author={Siedenburg, Kai and Fujinaga, Ichiro and McAdams, Stephen},
  journal={Journal of New Music Research},
  volume={45},
  number={1},
  pages={27--41},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{stuckeySecondStepData2015,
  title = {The Second Step in Data Analysis: {{Coding}} Qualitative Research Data},
  shorttitle = {The Second Step in Data Analysis},
  author = {Stuckey, Heather L.},
  year = {2015},
  month = jun,
  volume = {03},
  pages = {7--10},
  publisher = {{Thieme Medical and Scientific Publishers Private Ltd.}},
  issn = {2321-0656, 2321-0664},
  doi = {10.4103/2321-0656.140875},
  abstract = {Coding is a process used in the analysis of qualitative research, which takes time and creativity. Three steps will help facilitate this process:  1. Reading through the data and creating a storyline;  2. Categorizing the data into codes; and  3. Using memos for clarification and interpretation.  Remembering the research question or storyline, while coding will help keep the qualitative researcher focused on relevant codes. A data dictionary can be used to define the meaning of the codes and keep the process transparent. Coding is done using either predetermined (a priori) or emergent codes, and most often, a combination of the two. By using memos to help clarify how the researcher is constructing the codes and his/her interpretations, the analysis will be easier to write in the end and have more consistency. This paper describes the process of coding and writing memos in the analysis of qualitative data related to diabetes research.},
  copyright = {Thieme Medical and Scientific Publishers Private Ltd. A-12, Second Floor, Sector -2, NOIDA -201301, India},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7IDHJEEE/Stuckey - 2015 - The second step in data analysis Coding qualitati.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SRB3NC8X/2321-0656.html},
  journal = {Journal of Social Health and Diabetes},
  keywords = {Coding,interview coding,memos,methods,qualitative research},
  language = {en},
  number = {1}
}

@article{taylorPhoneticSymbolismFour1962,
  title = {Phonetic Symbolism in Four Unrelated Languages},
  author = {Taylor, Insup Kim and Taylor, Maurice M.},
  year = {1962},
  volume = {16},
  pages = {344--356},
  publisher = {{University of Toronto Press}},
  issn = {0008-4255},
  doi = {10.1037/h0083261},
  abstract = {To test whether or not there is an intrinsic correspondence between sounds and meanings, consonant-vowel-consonant nonsense syllables were reated by monolingual students from the United States, Japan, Korea, and South India, on the following dimensions: big-small, active-passive, warm-cold, and pleasant-unpleasant. Phonetic symbolism occurred under all conditions tested, but the meanings associated with any particular sound were different from language to language. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YQNZ52LA/Taylor and Taylor - 1962 - Phonetic symbolism in four unrelated languages.pdf},
  journal = {Canadian Journal of Psychology/Revue canadienne de psychologie},
  keywords = {Language,monolingual students,Monolingualism,phonetic symbolism,Phonetics,sounds and meanings,Syllables,Symbolism,unrelated languages,Vowels},
  number = {4}
}

@article{thoretSeeingCirclesDrawing2016,
  title = {Seeing {{Circles}} and {{Drawing Ellipses}}: {{When Sound Biases Reproduction}} of {{Visual Motion}}},
  shorttitle = {Seeing {{Circles}} and {{Drawing Ellipses}}},
  author = {Thoret, Etienne and Aramaki, Mitsuko and Bringoux, Lionel and Ystad, S{\o}lvi and {Kronland-Martinet}, Richard},
  year = {2016},
  month = apr,
  volume = {11},
  pages = {e0154475},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0154475},
  abstract = {The perception and production of biological movements is characterized by the 1/3 power law, a relation linking the curvature and the velocity of an intended action. In particular, motions are perceived and reproduced distorted when their kinematics deviate from this biological law. Whereas most studies dealing with this perceptual-motor relation focused on visual or kinaesthetic modalities in a unimodal context, in this paper we show that auditory dynamics strikingly biases visuomotor processes. Biologically consistent or inconsistent circular visual motions were used in combination with circular or elliptical auditory motions. Auditory motions were synthesized friction sounds mimicking those produced by the friction of the pen on a paper when someone is drawing. Sounds were presented diotically and the auditory motion velocity was evoked through the friction sound timbre variations without any spatial cues. Remarkably, when subjects were asked to reproduce circular visual motion while listening to sounds that evoked elliptical kinematics without seeing their hand, they drew elliptical shapes. Moreover, distortion induced by inconsistent elliptical kinematics in both visual and auditory modalities added up linearly. These results bring to light the substantial role of auditory dynamics in the visuo-motor coupling in a multisensory context.},
  file = {/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/V2TWN67E/Thoret et al. - 2016 - Seeing Circles and Drawing Ellipses When Sound Bi.pdf;/Users/sebastian/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CFR375V6/article.html},
  journal = {PLOS ONE},
  keywords = {Curvature,Ellipses,Geometry,Kinematics,Motion,movement,Sensory perception,Velocity,Vision},
  language = {en},
  number = {4}
}

@inproceedings{wolinShortStrawSimpleEffective,
  title={ShortStraw: A Simple and Effective Corner Finder for Polylines.},
  author={Wolin, Aaron and Eoff, Brian and Hammond, Tracy},
  booktitle={SBM},
  pages={33--40},
  year={2008}
}

@inproceedings{xiongRevisitingShortStrawImproving2009,
  title={Revisiting shortstraw: improving corner finding in sketch-based interfaces},
  author={Xiong, Yiyan and LaViola Jr, Joseph J},
  booktitle={Proceedings of the 6th Eurographics Symposium on Sketch-Based Interfaces and Modeling},
  pages={101--108},
  year={2009}
}

@incollection{engeln2021similarity,
  title={Similarity Analysis of Visual Sketch-based Search for Sounds},
  author={Engeln, Lars and Le, Nhat Long and McGinity, Matthew and Groh, Rainer},
  booktitle={Audio Mostly 2021},
  pages={101--108},
  year={2021}
}

@inproceedings{wan2019towards,
  title={Towards audio to scene image synthesis using generative adversarial network},
  author={Wan, Chia-Hung and Chuang, Shun-Po and Lee, Hung-Yi},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={496--500},
  year={2019},
  organization={IEEE}
}

@inproceedings{ishibashi2020investigating,
  title={Investigating audio data visualization for interactive sound recognition},
  author={Ishibashi, Tatsuya and Nakao, Yuri and Sugano, Yusuke},
  booktitle={Proceedings of the 25th International Conference on Intelligent User Interfaces},
  pages={67--77},
  year={2020}
}

@book{norman2013design,
  title={The design of everyday things: Revised and expanded edition},
  author={Norman, Don},
  year={2013},
  publisher={Basic books}
}

@article{paea2018information,
  title={Information Architecture (IA): Using Multidimensional Scaling (MDS) and K-Means Clustering Algorithm for Analysis of Card Sorting Data.},
  author={Paea, Sione and Baird, Ross},
  journal={Journal of Usability Studies},
  volume={13},
  number={3},
  year={2018}
}

@article{hayes2020there,
  title={There’s more to timbre than musical instruments: semantic dimensions of FM sounds},
  author={Hayes, Ben and Saitis, Charalampos and others},
  year={2020},
  publisher={Timbre 2020}
}

@article{hayes2021perceptual,
    title={Perceptual and semantic scaling of FM synthesis timbres: Common dimensions and the role of expertise},
    author={Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
    journal="ICMPC-ESCOM",
    year={2021}
}

@inproceedings{vahidi2021modulation,
  title={A Modulation Front-End for Music Audio Tagging},
  author={Vahidi, Cyrus and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2021},
  organization={IEEE}
}
